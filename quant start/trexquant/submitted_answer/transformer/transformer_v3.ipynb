{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vocabulary import Vocab\n",
    "from utils import *\n",
    "from embedding import PositionalEncoding, Embeddings\n",
    "from layers import *\n",
    "from criterion import KLLossMasked\n",
    "from optimizer import NoamOpt\n",
    "from bert import Bert, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_every_iter = 100\n",
    "validate_every_iter = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create_data()\n",
    "#create_data_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = 'model_v5/'\n",
    "if not os.path.isdir(directory):\n",
    "    os.mkdir(directory)\n",
    "model_save_path = 'bert.checkpoint'\n",
    "model_save_path = os.path.join(directory, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "small_size = False\n",
    "use_checkpoint = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "vocab = Vocab()\n",
    "V = len(vocab.char2id)\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "h = 8\n",
    "n_encoders = 6\n",
    "\n",
    "batch_size = 1024\n",
    "num_epochs = 1000\n",
    "device_id = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_attn = MultiHeadedAttention(h=h, d_model=d_model, d_k=d_model // h, d_v=d_model // h, dropout=0.1)\n",
    "feed_forward = FullyConnectedFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "position = PositionalEncoding(d_model, dropout=0.1)\n",
    "embedding = nn.Sequential(Embeddings(d_model=d_model, vocab=V), position)\n",
    "\n",
    "encoder = Encoder(self_attn=self_attn, feed_forward=feed_forward, size=d_model, dropout=0.1)\n",
    "generator = Generator(d_model=d_model, vocab_size=V)\n",
    "model = Bert(encoder=encoder, embedding=embedding, generator=generator, n_layers=n_encoders)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-9, betas=(0.9, 0.98), eps=1e-9)\n",
    "model_opt = NoamOpt(d_model, 2, 4000, opt)\n",
    "criterion = nn.KLDivLoss(reduction=\"batchmean\").to(device)\n",
    "#criterion = nn.CrossEntropyLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = read_train_data(filepath=\"./pairs_train.txt\", small = small_size)\n",
    "dev_data = read_dev_data(filepath=\"./pairs_valid.txt\", small = small_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading checkpoint from epoch 87, iter 140000\n"
     ]
    }
   ],
   "source": [
    "hist_valid_scores = []\n",
    "\n",
    "if use_checkpoint:\n",
    "    checkpoint = torch.load(\"./model_v3/bert.checkpointBest_epoch_87.checkpoint\")\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    step = checkpoint['_step']\n",
    "    rate = checkpoint['_rate']\n",
    "    current_train_iter = checkpoint['train_iter']\n",
    "    model_opt._step = step\n",
    "    model_opt._rate = rate\n",
    "    print(f'reading checkpoint from epoch {current_epoch}, iter {current_train_iter}')\n",
    "else:\n",
    "    current_epoch = 0\n",
    "    current_train_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    train_iter = report_loss = cum_loss = valid_num = 0\n",
    "    report_samples = cum_samples = 0\n",
    "    \n",
    "    for epoch in range(current_epoch, num_epochs):\n",
    "        print(\"=\" * 30)\n",
    "        model.train()\n",
    "        #loss_compute = KLLossMasked(model.generator, criterion, opt=model_opt)\n",
    "\n",
    "        start = time.time()\n",
    "        train_data_iter = create_words_batch_v2(train_data, vocab, mini_batch=batch_size, shuffle=True, device=device)\n",
    "        for i, batch in enumerate(train_data_iter):\n",
    "            if use_checkpoint and train_iter <= current_train_iter:\n",
    "                train_iter = current_train_iter + 1\n",
    "                continue\n",
    "            x = batch.src\n",
    "            #x_mask = batch.src_mask\n",
    "            x_mask = x.unsqueeze(1).to(model.device)\n",
    "            #width = x.shape[1]\n",
    "            if(batch.guessed is not None):\n",
    "                x = torch.cat([batch.src, batch.guessed], dim = 1)\n",
    "                #x_mask = ((x != mask_token) & (x != pad_token)).unsqueeze(-2)\n",
    "                x_mask = x.unsqueeze(1).to(model.device)\n",
    "            out = model.forward(x, x_mask)\n",
    "\n",
    "            generator_mask = torch.zeros(x.shape[0], V, device=device)\n",
    "            generator_mask = generator_mask.scatter_(1, x, mask_token)\n",
    "\n",
    "            x = model.generator(out, generator_mask)\n",
    "            x = nn.LogSoftmax(dim=1)(x)\n",
    "            y = batch.tgt.masked_fill(generator_mask == mask_token, 0)\n",
    "            y = y/(torch.sum(y, dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "            batch_loss = criterion(x, y)\n",
    "            loss = batch_loss\n",
    "            loss.backward()\n",
    "            model_opt.step()\n",
    "            model_opt.optimizer.zero_grad()\n",
    "            \n",
    "            #print(train_iter, loss.item(), torch.any(torch.isnan(out)), torch.any(torch.isnan(x)), torch.any(torch.isnan(y)), torch.all(y==0))\n",
    "            #if(torch.any(torch.isnan(loss))):\n",
    "            #    print(out, out.shape)\n",
    "            #    print(batch.src, batch.src.shape)\n",
    "            #    print(batch.src_mask, batch.src.shape)\n",
    "            #    print(batch.tgt)\n",
    "            #    print(generator_mask)\n",
    "            #    print(\"x\", x)\n",
    "            #    print(\"y\", y)\n",
    "            #    exit()\n",
    "\n",
    "            batch_loss_val = batch_loss.item()\n",
    "            report_loss += batch_loss_val\n",
    "            cum_loss += batch_loss_val\n",
    "            report_samples += batch_size\n",
    "            cum_samples += batch_size\n",
    "\n",
    "            train_iter += 1\n",
    "\n",
    "            if train_iter % log_every_iter == 0:\n",
    "                elapsed = time.time() - start\n",
    "                print(f'epoch {epoch}, iter {train_iter}, avg. loss {report_loss / report_samples:.5f} time elapsed {elapsed:.2f}sec')\n",
    "                start = time.time()\n",
    "                report_loss = report_samples = 0\n",
    "\n",
    "            if train_iter % validate_every_iter == 0:\n",
    "                print(f'epoch {epoch}, iter {train_iter}, cum. loss {cum_loss / cum_samples:.5f} examples {cum_samples}')\n",
    "                cum_samples = cum_loss = 0.\n",
    "\n",
    "                print('begin evaluation...')\n",
    "                valid_num += 1\n",
    "                acc = evaluate_acc(model, vocab, dev_data, device=model.device)\n",
    "                print(f'validation: iter {train_iter}, dev. acc {acc:.4f}')\n",
    "\n",
    "                valid_metric = acc\n",
    "\n",
    "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
    "                hist_valid_scores.append(valid_metric)\n",
    "\n",
    "                if is_better:\n",
    "                    print('save currently the best model to [%s]' % (model_save_path + \"Best_epoch_%d.checkpoint\"%(epoch)))\n",
    "                    torch.save({'epoch': epoch,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': model_opt.optimizer.state_dict(),\n",
    "                                'loss': cum_loss,\n",
    "                                '_rate': model_opt._rate,\n",
    "                                '_step': model_opt._step,\n",
    "                                'train_iter': train_iter,\n",
    "                                'hist_valid_scores': hist_valid_scores,\n",
    "                                },  model_save_path + \"Best_epoch_%d.checkpoint\"%(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "epoch 87, iter 140100, avg. loss 0.00115 time elapsed 82.11sec\n",
      "epoch 87, iter 140200, avg. loss 0.00110 time elapsed 22.72sec\n",
      "epoch 87, iter 140300, avg. loss 0.00109 time elapsed 22.66sec\n",
      "epoch 87, iter 140400, avg. loss 0.00108 time elapsed 23.01sec\n",
      "epoch 87, iter 140500, avg. loss 0.00108 time elapsed 22.97sec\n",
      "epoch 87, iter 140600, avg. loss 0.00108 time elapsed 22.75sec\n",
      "epoch 87, iter 140700, avg. loss 0.00107 time elapsed 22.84sec\n",
      "epoch 87, iter 140800, avg. loss 0.00107 time elapsed 22.65sec\n",
      "epoch 87, iter 140900, avg. loss 0.00107 time elapsed 22.78sec\n",
      "epoch 87, iter 141000, avg. loss 0.00107 time elapsed 22.78sec\n",
      "epoch 87, iter 141100, avg. loss 0.00106 time elapsed 23.08sec\n",
      "epoch 87, iter 141200, avg. loss 0.00106 time elapsed 22.91sec\n",
      "epoch 87, iter 141300, avg. loss 0.00106 time elapsed 22.87sec\n",
      "epoch 87, iter 141400, avg. loss 0.00106 time elapsed 22.71sec\n",
      "epoch 87, iter 141500, avg. loss 0.00106 time elapsed 22.71sec\n",
      "epoch 87, iter 141600, avg. loss 0.00106 time elapsed 22.78sec\n",
      "==============================\n",
      "epoch 88, iter 141700, avg. loss 0.00106 time elapsed 76.18sec\n",
      "epoch 88, iter 141800, avg. loss 0.00106 time elapsed 22.90sec\n",
      "epoch 88, iter 141900, avg. loss 0.00106 time elapsed 22.86sec\n",
      "epoch 88, iter 142000, avg. loss 0.00106 time elapsed 22.85sec\n",
      "epoch 88, iter 142100, avg. loss 0.00106 time elapsed 22.66sec\n",
      "epoch 88, iter 142200, avg. loss 0.00106 time elapsed 22.52sec\n",
      "epoch 88, iter 142300, avg. loss 0.00106 time elapsed 23.08sec\n",
      "epoch 88, iter 142400, avg. loss 0.00106 time elapsed 22.61sec\n",
      "epoch 88, iter 142500, avg. loss 0.00106 time elapsed 22.88sec\n",
      "epoch 88, iter 142600, avg. loss 0.00106 time elapsed 22.92sec\n",
      "epoch 88, iter 142700, avg. loss 0.00106 time elapsed 22.92sec\n",
      "epoch 88, iter 142800, avg. loss 0.00105 time elapsed 23.24sec\n",
      "epoch 88, iter 142900, avg. loss 0.00106 time elapsed 23.14sec\n",
      "epoch 88, iter 143000, avg. loss 0.00105 time elapsed 22.89sec\n",
      "epoch 88, iter 143100, avg. loss 0.00106 time elapsed 23.17sec\n",
      "epoch 88, iter 143200, avg. loss 0.00106 time elapsed 23.04sec\n",
      "==============================\n",
      "epoch 89, iter 143300, avg. loss 0.00105 time elapsed 67.68sec\n",
      "epoch 89, iter 143400, avg. loss 0.00106 time elapsed 22.63sec\n",
      "epoch 89, iter 143500, avg. loss 0.00105 time elapsed 22.75sec\n",
      "epoch 89, iter 143600, avg. loss 0.00105 time elapsed 22.84sec\n",
      "epoch 89, iter 143700, avg. loss 0.00105 time elapsed 22.69sec\n",
      "epoch 89, iter 143800, avg. loss 0.00105 time elapsed 22.69sec\n",
      "epoch 89, iter 143900, avg. loss 0.00105 time elapsed 22.70sec\n",
      "epoch 89, iter 144000, avg. loss 0.00105 time elapsed 22.86sec\n",
      "epoch 89, iter 144100, avg. loss 0.00105 time elapsed 22.97sec\n",
      "epoch 89, iter 144200, avg. loss 0.00106 time elapsed 22.69sec\n",
      "epoch 89, iter 144300, avg. loss 0.00105 time elapsed 23.15sec\n",
      "epoch 89, iter 144400, avg. loss 0.00105 time elapsed 22.78sec\n",
      "epoch 89, iter 144500, avg. loss 0.00105 time elapsed 22.95sec\n",
      "epoch 89, iter 144600, avg. loss 0.00105 time elapsed 22.80sec\n",
      "epoch 89, iter 144700, avg. loss 0.00105 time elapsed 22.66sec\n",
      "epoch 89, iter 144800, avg. loss 0.00105 time elapsed 22.92sec\n",
      "==============================\n",
      "epoch 90, iter 144900, avg. loss 0.00105 time elapsed 59.60sec\n",
      "epoch 90, iter 145000, avg. loss 0.00105 time elapsed 22.79sec\n",
      "epoch 90, iter 145000, cum. loss 0.00106 examples 5118976\n",
      "begin evaluation...\n",
      "validation: iter 145000, dev. acc 0.7122\n",
      "save currently the best model to [model_v5/bert.checkpointBest_epoch_90.checkpoint]\n",
      "epoch 90, iter 145100, avg. loss 0.00105 time elapsed 67.36sec\n",
      "epoch 90, iter 145200, avg. loss 0.00105 time elapsed 22.63sec\n",
      "epoch 90, iter 145300, avg. loss 0.00106 time elapsed 23.01sec\n",
      "epoch 90, iter 145400, avg. loss 0.00105 time elapsed 22.94sec\n",
      "epoch 90, iter 145500, avg. loss 0.00105 time elapsed 22.67sec\n",
      "epoch 90, iter 145600, avg. loss 0.00105 time elapsed 23.05sec\n",
      "epoch 90, iter 145700, avg. loss 0.00105 time elapsed 22.89sec\n",
      "epoch 90, iter 145800, avg. loss 0.00105 time elapsed 22.76sec\n",
      "epoch 90, iter 145900, avg. loss 0.00105 time elapsed 22.77sec\n",
      "epoch 90, iter 146000, avg. loss 0.00105 time elapsed 22.65sec\n",
      "epoch 90, iter 146100, avg. loss 0.00105 time elapsed 22.77sec\n",
      "epoch 90, iter 146200, avg. loss 0.00105 time elapsed 22.72sec\n",
      "epoch 90, iter 146300, avg. loss 0.00105 time elapsed 22.51sec\n",
      "epoch 90, iter 146400, avg. loss 0.00105 time elapsed 22.94sec\n",
      "epoch 90, iter 146500, avg. loss 0.00105 time elapsed 23.13sec\n",
      "==============================\n",
      "epoch 91, iter 146600, avg. loss 0.00105 time elapsed 80.88sec\n",
      "epoch 91, iter 146700, avg. loss 0.00105 time elapsed 22.84sec\n",
      "epoch 91, iter 146800, avg. loss 0.00105 time elapsed 22.88sec\n",
      "epoch 91, iter 146900, avg. loss 0.00105 time elapsed 22.93sec\n",
      "epoch 91, iter 147000, avg. loss 0.00105 time elapsed 23.05sec\n",
      "epoch 91, iter 147100, avg. loss 0.00105 time elapsed 22.89sec\n",
      "epoch 91, iter 147200, avg. loss 0.00105 time elapsed 22.71sec\n",
      "epoch 91, iter 147300, avg. loss 0.00105 time elapsed 22.95sec\n",
      "epoch 91, iter 147400, avg. loss 0.00104 time elapsed 23.14sec\n",
      "epoch 91, iter 147500, avg. loss 0.00105 time elapsed 22.58sec\n",
      "epoch 91, iter 147600, avg. loss 0.00105 time elapsed 22.66sec\n",
      "epoch 91, iter 147700, avg. loss 0.00104 time elapsed 22.56sec\n",
      "epoch 91, iter 147800, avg. loss 0.00105 time elapsed 23.11sec\n",
      "epoch 91, iter 147900, avg. loss 0.00104 time elapsed 22.88sec\n",
      "epoch 91, iter 148000, avg. loss 0.00105 time elapsed 22.58sec\n",
      "epoch 91, iter 148100, avg. loss 0.00104 time elapsed 22.59sec\n",
      "==============================\n",
      "epoch 92, iter 148200, avg. loss 0.00105 time elapsed 77.11sec\n",
      "epoch 92, iter 148300, avg. loss 0.00105 time elapsed 22.63sec\n",
      "epoch 92, iter 148400, avg. loss 0.00105 time elapsed 22.93sec\n",
      "epoch 92, iter 148500, avg. loss 0.00105 time elapsed 23.01sec\n",
      "epoch 92, iter 148600, avg. loss 0.00105 time elapsed 22.77sec\n",
      "epoch 92, iter 148700, avg. loss 0.00105 time elapsed 23.18sec\n",
      "epoch 92, iter 148800, avg. loss 0.00105 time elapsed 22.70sec\n",
      "epoch 92, iter 148900, avg. loss 0.00105 time elapsed 22.76sec\n",
      "epoch 92, iter 149000, avg. loss 0.00104 time elapsed 23.21sec\n",
      "epoch 92, iter 149100, avg. loss 0.00105 time elapsed 22.74sec\n",
      "epoch 92, iter 149200, avg. loss 0.00105 time elapsed 23.03sec\n",
      "epoch 92, iter 149300, avg. loss 0.00105 time elapsed 23.21sec\n",
      "epoch 92, iter 149400, avg. loss 0.00104 time elapsed 23.42sec\n",
      "epoch 92, iter 149500, avg. loss 0.00105 time elapsed 22.74sec\n",
      "epoch 92, iter 149600, avg. loss 0.00104 time elapsed 23.11sec\n",
      "epoch 92, iter 149700, avg. loss 0.00105 time elapsed 22.95sec\n",
      "==============================\n",
      "epoch 93, iter 149800, avg. loss 0.00105 time elapsed 37.19sec\n",
      "epoch 93, iter 149900, avg. loss 0.00105 time elapsed 22.74sec\n",
      "epoch 93, iter 150000, avg. loss 0.00105 time elapsed 23.20sec\n",
      "epoch 93, iter 150000, cum. loss 0.00105 examples 5120000.0\n",
      "begin evaluation...\n",
      "validation: iter 150000, dev. acc 0.7147\n",
      "save currently the best model to [model_v5/bert.checkpointBest_epoch_93.checkpoint]\n",
      "epoch 93, iter 150100, avg. loss 0.00104 time elapsed 69.72sec\n",
      "epoch 93, iter 150200, avg. loss 0.00105 time elapsed 22.62sec\n",
      "epoch 93, iter 150300, avg. loss 0.00104 time elapsed 22.97sec\n",
      "epoch 93, iter 150400, avg. loss 0.00105 time elapsed 22.73sec\n",
      "epoch 93, iter 150500, avg. loss 0.00105 time elapsed 22.79sec\n",
      "epoch 93, iter 150600, avg. loss 0.00105 time elapsed 22.82sec\n",
      "epoch 93, iter 150700, avg. loss 0.00104 time elapsed 22.58sec\n",
      "epoch 93, iter 150800, avg. loss 0.00104 time elapsed 22.81sec\n",
      "epoch 93, iter 150900, avg. loss 0.00104 time elapsed 22.97sec\n",
      "epoch 93, iter 151000, avg. loss 0.00104 time elapsed 23.15sec\n",
      "epoch 93, iter 151100, avg. loss 0.00105 time elapsed 23.11sec\n",
      "epoch 93, iter 151200, avg. loss 0.00104 time elapsed 22.70sec\n",
      "epoch 93, iter 151300, avg. loss 0.00104 time elapsed 22.77sec\n",
      "epoch 93, iter 151400, avg. loss 0.00105 time elapsed 23.21sec\n",
      "==============================\n",
      "epoch 94, iter 151500, avg. loss 0.00104 time elapsed 80.03sec\n",
      "epoch 94, iter 151600, avg. loss 0.00105 time elapsed 22.90sec\n",
      "epoch 94, iter 151700, avg. loss 0.00105 time elapsed 22.76sec\n",
      "epoch 94, iter 151800, avg. loss 0.00105 time elapsed 22.65sec\n",
      "epoch 94, iter 151900, avg. loss 0.00104 time elapsed 23.01sec\n",
      "epoch 94, iter 152000, avg. loss 0.00105 time elapsed 23.10sec\n",
      "epoch 94, iter 152100, avg. loss 0.00104 time elapsed 22.79sec\n",
      "epoch 94, iter 152200, avg. loss 0.00105 time elapsed 22.82sec\n",
      "epoch 94, iter 152300, avg. loss 0.00104 time elapsed 23.04sec\n",
      "epoch 94, iter 152400, avg. loss 0.00104 time elapsed 22.80sec\n",
      "epoch 94, iter 152500, avg. loss 0.00104 time elapsed 22.65sec\n",
      "epoch 94, iter 152600, avg. loss 0.00104 time elapsed 22.83sec\n",
      "epoch 94, iter 152700, avg. loss 0.00104 time elapsed 22.73sec\n",
      "epoch 94, iter 152800, avg. loss 0.00105 time elapsed 22.82sec\n",
      "epoch 94, iter 152900, avg. loss 0.00104 time elapsed 22.93sec\n",
      "epoch 94, iter 153000, avg. loss 0.00105 time elapsed 23.21sec\n",
      "==============================\n",
      "epoch 95, iter 153100, avg. loss 0.00104 time elapsed 50.25sec\n",
      "epoch 95, iter 153200, avg. loss 0.00104 time elapsed 22.55sec\n",
      "epoch 95, iter 153300, avg. loss 0.00104 time elapsed 22.84sec\n",
      "epoch 95, iter 153400, avg. loss 0.00105 time elapsed 23.28sec\n",
      "epoch 95, iter 153500, avg. loss 0.00104 time elapsed 22.69sec\n",
      "epoch 95, iter 153600, avg. loss 0.00104 time elapsed 22.96sec\n",
      "epoch 95, iter 153700, avg. loss 0.00105 time elapsed 22.98sec\n",
      "epoch 95, iter 153800, avg. loss 0.00104 time elapsed 22.71sec\n",
      "epoch 95, iter 153900, avg. loss 0.00104 time elapsed 22.48sec\n",
      "epoch 95, iter 154000, avg. loss 0.00104 time elapsed 22.56sec\n",
      "epoch 95, iter 154100, avg. loss 0.00104 time elapsed 22.86sec\n",
      "epoch 95, iter 154200, avg. loss 0.00104 time elapsed 22.67sec\n",
      "epoch 95, iter 154300, avg. loss 0.00104 time elapsed 22.90sec\n",
      "epoch 95, iter 154400, avg. loss 0.00104 time elapsed 22.72sec\n",
      "epoch 95, iter 154500, avg. loss 0.00104 time elapsed 22.54sec\n",
      "epoch 95, iter 154600, avg. loss 0.00104 time elapsed 22.58sec\n",
      "==============================\n",
      "epoch 96, iter 154700, avg. loss 0.00104 time elapsed 63.05sec\n",
      "epoch 96, iter 154800, avg. loss 0.00104 time elapsed 22.84sec\n",
      "epoch 96, iter 154900, avg. loss 0.00104 time elapsed 22.88sec\n",
      "epoch 96, iter 155000, avg. loss 0.00104 time elapsed 22.77sec\n",
      "epoch 96, iter 155000, cum. loss 0.00104 examples 5120000.0\n",
      "begin evaluation...\n",
      "validation: iter 155000, dev. acc 0.7172\n",
      "save currently the best model to [model_v5/bert.checkpointBest_epoch_96.checkpoint]\n",
      "epoch 96, iter 155100, avg. loss 0.00104 time elapsed 57.73sec\n",
      "epoch 96, iter 155200, avg. loss 0.00104 time elapsed 22.89sec\n",
      "epoch 96, iter 155300, avg. loss 0.00104 time elapsed 22.56sec\n",
      "epoch 96, iter 155400, avg. loss 0.00104 time elapsed 22.73sec\n",
      "epoch 96, iter 155500, avg. loss 0.00104 time elapsed 22.89sec\n",
      "epoch 96, iter 155600, avg. loss 0.00104 time elapsed 22.81sec\n",
      "epoch 96, iter 155700, avg. loss 0.00104 time elapsed 22.71sec\n",
      "epoch 96, iter 155800, avg. loss 0.00104 time elapsed 22.73sec\n",
      "epoch 96, iter 155900, avg. loss 0.00104 time elapsed 22.63sec\n",
      "epoch 96, iter 156000, avg. loss 0.00104 time elapsed 22.83sec\n",
      "epoch 96, iter 156100, avg. loss 0.00104 time elapsed 23.06sec\n",
      "epoch 96, iter 156200, avg. loss 0.00104 time elapsed 22.67sec\n",
      "epoch 96, iter 156300, avg. loss 0.00104 time elapsed 22.57sec\n",
      "==============================\n",
      "epoch 97, iter 156400, avg. loss 0.00104 time elapsed 54.77sec\n",
      "epoch 97, iter 156500, avg. loss 0.00104 time elapsed 22.38sec\n",
      "epoch 97, iter 156600, avg. loss 0.00104 time elapsed 22.70sec\n",
      "epoch 97, iter 156700, avg. loss 0.00104 time elapsed 23.15sec\n",
      "epoch 97, iter 156800, avg. loss 0.00104 time elapsed 22.53sec\n",
      "epoch 97, iter 156900, avg. loss 0.00104 time elapsed 23.05sec\n",
      "epoch 97, iter 157000, avg. loss 0.00104 time elapsed 22.78sec\n",
      "epoch 97, iter 157100, avg. loss 0.00104 time elapsed 22.77sec\n",
      "epoch 97, iter 157200, avg. loss 0.00104 time elapsed 23.00sec\n",
      "epoch 97, iter 157300, avg. loss 0.00104 time elapsed 22.75sec\n",
      "epoch 97, iter 157400, avg. loss 0.00104 time elapsed 22.78sec\n",
      "epoch 97, iter 157500, avg. loss 0.00104 time elapsed 22.80sec\n",
      "epoch 97, iter 157600, avg. loss 0.00104 time elapsed 22.67sec\n",
      "epoch 97, iter 157700, avg. loss 0.00104 time elapsed 22.81sec\n",
      "epoch 97, iter 157800, avg. loss 0.00104 time elapsed 22.76sec\n",
      "epoch 97, iter 157900, avg. loss 0.00104 time elapsed 22.91sec\n",
      "==============================\n",
      "epoch 98, iter 158000, avg. loss 0.00104 time elapsed 71.47sec\n",
      "epoch 98, iter 158100, avg. loss 0.00104 time elapsed 22.90sec\n",
      "epoch 98, iter 158200, avg. loss 0.00104 time elapsed 22.69sec\n",
      "epoch 98, iter 158300, avg. loss 0.00104 time elapsed 23.02sec\n",
      "epoch 98, iter 158400, avg. loss 0.00104 time elapsed 22.87sec\n",
      "epoch 98, iter 158500, avg. loss 0.00104 time elapsed 22.94sec\n",
      "epoch 98, iter 158600, avg. loss 0.00104 time elapsed 22.62sec\n",
      "epoch 98, iter 158700, avg. loss 0.00104 time elapsed 22.80sec\n",
      "epoch 98, iter 158800, avg. loss 0.00104 time elapsed 23.05sec\n",
      "epoch 98, iter 158900, avg. loss 0.00104 time elapsed 22.70sec\n",
      "epoch 98, iter 159000, avg. loss 0.00104 time elapsed 23.06sec\n",
      "epoch 98, iter 159100, avg. loss 0.00104 time elapsed 22.90sec\n",
      "epoch 98, iter 159200, avg. loss 0.00104 time elapsed 22.63sec\n",
      "epoch 98, iter 159300, avg. loss 0.00104 time elapsed 22.96sec\n",
      "epoch 98, iter 159400, avg. loss 0.00104 time elapsed 22.82sec\n",
      "epoch 98, iter 159500, avg. loss 0.00104 time elapsed 23.06sec\n",
      "==============================\n",
      "epoch 99, iter 159600, avg. loss 0.00104 time elapsed 62.56sec\n",
      "epoch 99, iter 159700, avg. loss 0.00103 time elapsed 22.70sec\n",
      "epoch 99, iter 159800, avg. loss 0.00104 time elapsed 22.82sec\n",
      "epoch 99, iter 159900, avg. loss 0.00104 time elapsed 22.81sec\n",
      "epoch 99, iter 160000, avg. loss 0.00104 time elapsed 23.22sec\n",
      "epoch 99, iter 160000, cum. loss 0.00104 examples 5120000.0\n",
      "begin evaluation...\n",
      "validation: iter 160000, dev. acc 0.7190\n",
      "save currently the best model to [model_v5/bert.checkpointBest_epoch_99.checkpoint]\n",
      "epoch 99, iter 160100, avg. loss 0.00103 time elapsed 55.98sec\n",
      "epoch 99, iter 160200, avg. loss 0.00104 time elapsed 22.86sec\n",
      "epoch 99, iter 160300, avg. loss 0.00104 time elapsed 22.70sec\n",
      "epoch 99, iter 160400, avg. loss 0.00104 time elapsed 22.82sec\n",
      "epoch 99, iter 160500, avg. loss 0.00104 time elapsed 22.66sec\n",
      "epoch 99, iter 160600, avg. loss 0.00104 time elapsed 22.74sec\n",
      "epoch 99, iter 160700, avg. loss 0.00104 time elapsed 23.08sec\n",
      "epoch 99, iter 160800, avg. loss 0.00104 time elapsed 23.02sec\n",
      "epoch 99, iter 160900, avg. loss 0.00104 time elapsed 22.77sec\n",
      "epoch 99, iter 161000, avg. loss 0.00103 time elapsed 22.69sec\n",
      "epoch 99, iter 161100, avg. loss 0.00104 time elapsed 22.78sec\n",
      "epoch 99, iter 161200, avg. loss 0.00104 time elapsed 22.52sec\n",
      "==============================\n",
      "epoch 100, iter 161300, avg. loss 0.00104 time elapsed 62.91sec\n",
      "epoch 100, iter 161400, avg. loss 0.00104 time elapsed 22.76sec\n",
      "epoch 100, iter 161500, avg. loss 0.00103 time elapsed 22.96sec\n",
      "epoch 100, iter 161600, avg. loss 0.00104 time elapsed 22.76sec\n",
      "epoch 100, iter 161700, avg. loss 0.00104 time elapsed 22.91sec\n",
      "epoch 100, iter 161800, avg. loss 0.00104 time elapsed 22.80sec\n",
      "epoch 100, iter 161900, avg. loss 0.00104 time elapsed 22.72sec\n",
      "epoch 100, iter 162000, avg. loss 0.00104 time elapsed 22.95sec\n",
      "epoch 100, iter 162100, avg. loss 0.00104 time elapsed 22.79sec\n",
      "epoch 100, iter 162200, avg. loss 0.00104 time elapsed 22.85sec\n",
      "epoch 100, iter 162300, avg. loss 0.00104 time elapsed 22.85sec\n",
      "epoch 100, iter 162400, avg. loss 0.00104 time elapsed 22.79sec\n",
      "epoch 100, iter 162500, avg. loss 0.00104 time elapsed 22.90sec\n",
      "epoch 100, iter 162600, avg. loss 0.00104 time elapsed 22.79sec\n",
      "epoch 100, iter 162700, avg. loss 0.00104 time elapsed 22.78sec\n",
      "epoch 100, iter 162800, avg. loss 0.00104 time elapsed 22.67sec\n",
      "==============================\n",
      "epoch 101, iter 162900, avg. loss 0.00104 time elapsed 76.14sec\n",
      "epoch 101, iter 163000, avg. loss 0.00104 time elapsed 22.85sec\n",
      "epoch 101, iter 163100, avg. loss 0.00104 time elapsed 22.66sec\n",
      "epoch 101, iter 163200, avg. loss 0.00103 time elapsed 22.95sec\n",
      "epoch 101, iter 163300, avg. loss 0.00103 time elapsed 22.62sec\n",
      "epoch 101, iter 163400, avg. loss 0.00103 time elapsed 22.80sec\n",
      "epoch 101, iter 163500, avg. loss 0.00104 time elapsed 23.02sec\n",
      "epoch 101, iter 163600, avg. loss 0.00103 time elapsed 22.70sec\n",
      "epoch 101, iter 163700, avg. loss 0.00103 time elapsed 23.22sec\n",
      "epoch 101, iter 163800, avg. loss 0.00104 time elapsed 22.99sec\n",
      "epoch 101, iter 163900, avg. loss 0.00103 time elapsed 22.59sec\n",
      "epoch 101, iter 164000, avg. loss 0.00103 time elapsed 22.59sec\n",
      "epoch 101, iter 164100, avg. loss 0.00104 time elapsed 22.89sec\n",
      "epoch 101, iter 164200, avg. loss 0.00103 time elapsed 22.85sec\n",
      "epoch 101, iter 164300, avg. loss 0.00103 time elapsed 22.45sec\n",
      "epoch 101, iter 164400, avg. loss 0.00104 time elapsed 23.00sec\n",
      "==============================\n",
      "epoch 102, iter 164500, avg. loss 0.00103 time elapsed 50.09sec\n",
      "epoch 102, iter 164600, avg. loss 0.00103 time elapsed 22.40sec\n",
      "epoch 102, iter 164700, avg. loss 0.00103 time elapsed 22.71sec\n",
      "epoch 102, iter 164800, avg. loss 0.00103 time elapsed 23.03sec\n",
      "epoch 102, iter 164900, avg. loss 0.00104 time elapsed 22.88sec\n",
      "epoch 102, iter 165000, avg. loss 0.00104 time elapsed 22.89sec\n",
      "epoch 102, iter 165000, cum. loss 0.00104 examples 5120000.0\n",
      "begin evaluation...\n",
      "validation: iter 165000, dev. acc 0.7196\n",
      "save currently the best model to [model_v5/bert.checkpointBest_epoch_102.checkpoint]\n",
      "epoch 102, iter 165100, avg. loss 0.00104 time elapsed 56.30sec\n",
      "epoch 102, iter 165200, avg. loss 0.00103 time elapsed 22.90sec\n",
      "epoch 102, iter 165300, avg. loss 0.00103 time elapsed 22.82sec\n",
      "epoch 102, iter 165400, avg. loss 0.00103 time elapsed 22.93sec\n",
      "epoch 102, iter 165500, avg. loss 0.00103 time elapsed 22.90sec\n",
      "epoch 102, iter 165600, avg. loss 0.00104 time elapsed 22.84sec\n",
      "epoch 102, iter 165700, avg. loss 0.00104 time elapsed 22.54sec\n",
      "epoch 102, iter 165800, avg. loss 0.00104 time elapsed 22.91sec\n",
      "epoch 102, iter 165900, avg. loss 0.00104 time elapsed 22.94sec\n",
      "epoch 102, iter 166000, avg. loss 0.00103 time elapsed 22.97sec\n",
      "==============================\n",
      "epoch 103, iter 166100, avg. loss 0.00104 time elapsed 50.12sec\n",
      "epoch 103, iter 166200, avg. loss 0.00103 time elapsed 22.19sec\n",
      "epoch 103, iter 166300, avg. loss 0.00103 time elapsed 22.72sec\n",
      "epoch 103, iter 166400, avg. loss 0.00103 time elapsed 22.85sec\n",
      "epoch 103, iter 166500, avg. loss 0.00103 time elapsed 22.76sec\n",
      "epoch 103, iter 166600, avg. loss 0.00103 time elapsed 23.20sec\n",
      "epoch 103, iter 166700, avg. loss 0.00104 time elapsed 22.63sec\n",
      "epoch 103, iter 166800, avg. loss 0.00104 time elapsed 22.81sec\n",
      "epoch 103, iter 166900, avg. loss 0.00103 time elapsed 22.79sec\n",
      "epoch 103, iter 167000, avg. loss 0.00103 time elapsed 22.90sec\n",
      "epoch 103, iter 167100, avg. loss 0.00103 time elapsed 22.89sec\n",
      "epoch 103, iter 167200, avg. loss 0.00103 time elapsed 22.65sec\n",
      "epoch 103, iter 167300, avg. loss 0.00103 time elapsed 23.06sec\n",
      "epoch 103, iter 167400, avg. loss 0.00103 time elapsed 23.03sec\n",
      "epoch 103, iter 167500, avg. loss 0.00103 time elapsed 22.73sec\n",
      "epoch 103, iter 167600, avg. loss 0.00103 time elapsed 22.62sec\n",
      "epoch 103, iter 167700, avg. loss 0.00103 time elapsed 22.61sec\n",
      "==============================\n",
      "epoch 104, iter 167800, avg. loss 0.00103 time elapsed 75.26sec\n",
      "epoch 104, iter 167900, avg. loss 0.00103 time elapsed 22.65sec\n",
      "epoch 104, iter 168000, avg. loss 0.00103 time elapsed 22.77sec\n",
      "epoch 104, iter 168100, avg. loss 0.00103 time elapsed 22.74sec\n",
      "epoch 104, iter 168200, avg. loss 0.00103 time elapsed 22.74sec\n",
      "epoch 104, iter 168300, avg. loss 0.00103 time elapsed 22.87sec\n",
      "epoch 104, iter 168400, avg. loss 0.00103 time elapsed 22.80sec\n",
      "epoch 104, iter 168500, avg. loss 0.00103 time elapsed 22.52sec\n",
      "epoch 104, iter 168600, avg. loss 0.00103 time elapsed 22.80sec\n",
      "epoch 104, iter 168700, avg. loss 0.00103 time elapsed 23.23sec\n",
      "epoch 104, iter 168800, avg. loss 0.00103 time elapsed 22.69sec\n",
      "epoch 104, iter 168900, avg. loss 0.00104 time elapsed 22.87sec\n",
      "epoch 104, iter 169000, avg. loss 0.00103 time elapsed 22.85sec\n",
      "epoch 104, iter 169100, avg. loss 0.00103 time elapsed 22.94sec\n",
      "epoch 104, iter 169200, avg. loss 0.00103 time elapsed 22.92sec\n",
      "epoch 104, iter 169300, avg. loss 0.00103 time elapsed 22.87sec\n",
      "==============================\n",
      "epoch 105, iter 169400, avg. loss 0.00103 time elapsed 55.79sec\n",
      "epoch 105, iter 169500, avg. loss 0.00103 time elapsed 22.76sec\n",
      "epoch 105, iter 169600, avg. loss 0.00103 time elapsed 22.72sec\n",
      "epoch 105, iter 169700, avg. loss 0.00103 time elapsed 22.80sec\n",
      "epoch 105, iter 169800, avg. loss 0.00103 time elapsed 22.79sec\n",
      "epoch 105, iter 169900, avg. loss 0.00103 time elapsed 22.80sec\n",
      "epoch 105, iter 170000, avg. loss 0.00103 time elapsed 22.74sec\n",
      "epoch 105, iter 170000, cum. loss 0.00103 examples 5120000.0\n",
      "begin evaluation...\n",
      "validation: iter 170000, dev. acc 0.7214\n",
      "save currently the best model to [model_v5/bert.checkpointBest_epoch_105.checkpoint]\n",
      "epoch 105, iter 170100, avg. loss 0.00103 time elapsed 54.41sec\n",
      "epoch 105, iter 170200, avg. loss 0.00103 time elapsed 22.90sec\n",
      "epoch 105, iter 170300, avg. loss 0.00103 time elapsed 22.79sec\n",
      "epoch 105, iter 170400, avg. loss 0.00103 time elapsed 22.97sec\n",
      "epoch 105, iter 170500, avg. loss 0.00103 time elapsed 22.98sec\n",
      "epoch 105, iter 170600, avg. loss 0.00103 time elapsed 22.79sec\n",
      "epoch 105, iter 170700, avg. loss 0.00103 time elapsed 22.96sec\n",
      "epoch 105, iter 170800, avg. loss 0.00103 time elapsed 23.03sec\n",
      "epoch 105, iter 170900, avg. loss 0.00103 time elapsed 23.33sec\n",
      "==============================\n",
      "epoch 106, iter 171000, avg. loss 0.00103 time elapsed 60.03sec\n",
      "epoch 106, iter 171100, avg. loss 0.00103 time elapsed 22.53sec\n",
      "epoch 106, iter 171200, avg. loss 0.00103 time elapsed 22.73sec\n",
      "epoch 106, iter 171300, avg. loss 0.00103 time elapsed 22.99sec\n",
      "epoch 106, iter 171400, avg. loss 0.00103 time elapsed 22.78sec\n",
      "epoch 106, iter 171500, avg. loss 0.00103 time elapsed 22.73sec\n",
      "epoch 106, iter 171600, avg. loss 0.00103 time elapsed 22.72sec\n",
      "epoch 106, iter 171700, avg. loss 0.00103 time elapsed 23.02sec\n",
      "epoch 106, iter 171800, avg. loss 0.00103 time elapsed 22.94sec\n",
      "epoch 106, iter 171900, avg. loss 0.00103 time elapsed 22.87sec\n",
      "epoch 106, iter 172000, avg. loss 0.00103 time elapsed 22.55sec\n",
      "epoch 106, iter 172100, avg. loss 0.00103 time elapsed 22.80sec\n",
      "epoch 106, iter 172200, avg. loss 0.00103 time elapsed 23.00sec\n",
      "epoch 106, iter 172300, avg. loss 0.00103 time elapsed 23.16sec\n",
      "epoch 106, iter 172400, avg. loss 0.00103 time elapsed 23.14sec\n",
      "epoch 106, iter 172500, avg. loss 0.00103 time elapsed 22.86sec\n",
      "epoch 106, iter 172600, avg. loss 0.00103 time elapsed 22.76sec\n",
      "==============================\n",
      "epoch 107, iter 172700, avg. loss 0.00103 time elapsed 55.42sec\n",
      "epoch 107, iter 172800, avg. loss 0.00103 time elapsed 22.89sec\n",
      "epoch 107, iter 172900, avg. loss 0.00103 time elapsed 22.94sec\n",
      "epoch 107, iter 173000, avg. loss 0.00103 time elapsed 22.98sec\n",
      "epoch 107, iter 173100, avg. loss 0.00103 time elapsed 23.05sec\n",
      "epoch 107, iter 173200, avg. loss 0.00103 time elapsed 23.05sec\n",
      "epoch 107, iter 173300, avg. loss 0.00103 time elapsed 22.82sec\n",
      "epoch 107, iter 173400, avg. loss 0.00103 time elapsed 22.95sec\n",
      "epoch 107, iter 173500, avg. loss 0.00103 time elapsed 22.78sec\n",
      "epoch 107, iter 173600, avg. loss 0.00103 time elapsed 22.82sec\n",
      "epoch 107, iter 173700, avg. loss 0.00103 time elapsed 23.17sec\n",
      "epoch 107, iter 173800, avg. loss 0.00103 time elapsed 22.89sec\n",
      "epoch 107, iter 173900, avg. loss 0.00103 time elapsed 23.05sec\n",
      "epoch 107, iter 174000, avg. loss 0.00103 time elapsed 22.76sec\n",
      "epoch 107, iter 174100, avg. loss 0.00103 time elapsed 22.53sec\n",
      "epoch 107, iter 174200, avg. loss 0.00103 time elapsed 22.89sec\n",
      "==============================\n",
      "epoch 108, iter 174300, avg. loss 0.00102 time elapsed 72.70sec\n",
      "epoch 108, iter 174400, avg. loss 0.00103 time elapsed 22.54sec\n",
      "epoch 108, iter 174500, avg. loss 0.00103 time elapsed 22.95sec\n",
      "epoch 108, iter 174600, avg. loss 0.00103 time elapsed 22.86sec\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_predict(model, input_str, vocab, mask_token=mask_token):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    word = list(input_str.replace(\" \", \"\"))\n",
    "    \n",
    "    vocab_size = len(vocab.char2id)\n",
    "    word_token = torch.from_numpy(np.array([vocab.char2id[c] for c in word])).unsqueeze(0).to(model.device)\n",
    "    word_token_mask = word_token.unsqueeze(1).to(model.device)\n",
    "    print(word_token, word_token_mask)\n",
    "    out = model.forward(word_token, word_token_mask)\n",
    "    generator_mask = torch.zeros(word_token.shape[0], vocab_size, device=model.device)\n",
    "    generator_mask = generator_mask.scatter_(1, word_token, mask_token)\n",
    "\n",
    "    #batch_loss = loss_compute(out, batch.tgt, generator_mask)\n",
    "    x = model.generator(out, generator_mask)\n",
    "    x = x.masked_fill(generator_mask == mask_token, -1e9)\n",
    "    x = (torch.argmax((nn.Softmax(dim=1)(x))).cpu().numpy()).astype(int)\n",
    "    x = vocab.id2char[int(x)]\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_str = \"a p p l _ \"\n",
    "model_predict(model, input_str, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
