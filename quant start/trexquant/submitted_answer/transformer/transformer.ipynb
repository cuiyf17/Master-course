{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vocabulary import Vocab\n",
    "from utils import *\n",
    "from embedding import PositionalEncoding, Embeddings\n",
    "from layers import *\n",
    "from criterion import KLLossMasked\n",
    "from optimizer import NoamOpt\n",
    "from bert import Bert, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_every_iter = 100\n",
    "validate_every_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = 'model/'\n",
    "if not os.path.isdir(directory):\n",
    "    os.mkdir(directory)\n",
    "model_save_path = 'bert.checkpoint'\n",
    "model_save_path = os.path.join(directory, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_size = False\n",
    "use_checkpoint = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "vocab = Vocab()\n",
    "V = len(vocab.char2id)\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "h = 8\n",
    "n_encoders = 6\n",
    "\n",
    "batch_size = 1024\n",
    "num_epochs = 100\n",
    "device_id = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_attn = MultiHeadedAttention(h=h, d_model=d_model, d_k=d_model // h, d_v=d_model // h, dropout=0.1)\n",
    "feed_forward = FullyConnectedFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "position = PositionalEncoding(d_model, dropout=0.1)\n",
    "embedding = nn.Sequential(Embeddings(d_model=d_model, vocab=V), position)\n",
    "\n",
    "encoder = Encoder(self_attn=self_attn, feed_forward=feed_forward, size=d_model, dropout=0.1)\n",
    "generator = Generator(d_model=d_model, vocab_size=V)\n",
    "model = Bert(encoder=encoder, embedding=embedding, generator=generator, n_layers=n_encoders)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-9, betas=(0.9, 0.98), eps=1e-9)\n",
    "model_opt = NoamOpt(d_model, 2, 4000, opt)\n",
    "criterion = nn.KLDivLoss(reduction=\"batchmean\").to(device)\n",
    "#criterion = nn.CrossEntropyLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = read_train_data(filepath=\"./pairs_train.txt\", small = small_size)\n",
    "dev_data = read_dev_data(filepath=\"./pairs_valid.txt\", small = small_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_valid_scores = []\n",
    "\n",
    "if use_checkpoint:\n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    step = checkpoint['_step']\n",
    "    rate = checkpoint['_rate']\n",
    "    current_train_iter = checkpoint['train_iter']\n",
    "    model_opt._step = step\n",
    "    model_opt._rate = rate\n",
    "    print(f'reading checkpoint from epoch {current_epoch}, iter {current_train_iter}')\n",
    "else:\n",
    "    current_epoch = 0\n",
    "    current_train_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    train_iter = report_loss = cum_loss = valid_num = 0\n",
    "    report_samples = cum_samples = 0\n",
    "    for epoch in range(current_epoch, num_epochs):\n",
    "        print(\"=\" * 30)\n",
    "        model.train()\n",
    "        #loss_compute = KLLossMasked(model.generator, criterion, opt=model_opt)\n",
    "\n",
    "        start = time.time()\n",
    "        train_data_iter = create_words_batch(train_data, vocab, mini_batch=batch_size, shuffle=False, device=device)\n",
    "        for i, batch in enumerate(train_data_iter):\n",
    "            if use_checkpoint and train_iter <= current_train_iter:\n",
    "                train_iter = current_train_iter\n",
    "                continue\n",
    "            out = model.forward(batch.src, batch.src_mask)\n",
    "            generator_mask = torch.zeros(batch.src.shape[0], V, device=device)\n",
    "            generator_mask = generator_mask.scatter_(1, batch.src, mask_token)\n",
    "\n",
    "            #batch_loss = loss_compute(out, batch.tgt, generator_mask)\n",
    "            x = model.generator(out, generator_mask)\n",
    "            #x = x.masked_fill(generator_mask == mask_token, -1e9)\n",
    "            x = nn.LogSoftmax(dim=1)(x)\n",
    "            y = batch.tgt.masked_fill(generator_mask == mask_token, 0)\n",
    "            y = y/(torch.sum(y, dim=1, keepdim=True) + 1e-12)\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_loss = criterion(x, y)\n",
    "            loss = batch_loss\n",
    "            loss.backward()\n",
    "            model_opt.step()\n",
    "            model_opt.optimizer.zero_grad()\n",
    "\n",
    "            #print(train_iter, loss.item(), torch.any(torch.isnan(out)), torch.any(torch.isnan(x)), torch.any(torch.isnan(y)), torch.all(y==0))\n",
    "            #if(torch.any(torch.isnan(loss))):\n",
    "            #    print(out)\n",
    "            #    print(batch.src)\n",
    "            #    print(batch.src_mask)\n",
    "            #    print(batch.tgt)\n",
    "            #    print(generator_mask)\n",
    "            #    print(\"x\", x)\n",
    "            #    print(\"y\", y)\n",
    "            #    exit()\n",
    "            batch_loss_val = batch_loss.item()\n",
    "            report_loss += batch_loss_val\n",
    "            cum_loss += batch_loss_val\n",
    "            report_samples += batch_size\n",
    "            cum_samples += batch_size\n",
    "\n",
    "            train_iter += 1\n",
    "\n",
    "            if train_iter % log_every_iter == 0:\n",
    "                elapsed = time.time() - start\n",
    "                print(f'epoch {epoch}, iter {train_iter}, avg. loss {report_loss / report_samples:.5f} time elapsed {elapsed:.2f}sec')\n",
    "                start = time.time()\n",
    "                report_loss = report_samples = 0\n",
    "\n",
    "            if train_iter % validate_every_iter == 0:\n",
    "                print(f'epoch {epoch}, iter {train_iter}, cum. loss {cum_loss / cum_samples:.5f} examples {cum_samples}')\n",
    "                cum_samples = cum_loss = 0.\n",
    "\n",
    "                print('begin evaluation...')\n",
    "                valid_num += 1\n",
    "                acc = evaluate_acc(model, vocab, dev_data, device=model.device)\n",
    "                print(f'validation: iter {train_iter}, dev. acc {acc:.4f}')\n",
    "\n",
    "                valid_metric = acc\n",
    "\n",
    "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
    "                hist_valid_scores.append(valid_metric)\n",
    "\n",
    "                if is_better:\n",
    "                    print('save currently the best model to [%s]' % (model_save_path + \"Best_epoch_%d.checkpoint\"%(epoch)))\n",
    "                    torch.save({'epoch': epoch,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': model_opt.optimizer.state_dict(),\n",
    "                                'loss': cum_loss,\n",
    "                                '_rate': model_opt._rate,\n",
    "                                '_step': model_opt._step,\n",
    "                                'train_iter': train_iter,\n",
    "                                'hist_valid_scores': hist_valid_scores,\n",
    "                                },  model_save_path + \"Best_epoch_%d.checkpoint\"%(epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "epoch 0, iter 100, avg. loss 0.00201 time elapsed 23.30sec\n",
      "epoch 0, iter 200, avg. loss 0.00164 time elapsed 22.12sec\n",
      "epoch 0, iter 300, avg. loss 0.00158 time elapsed 22.44sec\n",
      "epoch 0, iter 400, avg. loss 0.00154 time elapsed 22.70sec\n",
      "epoch 0, iter 500, avg. loss 0.00152 time elapsed 22.72sec\n",
      "epoch 0, iter 600, avg. loss 0.00151 time elapsed 22.41sec\n",
      "epoch 0, iter 700, avg. loss 0.00149 time elapsed 22.23sec\n",
      "epoch 0, iter 800, avg. loss 0.00148 time elapsed 21.87sec\n",
      "epoch 0, iter 900, avg. loss 0.00146 time elapsed 22.03sec\n",
      "epoch 0, iter 1000, avg. loss 0.00145 time elapsed 22.81sec\n",
      "epoch 0, iter 1100, avg. loss 0.00144 time elapsed 22.39sec\n",
      "epoch 0, iter 1200, avg. loss 0.00143 time elapsed 21.89sec\n",
      "epoch 0, iter 1300, avg. loss 0.00142 time elapsed 22.03sec\n",
      "epoch 0, iter 1400, avg. loss 0.00141 time elapsed 22.11sec\n",
      "epoch 0, iter 1500, avg. loss 0.00140 time elapsed 21.99sec\n",
      "==============================\n",
      "epoch 1, iter 1600, avg. loss 0.00139 time elapsed 2.65sec\n",
      "epoch 1, iter 1700, avg. loss 0.00139 time elapsed 22.32sec\n",
      "epoch 1, iter 1800, avg. loss 0.00138 time elapsed 22.09sec\n",
      "epoch 1, iter 1900, avg. loss 0.00137 time elapsed 22.35sec\n",
      "epoch 1, iter 2000, avg. loss 0.00136 time elapsed 22.29sec\n",
      "epoch 1, iter 2100, avg. loss 0.00136 time elapsed 22.05sec\n",
      "epoch 1, iter 2200, avg. loss 0.00136 time elapsed 22.87sec\n",
      "epoch 1, iter 2300, avg. loss 0.00135 time elapsed 22.46sec\n",
      "epoch 1, iter 2400, avg. loss 0.00135 time elapsed 22.76sec\n",
      "epoch 1, iter 2500, avg. loss 0.00135 time elapsed 22.95sec\n",
      "epoch 1, iter 2600, avg. loss 0.00134 time elapsed 22.97sec\n",
      "epoch 1, iter 2700, avg. loss 0.00134 time elapsed 23.36sec\n",
      "epoch 1, iter 2800, avg. loss 0.00135 time elapsed 23.00sec\n",
      "epoch 1, iter 2900, avg. loss 0.00135 time elapsed 22.02sec\n",
      "epoch 1, iter 3000, avg. loss 0.00135 time elapsed 22.58sec\n",
      "epoch 1, iter 3100, avg. loss 0.00136 time elapsed 22.06sec\n",
      "==============================\n",
      "epoch 2, iter 3200, avg. loss 0.00136 time elapsed 5.41sec\n",
      "epoch 2, iter 3300, avg. loss 0.00137 time elapsed 22.28sec\n",
      "epoch 2, iter 3400, avg. loss 0.00138 time elapsed 21.79sec\n",
      "epoch 2, iter 3500, avg. loss 0.00140 time elapsed 22.42sec\n",
      "epoch 2, iter 3600, avg. loss 0.00140 time elapsed 22.24sec\n",
      "epoch 2, iter 3700, avg. loss 0.00143 time elapsed 22.15sec\n",
      "epoch 2, iter 3800, avg. loss 0.00144 time elapsed 21.86sec\n",
      "epoch 2, iter 3900, avg. loss 0.00145 time elapsed 22.28sec\n",
      "epoch 2, iter 4000, avg. loss 0.00147 time elapsed 22.59sec\n",
      "epoch 2, iter 4100, avg. loss 0.00148 time elapsed 22.55sec\n",
      "epoch 2, iter 4200, avg. loss 0.00150 time elapsed 22.65sec\n",
      "epoch 2, iter 4300, avg. loss 0.00150 time elapsed 22.88sec\n",
      "epoch 2, iter 4400, avg. loss 0.00151 time elapsed 22.20sec\n",
      "epoch 2, iter 4500, avg. loss 0.00151 time elapsed 22.06sec\n",
      "epoch 2, iter 4600, avg. loss 0.00152 time elapsed 22.13sec\n",
      "epoch 2, iter 4700, avg. loss 0.00153 time elapsed 22.64sec\n",
      "==============================\n",
      "epoch 3, iter 4800, avg. loss 0.00152 time elapsed 8.34sec\n",
      "epoch 3, iter 4900, avg. loss 0.00152 time elapsed 23.22sec\n",
      "epoch 3, iter 5000, avg. loss 0.00152 time elapsed 22.92sec\n",
      "epoch 3, iter 5100, avg. loss 0.00152 time elapsed 23.33sec\n",
      "epoch 3, iter 5200, avg. loss 0.00151 time elapsed 23.14sec\n",
      "epoch 3, iter 5300, avg. loss 0.00151 time elapsed 21.99sec\n",
      "epoch 3, iter 5400, avg. loss 0.00152 time elapsed 21.84sec\n",
      "epoch 3, iter 5500, avg. loss 0.00151 time elapsed 21.75sec\n",
      "epoch 3, iter 5600, avg. loss 0.00151 time elapsed 21.88sec\n",
      "epoch 3, iter 5700, avg. loss 0.00151 time elapsed 21.98sec\n",
      "epoch 3, iter 5800, avg. loss 0.00150 time elapsed 22.04sec\n",
      "epoch 3, iter 5900, avg. loss 0.00152 time elapsed 22.37sec\n",
      "epoch 3, iter 6000, avg. loss 0.00151 time elapsed 21.91sec\n",
      "epoch 3, iter 6100, avg. loss 0.00150 time elapsed 22.08sec\n",
      "epoch 3, iter 6200, avg. loss 0.00150 time elapsed 22.01sec\n",
      "epoch 3, iter 6300, avg. loss 0.00150 time elapsed 21.84sec\n",
      "==============================\n",
      "epoch 4, iter 6400, avg. loss 0.00151 time elapsed 10.72sec\n",
      "epoch 4, iter 6500, avg. loss 0.00150 time elapsed 22.14sec\n",
      "epoch 4, iter 6600, avg. loss 0.00151 time elapsed 22.00sec\n",
      "epoch 4, iter 6700, avg. loss 0.00150 time elapsed 22.77sec\n",
      "epoch 4, iter 6800, avg. loss 0.00150 time elapsed 22.17sec\n",
      "epoch 4, iter 6900, avg. loss 0.00150 time elapsed 22.01sec\n",
      "epoch 4, iter 7000, avg. loss 0.00150 time elapsed 22.07sec\n",
      "epoch 4, iter 7100, avg. loss 0.00150 time elapsed 21.77sec\n",
      "epoch 4, iter 7200, avg. loss 0.00150 time elapsed 22.20sec\n",
      "epoch 4, iter 7300, avg. loss 0.00151 time elapsed 22.53sec\n",
      "epoch 4, iter 7400, avg. loss 0.00150 time elapsed 22.65sec\n",
      "epoch 4, iter 7500, avg. loss 0.00151 time elapsed 22.90sec\n",
      "epoch 4, iter 7600, avg. loss 0.00150 time elapsed 22.45sec\n",
      "epoch 4, iter 7700, avg. loss 0.00150 time elapsed 22.16sec\n",
      "epoch 4, iter 7800, avg. loss 0.00150 time elapsed 22.31sec\n",
      "epoch 4, iter 7900, avg. loss 0.00150 time elapsed 22.42sec\n",
      "==============================\n",
      "epoch 5, iter 8000, avg. loss 0.00150 time elapsed 13.42sec\n",
      "epoch 5, iter 8100, avg. loss 0.00150 time elapsed 21.90sec\n",
      "epoch 5, iter 8200, avg. loss 0.00150 time elapsed 21.95sec\n",
      "epoch 5, iter 8300, avg. loss 0.00150 time elapsed 22.53sec\n",
      "epoch 5, iter 8400, avg. loss 0.00150 time elapsed 22.51sec\n",
      "epoch 5, iter 8500, avg. loss 0.00149 time elapsed 21.89sec\n",
      "epoch 5, iter 8600, avg. loss 0.00150 time elapsed 21.81sec\n",
      "epoch 5, iter 8700, avg. loss 0.00149 time elapsed 21.89sec\n",
      "epoch 5, iter 8800, avg. loss 0.00149 time elapsed 21.69sec\n",
      "epoch 5, iter 8900, avg. loss 0.00149 time elapsed 22.04sec\n",
      "epoch 5, iter 9000, avg. loss 0.00149 time elapsed 22.06sec\n",
      "epoch 5, iter 9100, avg. loss 0.00149 time elapsed 22.19sec\n",
      "epoch 5, iter 9200, avg. loss 0.00149 time elapsed 21.79sec\n",
      "epoch 5, iter 9300, avg. loss 0.00149 time elapsed 22.17sec\n",
      "epoch 5, iter 9400, avg. loss 0.00149 time elapsed 21.88sec\n",
      "epoch 5, iter 9500, avg. loss 0.00149 time elapsed 22.00sec\n",
      "==============================\n",
      "epoch 6, iter 9600, avg. loss 0.00150 time elapsed 16.10sec\n",
      "epoch 6, iter 9700, avg. loss 0.00149 time elapsed 22.31sec\n",
      "epoch 6, iter 9800, avg. loss 0.00150 time elapsed 22.36sec\n",
      "epoch 6, iter 9900, avg. loss 0.00149 time elapsed 22.34sec\n",
      "epoch 6, iter 10000, avg. loss 0.00149 time elapsed 22.42sec\n",
      "epoch 6, iter 10000, cum. loss 0.00147 examples 10240000\n",
      "begin evaluation...\n",
      "validation: iter 10000, dev. acc 0.5163\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_6.checkpoint]\n",
      "epoch 6, iter 10100, avg. loss 0.00149 time elapsed 48.38sec\n",
      "epoch 6, iter 10200, avg. loss 0.00150 time elapsed 21.65sec\n",
      "epoch 6, iter 10300, avg. loss 0.00148 time elapsed 21.66sec\n",
      "epoch 6, iter 10400, avg. loss 0.00148 time elapsed 21.78sec\n",
      "epoch 6, iter 10500, avg. loss 0.00148 time elapsed 21.93sec\n",
      "epoch 6, iter 10600, avg. loss 0.00148 time elapsed 22.31sec\n",
      "epoch 6, iter 10700, avg. loss 0.00148 time elapsed 23.05sec\n",
      "epoch 6, iter 10800, avg. loss 0.00148 time elapsed 22.63sec\n",
      "epoch 6, iter 10900, avg. loss 0.00148 time elapsed 22.66sec\n",
      "epoch 6, iter 11000, avg. loss 0.00147 time elapsed 21.87sec\n",
      "epoch 6, iter 11100, avg. loss 0.00148 time elapsed 21.96sec\n",
      "==============================\n",
      "epoch 7, iter 11200, avg. loss 0.00148 time elapsed 19.25sec\n",
      "epoch 7, iter 11300, avg. loss 0.00148 time elapsed 22.58sec\n",
      "epoch 7, iter 11400, avg. loss 0.00148 time elapsed 22.47sec\n",
      "epoch 7, iter 11500, avg. loss 0.00148 time elapsed 22.81sec\n",
      "epoch 7, iter 11600, avg. loss 0.00148 time elapsed 22.80sec\n",
      "epoch 7, iter 11700, avg. loss 0.00148 time elapsed 22.75sec\n",
      "epoch 7, iter 11800, avg. loss 0.00148 time elapsed 22.53sec\n",
      "epoch 7, iter 11900, avg. loss 0.00148 time elapsed 22.51sec\n",
      "epoch 7, iter 12000, avg. loss 0.00148 time elapsed 22.64sec\n",
      "epoch 7, iter 12100, avg. loss 0.00147 time elapsed 22.26sec\n",
      "epoch 7, iter 12200, avg. loss 0.00146 time elapsed 22.63sec\n",
      "epoch 7, iter 12300, avg. loss 0.00147 time elapsed 21.82sec\n",
      "epoch 7, iter 12400, avg. loss 0.00146 time elapsed 21.83sec\n",
      "epoch 7, iter 12500, avg. loss 0.00147 time elapsed 21.95sec\n",
      "epoch 7, iter 12600, avg. loss 0.00147 time elapsed 22.04sec\n",
      "epoch 7, iter 12700, avg. loss 0.00146 time elapsed 22.49sec\n",
      "==============================\n",
      "epoch 8, iter 12800, avg. loss 0.00146 time elapsed 21.26sec\n",
      "epoch 8, iter 12900, avg. loss 0.00146 time elapsed 22.33sec\n",
      "epoch 8, iter 13000, avg. loss 0.00146 time elapsed 22.60sec\n",
      "epoch 8, iter 13100, avg. loss 0.00145 time elapsed 23.13sec\n",
      "epoch 8, iter 13200, avg. loss 0.00146 time elapsed 22.60sec\n",
      "epoch 8, iter 13300, avg. loss 0.00146 time elapsed 22.27sec\n",
      "epoch 8, iter 13400, avg. loss 0.00145 time elapsed 22.29sec\n",
      "epoch 8, iter 13500, avg. loss 0.00145 time elapsed 22.35sec\n",
      "epoch 8, iter 13600, avg. loss 0.00145 time elapsed 22.30sec\n",
      "epoch 8, iter 13700, avg. loss 0.00145 time elapsed 22.43sec\n",
      "epoch 8, iter 13800, avg. loss 0.00145 time elapsed 22.73sec\n",
      "epoch 8, iter 13900, avg. loss 0.00146 time elapsed 22.26sec\n",
      "epoch 8, iter 14000, avg. loss 0.00146 time elapsed 22.31sec\n",
      "epoch 8, iter 14100, avg. loss 0.00145 time elapsed 22.61sec\n",
      "epoch 8, iter 14200, avg. loss 0.00145 time elapsed 22.32sec\n",
      "==============================\n",
      "epoch 9, iter 14300, avg. loss 0.00145 time elapsed 1.80sec\n",
      "epoch 9, iter 14400, avg. loss 0.00145 time elapsed 22.64sec\n",
      "epoch 9, iter 14500, avg. loss 0.00145 time elapsed 22.39sec\n",
      "epoch 9, iter 14600, avg. loss 0.00145 time elapsed 22.67sec\n",
      "epoch 9, iter 14700, avg. loss 0.00144 time elapsed 22.71sec\n",
      "epoch 9, iter 14800, avg. loss 0.00145 time elapsed 22.46sec\n",
      "epoch 9, iter 14900, avg. loss 0.00145 time elapsed 22.45sec\n",
      "epoch 9, iter 15000, avg. loss 0.00145 time elapsed 22.24sec\n",
      "epoch 9, iter 15100, avg. loss 0.00144 time elapsed 22.31sec\n",
      "epoch 9, iter 15200, avg. loss 0.00145 time elapsed 21.98sec\n",
      "epoch 9, iter 15300, avg. loss 0.00144 time elapsed 21.73sec\n",
      "epoch 9, iter 15400, avg. loss 0.00144 time elapsed 22.19sec\n",
      "epoch 9, iter 15500, avg. loss 0.00144 time elapsed 21.78sec\n",
      "epoch 9, iter 15600, avg. loss 0.00144 time elapsed 22.01sec\n",
      "epoch 9, iter 15700, avg. loss 0.00144 time elapsed 22.61sec\n",
      "epoch 9, iter 15800, avg. loss 0.00144 time elapsed 22.31sec\n",
      "==============================\n",
      "epoch 10, iter 15900, avg. loss 0.00144 time elapsed 4.41sec\n",
      "epoch 10, iter 16000, avg. loss 0.00144 time elapsed 22.10sec\n",
      "epoch 10, iter 16100, avg. loss 0.00144 time elapsed 21.85sec\n",
      "epoch 10, iter 16200, avg. loss 0.00143 time elapsed 22.18sec\n",
      "epoch 10, iter 16300, avg. loss 0.00143 time elapsed 22.04sec\n",
      "epoch 10, iter 16400, avg. loss 0.00143 time elapsed 22.04sec\n",
      "epoch 10, iter 16500, avg. loss 0.00144 time elapsed 21.69sec\n",
      "epoch 10, iter 16600, avg. loss 0.00143 time elapsed 21.80sec\n",
      "epoch 10, iter 16700, avg. loss 0.00143 time elapsed 21.84sec\n",
      "epoch 10, iter 16800, avg. loss 0.00143 time elapsed 21.79sec\n",
      "epoch 10, iter 16900, avg. loss 0.00142 time elapsed 21.93sec\n",
      "epoch 10, iter 17000, avg. loss 0.00142 time elapsed 22.21sec\n",
      "epoch 10, iter 17100, avg. loss 0.00142 time elapsed 21.89sec\n",
      "epoch 10, iter 17200, avg. loss 0.00142 time elapsed 21.96sec\n",
      "epoch 10, iter 17300, avg. loss 0.00142 time elapsed 21.79sec\n",
      "epoch 10, iter 17400, avg. loss 0.00142 time elapsed 21.87sec\n",
      "==============================\n",
      "epoch 11, iter 17500, avg. loss 0.00142 time elapsed 7.13sec\n",
      "epoch 11, iter 17600, avg. loss 0.00142 time elapsed 21.93sec\n",
      "epoch 11, iter 17700, avg. loss 0.00142 time elapsed 21.79sec\n",
      "epoch 11, iter 17800, avg. loss 0.00141 time elapsed 22.69sec\n",
      "epoch 11, iter 17900, avg. loss 0.00142 time elapsed 22.58sec\n",
      "epoch 11, iter 18000, avg. loss 0.00141 time elapsed 22.43sec\n",
      "epoch 11, iter 18100, avg. loss 0.00142 time elapsed 21.93sec\n",
      "epoch 11, iter 18200, avg. loss 0.00141 time elapsed 21.66sec\n",
      "epoch 11, iter 18300, avg. loss 0.00141 time elapsed 21.69sec\n",
      "epoch 11, iter 18400, avg. loss 0.00141 time elapsed 21.82sec\n",
      "epoch 11, iter 18500, avg. loss 0.00141 time elapsed 21.94sec\n",
      "epoch 11, iter 18600, avg. loss 0.00141 time elapsed 22.23sec\n",
      "epoch 11, iter 18700, avg. loss 0.00141 time elapsed 21.87sec\n",
      "epoch 11, iter 18800, avg. loss 0.00141 time elapsed 22.02sec\n",
      "epoch 11, iter 18900, avg. loss 0.00141 time elapsed 21.93sec\n",
      "epoch 11, iter 19000, avg. loss 0.00141 time elapsed 21.92sec\n",
      "==============================\n",
      "epoch 12, iter 19100, avg. loss 0.00141 time elapsed 9.78sec\n",
      "epoch 12, iter 19200, avg. loss 0.00141 time elapsed 22.22sec\n",
      "epoch 12, iter 19300, avg. loss 0.00141 time elapsed 21.87sec\n",
      "epoch 12, iter 19400, avg. loss 0.00140 time elapsed 22.49sec\n",
      "epoch 12, iter 19500, avg. loss 0.00140 time elapsed 22.64sec\n",
      "epoch 12, iter 19600, avg. loss 0.00140 time elapsed 22.49sec\n",
      "epoch 12, iter 19700, avg. loss 0.00140 time elapsed 22.50sec\n",
      "epoch 12, iter 19800, avg. loss 0.00140 time elapsed 22.20sec\n",
      "epoch 12, iter 19900, avg. loss 0.00140 time elapsed 22.22sec\n",
      "epoch 12, iter 20000, avg. loss 0.00140 time elapsed 22.51sec\n",
      "epoch 12, iter 20000, cum. loss 0.00144 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 20000, dev. acc 0.5658\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_12.checkpoint]\n",
      "epoch 12, iter 20100, avg. loss 0.00140 time elapsed 48.66sec\n",
      "epoch 12, iter 20200, avg. loss 0.00140 time elapsed 22.66sec\n",
      "epoch 12, iter 20300, avg. loss 0.00139 time elapsed 22.09sec\n",
      "epoch 12, iter 20400, avg. loss 0.00139 time elapsed 22.09sec\n",
      "epoch 12, iter 20500, avg. loss 0.00139 time elapsed 21.72sec\n",
      "epoch 12, iter 20600, avg. loss 0.00140 time elapsed 21.77sec\n",
      "==============================\n",
      "epoch 13, iter 20700, avg. loss 0.00140 time elapsed 12.49sec\n",
      "epoch 13, iter 20800, avg. loss 0.00140 time elapsed 21.92sec\n",
      "epoch 13, iter 20900, avg. loss 0.00140 time elapsed 21.79sec\n",
      "epoch 13, iter 21000, avg. loss 0.00140 time elapsed 22.24sec\n",
      "epoch 13, iter 21100, avg. loss 0.00140 time elapsed 22.20sec\n",
      "epoch 13, iter 21200, avg. loss 0.00139 time elapsed 21.67sec\n",
      "epoch 13, iter 21300, avg. loss 0.00139 time elapsed 21.75sec\n",
      "epoch 13, iter 21400, avg. loss 0.00139 time elapsed 21.72sec\n",
      "epoch 13, iter 21500, avg. loss 0.00139 time elapsed 21.64sec\n",
      "epoch 13, iter 21600, avg. loss 0.00139 time elapsed 21.91sec\n",
      "epoch 13, iter 21700, avg. loss 0.00138 time elapsed 22.09sec\n",
      "epoch 13, iter 21800, avg. loss 0.00138 time elapsed 22.14sec\n",
      "epoch 13, iter 21900, avg. loss 0.00138 time elapsed 21.73sec\n",
      "epoch 13, iter 22000, avg. loss 0.00138 time elapsed 22.01sec\n",
      "epoch 13, iter 22100, avg. loss 0.00138 time elapsed 21.87sec\n",
      "epoch 13, iter 22200, avg. loss 0.00138 time elapsed 21.96sec\n",
      "==============================\n",
      "epoch 14, iter 22300, avg. loss 0.00138 time elapsed 15.16sec\n",
      "epoch 14, iter 22400, avg. loss 0.00138 time elapsed 22.04sec\n",
      "epoch 14, iter 22500, avg. loss 0.00138 time elapsed 22.27sec\n",
      "epoch 14, iter 22600, avg. loss 0.00138 time elapsed 22.98sec\n",
      "epoch 14, iter 22700, avg. loss 0.00138 time elapsed 22.91sec\n",
      "epoch 14, iter 22800, avg. loss 0.00138 time elapsed 22.42sec\n",
      "epoch 14, iter 22900, avg. loss 0.00138 time elapsed 22.20sec\n",
      "epoch 14, iter 23000, avg. loss 0.00137 time elapsed 22.17sec\n",
      "epoch 14, iter 23100, avg. loss 0.00137 time elapsed 22.30sec\n",
      "epoch 14, iter 23200, avg. loss 0.00137 time elapsed 22.40sec\n",
      "epoch 14, iter 23300, avg. loss 0.00137 time elapsed 22.47sec\n",
      "epoch 14, iter 23400, avg. loss 0.00137 time elapsed 22.71sec\n",
      "epoch 14, iter 23500, avg. loss 0.00137 time elapsed 22.32sec\n",
      "epoch 14, iter 23600, avg. loss 0.00137 time elapsed 22.47sec\n",
      "epoch 14, iter 23700, avg. loss 0.00137 time elapsed 22.31sec\n",
      "epoch 14, iter 23800, avg. loss 0.00137 time elapsed 22.39sec\n",
      "==============================\n",
      "epoch 15, iter 23900, avg. loss 0.00137 time elapsed 18.20sec\n",
      "epoch 15, iter 24000, avg. loss 0.00137 time elapsed 22.39sec\n",
      "epoch 15, iter 24100, avg. loss 0.00136 time elapsed 22.38sec\n",
      "epoch 15, iter 24200, avg. loss 0.00136 time elapsed 22.75sec\n",
      "epoch 15, iter 24300, avg. loss 0.00136 time elapsed 22.54sec\n",
      "epoch 15, iter 24400, avg. loss 0.00137 time elapsed 22.52sec\n",
      "epoch 15, iter 24500, avg. loss 0.00136 time elapsed 22.19sec\n",
      "epoch 15, iter 24600, avg. loss 0.00136 time elapsed 22.20sec\n",
      "epoch 15, iter 24700, avg. loss 0.00136 time elapsed 22.36sec\n",
      "epoch 15, iter 24800, avg. loss 0.00136 time elapsed 22.52sec\n",
      "epoch 15, iter 24900, avg. loss 0.00136 time elapsed 22.60sec\n",
      "epoch 15, iter 25000, avg. loss 0.00136 time elapsed 22.48sec\n",
      "epoch 15, iter 25100, avg. loss 0.00135 time elapsed 22.46sec\n",
      "epoch 15, iter 25200, avg. loss 0.00135 time elapsed 22.49sec\n",
      "epoch 15, iter 25300, avg. loss 0.00135 time elapsed 22.24sec\n",
      "epoch 15, iter 25400, avg. loss 0.00135 time elapsed 22.47sec\n",
      "==============================\n",
      "epoch 16, iter 25500, avg. loss 0.00135 time elapsed 20.81sec\n",
      "epoch 16, iter 25600, avg. loss 0.00135 time elapsed 22.36sec\n",
      "epoch 16, iter 25700, avg. loss 0.00135 time elapsed 22.59sec\n",
      "epoch 16, iter 25800, avg. loss 0.00135 time elapsed 22.81sec\n",
      "epoch 16, iter 25900, avg. loss 0.00135 time elapsed 22.09sec\n",
      "epoch 16, iter 26000, avg. loss 0.00135 time elapsed 22.26sec\n",
      "epoch 16, iter 26100, avg. loss 0.00135 time elapsed 22.17sec\n",
      "epoch 16, iter 26200, avg. loss 0.00135 time elapsed 22.36sec\n",
      "epoch 16, iter 26300, avg. loss 0.00135 time elapsed 22.11sec\n",
      "epoch 16, iter 26400, avg. loss 0.00134 time elapsed 22.62sec\n",
      "epoch 16, iter 26500, avg. loss 0.00134 time elapsed 22.61sec\n",
      "epoch 16, iter 26600, avg. loss 0.00134 time elapsed 22.34sec\n",
      "epoch 16, iter 26700, avg. loss 0.00134 time elapsed 22.56sec\n",
      "epoch 16, iter 26800, avg. loss 0.00134 time elapsed 22.47sec\n",
      "epoch 16, iter 26900, avg. loss 0.00134 time elapsed 22.44sec\n",
      "==============================\n",
      "epoch 17, iter 27000, avg. loss 0.00134 time elapsed 0.92sec\n",
      "epoch 17, iter 27100, avg. loss 0.00134 time elapsed 22.66sec\n",
      "epoch 17, iter 27200, avg. loss 0.00135 time elapsed 22.45sec\n",
      "epoch 17, iter 27300, avg. loss 0.00134 time elapsed 22.66sec\n",
      "epoch 17, iter 27400, avg. loss 0.00134 time elapsed 22.80sec\n",
      "epoch 17, iter 27500, avg. loss 0.00134 time elapsed 22.44sec\n",
      "epoch 17, iter 27600, avg. loss 0.00134 time elapsed 22.29sec\n",
      "epoch 17, iter 27700, avg. loss 0.00134 time elapsed 22.27sec\n",
      "epoch 17, iter 27800, avg. loss 0.00134 time elapsed 22.27sec\n",
      "epoch 17, iter 27900, avg. loss 0.00134 time elapsed 22.11sec\n",
      "epoch 17, iter 28000, avg. loss 0.00134 time elapsed 21.95sec\n",
      "epoch 17, iter 28100, avg. loss 0.00134 time elapsed 22.31sec\n",
      "epoch 17, iter 28200, avg. loss 0.00133 time elapsed 21.81sec\n",
      "epoch 17, iter 28300, avg. loss 0.00133 time elapsed 21.75sec\n",
      "epoch 17, iter 28400, avg. loss 0.00133 time elapsed 22.09sec\n",
      "epoch 17, iter 28500, avg. loss 0.00133 time elapsed 21.78sec\n",
      "==============================\n",
      "epoch 18, iter 28600, avg. loss 0.00133 time elapsed 3.53sec\n",
      "epoch 18, iter 28700, avg. loss 0.00133 time elapsed 22.38sec\n",
      "epoch 18, iter 28800, avg. loss 0.00133 time elapsed 22.47sec\n",
      "epoch 18, iter 28900, avg. loss 0.00133 time elapsed 22.73sec\n",
      "epoch 18, iter 29000, avg. loss 0.00133 time elapsed 22.66sec\n",
      "epoch 18, iter 29100, avg. loss 0.00133 time elapsed 22.59sec\n",
      "epoch 18, iter 29200, avg. loss 0.00133 time elapsed 22.17sec\n",
      "epoch 18, iter 29300, avg. loss 0.00132 time elapsed 22.12sec\n",
      "epoch 18, iter 29400, avg. loss 0.00132 time elapsed 21.79sec\n",
      "epoch 18, iter 29500, avg. loss 0.00133 time elapsed 21.75sec\n",
      "epoch 18, iter 29600, avg. loss 0.00132 time elapsed 21.93sec\n",
      "epoch 18, iter 29700, avg. loss 0.00132 time elapsed 22.26sec\n",
      "epoch 18, iter 29800, avg. loss 0.00132 time elapsed 22.58sec\n",
      "epoch 18, iter 29900, avg. loss 0.00132 time elapsed 22.27sec\n",
      "epoch 18, iter 30000, avg. loss 0.00132 time elapsed 22.44sec\n",
      "epoch 18, iter 30000, cum. loss 0.00136 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 30000, dev. acc 0.6047\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_18.checkpoint]\n",
      "epoch 18, iter 30100, avg. loss 0.00132 time elapsed 48.43sec\n",
      "==============================\n",
      "epoch 19, iter 30200, avg. loss 0.00132 time elapsed 6.36sec\n",
      "epoch 19, iter 30300, avg. loss 0.00132 time elapsed 22.56sec\n",
      "epoch 19, iter 30400, avg. loss 0.00132 time elapsed 22.28sec\n",
      "epoch 19, iter 30500, avg. loss 0.00131 time elapsed 22.90sec\n",
      "epoch 19, iter 30600, avg. loss 0.00132 time elapsed 22.55sec\n",
      "epoch 19, iter 30700, avg. loss 0.00132 time elapsed 22.47sec\n",
      "epoch 19, iter 30800, avg. loss 0.00132 time elapsed 22.23sec\n",
      "epoch 19, iter 30900, avg. loss 0.00131 time elapsed 22.16sec\n",
      "epoch 19, iter 31000, avg. loss 0.00131 time elapsed 22.23sec\n",
      "epoch 19, iter 31100, avg. loss 0.00132 time elapsed 22.24sec\n",
      "epoch 19, iter 31200, avg. loss 0.00131 time elapsed 21.90sec\n",
      "epoch 19, iter 31300, avg. loss 0.00131 time elapsed 22.15sec\n",
      "epoch 19, iter 31400, avg. loss 0.00131 time elapsed 21.83sec\n",
      "epoch 19, iter 31500, avg. loss 0.00131 time elapsed 21.87sec\n",
      "epoch 19, iter 31600, avg. loss 0.00131 time elapsed 21.86sec\n",
      "epoch 19, iter 31700, avg. loss 0.00131 time elapsed 21.84sec\n",
      "==============================\n",
      "epoch 20, iter 31800, avg. loss 0.00131 time elapsed 8.85sec\n",
      "epoch 20, iter 31900, avg. loss 0.00131 time elapsed 22.03sec\n",
      "epoch 20, iter 32000, avg. loss 0.00131 time elapsed 21.75sec\n",
      "epoch 20, iter 32100, avg. loss 0.00130 time elapsed 22.91sec\n",
      "epoch 20, iter 32200, avg. loss 0.00131 time elapsed 22.63sec\n",
      "epoch 20, iter 32300, avg. loss 0.00130 time elapsed 21.88sec\n",
      "epoch 20, iter 32400, avg. loss 0.00131 time elapsed 21.83sec\n",
      "epoch 20, iter 32500, avg. loss 0.00130 time elapsed 21.59sec\n",
      "epoch 20, iter 32600, avg. loss 0.00130 time elapsed 21.74sec\n",
      "epoch 20, iter 32700, avg. loss 0.00130 time elapsed 22.01sec\n",
      "epoch 20, iter 32800, avg. loss 0.00130 time elapsed 21.86sec\n",
      "epoch 20, iter 32900, avg. loss 0.00130 time elapsed 22.26sec\n",
      "epoch 20, iter 33000, avg. loss 0.00130 time elapsed 21.81sec\n",
      "epoch 20, iter 33100, avg. loss 0.00129 time elapsed 22.64sec\n",
      "epoch 20, iter 33200, avg. loss 0.00129 time elapsed 22.28sec\n",
      "epoch 20, iter 33300, avg. loss 0.00129 time elapsed 22.25sec\n",
      "==============================\n",
      "epoch 21, iter 33400, avg. loss 0.00129 time elapsed 11.48sec\n",
      "epoch 21, iter 33500, avg. loss 0.00130 time elapsed 22.01sec\n",
      "epoch 21, iter 33600, avg. loss 0.00129 time elapsed 21.77sec\n",
      "epoch 21, iter 33700, avg. loss 0.00129 time elapsed 22.44sec\n",
      "epoch 21, iter 33800, avg. loss 0.00129 time elapsed 22.06sec\n",
      "epoch 21, iter 33900, avg. loss 0.00129 time elapsed 21.85sec\n",
      "epoch 21, iter 34000, avg. loss 0.00130 time elapsed 21.87sec\n",
      "epoch 21, iter 34100, avg. loss 0.00129 time elapsed 21.75sec\n",
      "epoch 21, iter 34200, avg. loss 0.00128 time elapsed 21.60sec\n",
      "epoch 21, iter 34300, avg. loss 0.00129 time elapsed 21.93sec\n",
      "epoch 21, iter 34400, avg. loss 0.00128 time elapsed 21.95sec\n",
      "epoch 21, iter 34500, avg. loss 0.00128 time elapsed 22.69sec\n",
      "epoch 21, iter 34600, avg. loss 0.00128 time elapsed 22.13sec\n",
      "epoch 21, iter 34700, avg. loss 0.00128 time elapsed 22.64sec\n",
      "epoch 21, iter 34800, avg. loss 0.00129 time elapsed 22.25sec\n",
      "epoch 21, iter 34900, avg. loss 0.00128 time elapsed 22.43sec\n",
      "==============================\n",
      "epoch 22, iter 35000, avg. loss 0.00128 time elapsed 14.44sec\n",
      "epoch 22, iter 35100, avg. loss 0.00129 time elapsed 22.44sec\n",
      "epoch 22, iter 35200, avg. loss 0.00128 time elapsed 22.16sec\n",
      "epoch 22, iter 35300, avg. loss 0.00128 time elapsed 22.97sec\n",
      "epoch 22, iter 35400, avg. loss 0.00128 time elapsed 22.70sec\n",
      "epoch 22, iter 35500, avg. loss 0.00128 time elapsed 21.62sec\n",
      "epoch 22, iter 35600, avg. loss 0.00128 time elapsed 21.81sec\n",
      "epoch 22, iter 35700, avg. loss 0.00128 time elapsed 21.75sec\n",
      "epoch 22, iter 35800, avg. loss 0.00128 time elapsed 21.62sec\n",
      "epoch 22, iter 35900, avg. loss 0.00128 time elapsed 22.00sec\n",
      "epoch 22, iter 36000, avg. loss 0.00127 time elapsed 21.85sec\n",
      "epoch 22, iter 36100, avg. loss 0.00128 time elapsed 22.20sec\n",
      "epoch 22, iter 36200, avg. loss 0.00127 time elapsed 21.80sec\n",
      "epoch 22, iter 36300, avg. loss 0.00127 time elapsed 22.02sec\n",
      "epoch 22, iter 36400, avg. loss 0.00127 time elapsed 21.84sec\n",
      "epoch 22, iter 36500, avg. loss 0.00127 time elapsed 21.96sec\n",
      "==============================\n",
      "epoch 23, iter 36600, avg. loss 0.00127 time elapsed 16.82sec\n",
      "epoch 23, iter 36700, avg. loss 0.00127 time elapsed 21.98sec\n",
      "epoch 23, iter 36800, avg. loss 0.00127 time elapsed 21.86sec\n",
      "epoch 23, iter 36900, avg. loss 0.00128 time elapsed 22.66sec\n",
      "epoch 23, iter 37000, avg. loss 0.00127 time elapsed 22.55sec\n",
      "epoch 23, iter 37100, avg. loss 0.00127 time elapsed 22.40sec\n",
      "epoch 23, iter 37200, avg. loss 0.00127 time elapsed 22.23sec\n",
      "epoch 23, iter 37300, avg. loss 0.00127 time elapsed 22.28sec\n",
      "epoch 23, iter 37400, avg. loss 0.00127 time elapsed 22.29sec\n",
      "epoch 23, iter 37500, avg. loss 0.00127 time elapsed 22.66sec\n",
      "epoch 23, iter 37600, avg. loss 0.00126 time elapsed 22.46sec\n",
      "epoch 23, iter 37700, avg. loss 0.00127 time elapsed 22.47sec\n",
      "epoch 23, iter 37800, avg. loss 0.00126 time elapsed 22.34sec\n",
      "epoch 23, iter 37900, avg. loss 0.00126 time elapsed 22.46sec\n",
      "epoch 23, iter 38000, avg. loss 0.00127 time elapsed 22.19sec\n",
      "epoch 23, iter 38100, avg. loss 0.00127 time elapsed 22.51sec\n",
      "==============================\n",
      "epoch 24, iter 38200, avg. loss 0.00127 time elapsed 20.02sec\n",
      "epoch 24, iter 38300, avg. loss 0.00127 time elapsed 22.37sec\n",
      "epoch 24, iter 38400, avg. loss 0.00127 time elapsed 22.60sec\n",
      "epoch 24, iter 38500, avg. loss 0.00126 time elapsed 22.79sec\n",
      "epoch 24, iter 38600, avg. loss 0.00126 time elapsed 22.28sec\n",
      "epoch 24, iter 38700, avg. loss 0.00126 time elapsed 21.73sec\n",
      "epoch 24, iter 38800, avg. loss 0.00126 time elapsed 21.72sec\n",
      "epoch 24, iter 38900, avg. loss 0.00126 time elapsed 21.77sec\n",
      "epoch 24, iter 39000, avg. loss 0.00126 time elapsed 21.69sec\n",
      "epoch 24, iter 39100, avg. loss 0.00126 time elapsed 22.63sec\n",
      "epoch 24, iter 39200, avg. loss 0.00126 time elapsed 22.58sec\n",
      "epoch 24, iter 39300, avg. loss 0.00126 time elapsed 22.31sec\n",
      "epoch 24, iter 39400, avg. loss 0.00126 time elapsed 22.43sec\n",
      "epoch 24, iter 39500, avg. loss 0.00126 time elapsed 22.43sec\n",
      "epoch 24, iter 39600, avg. loss 0.00125 time elapsed 22.24sec\n",
      "epoch 24, iter 39700, avg. loss 0.00125 time elapsed 22.33sec\n",
      "==============================\n",
      "epoch 25, iter 39800, avg. loss 0.00126 time elapsed 22.60sec\n",
      "epoch 25, iter 39900, avg. loss 0.00126 time elapsed 22.41sec\n",
      "epoch 25, iter 40000, avg. loss 0.00125 time elapsed 22.58sec\n",
      "epoch 25, iter 40000, cum. loss 0.00129 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 40000, dev. acc 0.6318\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_25.checkpoint]\n",
      "epoch 25, iter 40100, avg. loss 0.00125 time elapsed 49.20sec\n",
      "epoch 25, iter 40200, avg. loss 0.00125 time elapsed 22.33sec\n",
      "epoch 25, iter 40300, avg. loss 0.00125 time elapsed 21.70sec\n",
      "epoch 25, iter 40400, avg. loss 0.00125 time elapsed 21.87sec\n",
      "epoch 25, iter 40500, avg. loss 0.00125 time elapsed 21.72sec\n",
      "epoch 25, iter 40600, avg. loss 0.00125 time elapsed 21.77sec\n",
      "epoch 25, iter 40700, avg. loss 0.00124 time elapsed 22.16sec\n",
      "epoch 25, iter 40800, avg. loss 0.00125 time elapsed 22.30sec\n",
      "epoch 25, iter 40900, avg. loss 0.00125 time elapsed 21.80sec\n",
      "epoch 25, iter 41000, avg. loss 0.00124 time elapsed 21.88sec\n",
      "epoch 25, iter 41100, avg. loss 0.00124 time elapsed 22.25sec\n",
      "epoch 25, iter 41200, avg. loss 0.00125 time elapsed 21.85sec\n",
      "==============================\n",
      "epoch 26, iter 41300, avg. loss 0.00124 time elapsed 2.64sec\n",
      "epoch 26, iter 41400, avg. loss 0.00125 time elapsed 22.25sec\n",
      "epoch 26, iter 41500, avg. loss 0.00125 time elapsed 22.00sec\n",
      "epoch 26, iter 41600, avg. loss 0.00124 time elapsed 22.41sec\n",
      "epoch 26, iter 41700, avg. loss 0.00124 time elapsed 22.89sec\n",
      "epoch 26, iter 41800, avg. loss 0.00124 time elapsed 22.61sec\n",
      "epoch 26, iter 41900, avg. loss 0.00125 time elapsed 22.48sec\n",
      "epoch 26, iter 42000, avg. loss 0.00124 time elapsed 22.27sec\n",
      "epoch 26, iter 42100, avg. loss 0.00124 time elapsed 22.55sec\n",
      "epoch 26, iter 42200, avg. loss 0.00124 time elapsed 22.52sec\n",
      "epoch 26, iter 42300, avg. loss 0.00123 time elapsed 22.49sec\n",
      "epoch 26, iter 42400, avg. loss 0.00123 time elapsed 22.88sec\n",
      "epoch 26, iter 42500, avg. loss 0.00124 time elapsed 22.64sec\n",
      "epoch 26, iter 42600, avg. loss 0.00123 time elapsed 22.41sec\n",
      "epoch 26, iter 42700, avg. loss 0.00124 time elapsed 22.82sec\n",
      "epoch 26, iter 42800, avg. loss 0.00123 time elapsed 22.37sec\n",
      "==============================\n",
      "epoch 27, iter 42900, avg. loss 0.00123 time elapsed 5.53sec\n",
      "epoch 27, iter 43000, avg. loss 0.00124 time elapsed 22.68sec\n",
      "epoch 27, iter 43100, avg. loss 0.00124 time elapsed 22.40sec\n",
      "epoch 27, iter 43200, avg. loss 0.00123 time elapsed 22.88sec\n",
      "epoch 27, iter 43300, avg. loss 0.00123 time elapsed 22.56sec\n",
      "epoch 27, iter 43400, avg. loss 0.00123 time elapsed 22.47sec\n",
      "epoch 27, iter 43500, avg. loss 0.00123 time elapsed 22.23sec\n",
      "epoch 27, iter 43600, avg. loss 0.00123 time elapsed 22.32sec\n",
      "epoch 27, iter 43700, avg. loss 0.00123 time elapsed 22.40sec\n",
      "epoch 27, iter 43800, avg. loss 0.00123 time elapsed 22.35sec\n",
      "epoch 27, iter 43900, avg. loss 0.00122 time elapsed 22.48sec\n",
      "epoch 27, iter 44000, avg. loss 0.00123 time elapsed 22.68sec\n",
      "epoch 27, iter 44100, avg. loss 0.00123 time elapsed 22.51sec\n",
      "epoch 27, iter 44200, avg. loss 0.00123 time elapsed 22.45sec\n",
      "epoch 27, iter 44300, avg. loss 0.00123 time elapsed 22.42sec\n",
      "epoch 27, iter 44400, avg. loss 0.00122 time elapsed 22.42sec\n",
      "==============================\n",
      "epoch 28, iter 44500, avg. loss 0.00123 time elapsed 8.30sec\n",
      "epoch 28, iter 44600, avg. loss 0.00123 time elapsed 22.53sec\n",
      "epoch 28, iter 44700, avg. loss 0.00123 time elapsed 22.28sec\n",
      "epoch 28, iter 44800, avg. loss 0.00122 time elapsed 22.92sec\n",
      "epoch 28, iter 44900, avg. loss 0.00122 time elapsed 22.58sec\n",
      "epoch 28, iter 45000, avg. loss 0.00122 time elapsed 22.44sec\n",
      "epoch 28, iter 45100, avg. loss 0.00123 time elapsed 22.37sec\n",
      "epoch 28, iter 45200, avg. loss 0.00122 time elapsed 22.36sec\n",
      "epoch 28, iter 45300, avg. loss 0.00122 time elapsed 22.45sec\n",
      "epoch 28, iter 45400, avg. loss 0.00122 time elapsed 22.52sec\n",
      "epoch 28, iter 45500, avg. loss 0.00122 time elapsed 22.61sec\n",
      "epoch 28, iter 45600, avg. loss 0.00122 time elapsed 22.86sec\n",
      "epoch 28, iter 45700, avg. loss 0.00122 time elapsed 22.21sec\n",
      "epoch 28, iter 45800, avg. loss 0.00121 time elapsed 22.57sec\n",
      "epoch 28, iter 45900, avg. loss 0.00122 time elapsed 22.46sec\n",
      "epoch 28, iter 46000, avg. loss 0.00122 time elapsed 22.26sec\n",
      "==============================\n",
      "epoch 29, iter 46100, avg. loss 0.00122 time elapsed 10.95sec\n",
      "epoch 29, iter 46200, avg. loss 0.00122 time elapsed 23.01sec\n",
      "epoch 29, iter 46300, avg. loss 0.00122 time elapsed 22.64sec\n",
      "epoch 29, iter 46400, avg. loss 0.00121 time elapsed 23.25sec\n",
      "epoch 29, iter 46500, avg. loss 0.00122 time elapsed 22.81sec\n",
      "epoch 29, iter 46600, avg. loss 0.00121 time elapsed 22.28sec\n",
      "epoch 29, iter 46700, avg. loss 0.00121 time elapsed 22.25sec\n",
      "epoch 29, iter 46800, avg. loss 0.00121 time elapsed 22.31sec\n",
      "epoch 29, iter 46900, avg. loss 0.00121 time elapsed 22.27sec\n",
      "epoch 29, iter 47000, avg. loss 0.00121 time elapsed 22.50sec\n",
      "epoch 29, iter 47100, avg. loss 0.00121 time elapsed 22.50sec\n",
      "epoch 29, iter 47200, avg. loss 0.00121 time elapsed 22.75sec\n",
      "epoch 29, iter 47300, avg. loss 0.00121 time elapsed 22.34sec\n",
      "epoch 29, iter 47400, avg. loss 0.00121 time elapsed 22.67sec\n",
      "epoch 29, iter 47500, avg. loss 0.00120 time elapsed 22.33sec\n",
      "epoch 29, iter 47600, avg. loss 0.00120 time elapsed 22.32sec\n",
      "==============================\n",
      "epoch 30, iter 47700, avg. loss 0.00120 time elapsed 13.62sec\n",
      "epoch 30, iter 47800, avg. loss 0.00121 time elapsed 22.64sec\n",
      "epoch 30, iter 47900, avg. loss 0.00120 time elapsed 22.50sec\n",
      "epoch 30, iter 48000, avg. loss 0.00120 time elapsed 22.90sec\n",
      "epoch 30, iter 48100, avg. loss 0.00120 time elapsed 22.63sec\n",
      "epoch 30, iter 48200, avg. loss 0.00120 time elapsed 21.72sec\n",
      "epoch 30, iter 48300, avg. loss 0.00121 time elapsed 21.77sec\n",
      "epoch 30, iter 48400, avg. loss 0.00120 time elapsed 21.76sec\n",
      "epoch 30, iter 48500, avg. loss 0.00120 time elapsed 21.63sec\n",
      "epoch 30, iter 48600, avg. loss 0.00120 time elapsed 21.96sec\n",
      "epoch 30, iter 48700, avg. loss 0.00120 time elapsed 21.88sec\n",
      "epoch 30, iter 48800, avg. loss 0.00120 time elapsed 22.27sec\n",
      "epoch 30, iter 48900, avg. loss 0.00119 time elapsed 21.70sec\n",
      "epoch 30, iter 49000, avg. loss 0.00119 time elapsed 21.98sec\n",
      "epoch 30, iter 49100, avg. loss 0.00120 time elapsed 21.88sec\n",
      "epoch 30, iter 49200, avg. loss 0.00120 time elapsed 21.95sec\n",
      "==============================\n",
      "epoch 31, iter 49300, avg. loss 0.00119 time elapsed 15.96sec\n",
      "epoch 31, iter 49400, avg. loss 0.00120 time elapsed 22.03sec\n",
      "epoch 31, iter 49500, avg. loss 0.00120 time elapsed 21.78sec\n",
      "epoch 31, iter 49600, avg. loss 0.00119 time elapsed 22.36sec\n",
      "epoch 31, iter 49700, avg. loss 0.00120 time elapsed 22.73sec\n",
      "epoch 31, iter 49800, avg. loss 0.00120 time elapsed 22.66sec\n",
      "epoch 31, iter 49900, avg. loss 0.00120 time elapsed 22.29sec\n",
      "epoch 31, iter 50000, avg. loss 0.00119 time elapsed 22.28sec\n",
      "epoch 31, iter 50000, cum. loss 0.00122 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 50000, dev. acc 0.6571\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_31.checkpoint]\n",
      "epoch 31, iter 50100, avg. loss 0.00119 time elapsed 49.31sec\n",
      "epoch 31, iter 50200, avg. loss 0.00119 time elapsed 22.78sec\n",
      "epoch 31, iter 50300, avg. loss 0.00119 time elapsed 22.90sec\n",
      "epoch 31, iter 50400, avg. loss 0.00119 time elapsed 23.10sec\n",
      "epoch 31, iter 50500, avg. loss 0.00119 time elapsed 22.67sec\n",
      "epoch 31, iter 50600, avg. loss 0.00119 time elapsed 22.82sec\n",
      "epoch 31, iter 50700, avg. loss 0.00119 time elapsed 22.70sec\n",
      "epoch 31, iter 50800, avg. loss 0.00119 time elapsed 22.63sec\n",
      "==============================\n",
      "epoch 32, iter 50900, avg. loss 0.00119 time elapsed 19.14sec\n",
      "epoch 32, iter 51000, avg. loss 0.00119 time elapsed 22.45sec\n",
      "epoch 32, iter 51100, avg. loss 0.00119 time elapsed 22.37sec\n",
      "epoch 32, iter 51200, avg. loss 0.00119 time elapsed 22.28sec\n",
      "epoch 32, iter 51300, avg. loss 0.00119 time elapsed 22.19sec\n",
      "epoch 32, iter 51400, avg. loss 0.00119 time elapsed 21.87sec\n",
      "epoch 32, iter 51500, avg. loss 0.00118 time elapsed 21.79sec\n",
      "epoch 32, iter 51600, avg. loss 0.00119 time elapsed 21.72sec\n",
      "epoch 32, iter 51700, avg. loss 0.00118 time elapsed 21.87sec\n",
      "epoch 32, iter 51800, avg. loss 0.00118 time elapsed 22.14sec\n",
      "epoch 32, iter 51900, avg. loss 0.00118 time elapsed 22.21sec\n",
      "epoch 32, iter 52000, avg. loss 0.00118 time elapsed 22.45sec\n",
      "epoch 32, iter 52100, avg. loss 0.00118 time elapsed 22.53sec\n",
      "epoch 32, iter 52200, avg. loss 0.00118 time elapsed 22.58sec\n",
      "epoch 32, iter 52300, avg. loss 0.00118 time elapsed 22.30sec\n",
      "epoch 32, iter 52400, avg. loss 0.00118 time elapsed 22.67sec\n",
      "==============================\n",
      "epoch 33, iter 52500, avg. loss 0.00118 time elapsed 21.77sec\n",
      "epoch 33, iter 52600, avg. loss 0.00118 time elapsed 22.51sec\n",
      "epoch 33, iter 52700, avg. loss 0.00118 time elapsed 22.66sec\n",
      "epoch 33, iter 52800, avg. loss 0.00118 time elapsed 23.23sec\n",
      "epoch 33, iter 52900, avg. loss 0.00118 time elapsed 22.89sec\n",
      "epoch 33, iter 53000, avg. loss 0.00118 time elapsed 22.64sec\n",
      "epoch 33, iter 53100, avg. loss 0.00118 time elapsed 22.55sec\n",
      "epoch 33, iter 53200, avg. loss 0.00118 time elapsed 22.60sec\n",
      "epoch 33, iter 53300, avg. loss 0.00117 time elapsed 22.54sec\n",
      "epoch 33, iter 53400, avg. loss 0.00117 time elapsed 22.86sec\n",
      "epoch 33, iter 53500, avg. loss 0.00118 time elapsed 23.11sec\n",
      "epoch 33, iter 53600, avg. loss 0.00117 time elapsed 22.66sec\n",
      "epoch 33, iter 53700, avg. loss 0.00117 time elapsed 22.66sec\n",
      "epoch 33, iter 53800, avg. loss 0.00118 time elapsed 22.92sec\n",
      "epoch 33, iter 53900, avg. loss 0.00117 time elapsed 22.69sec\n",
      "==============================\n",
      "epoch 34, iter 54000, avg. loss 0.00117 time elapsed 1.84sec\n",
      "epoch 34, iter 54100, avg. loss 0.00118 time elapsed 22.88sec\n",
      "epoch 34, iter 54200, avg. loss 0.00117 time elapsed 22.07sec\n",
      "epoch 34, iter 54300, avg. loss 0.00117 time elapsed 22.10sec\n",
      "epoch 34, iter 54400, avg. loss 0.00117 time elapsed 22.33sec\n",
      "epoch 34, iter 54500, avg. loss 0.00117 time elapsed 21.90sec\n",
      "epoch 34, iter 54600, avg. loss 0.00117 time elapsed 21.83sec\n",
      "epoch 34, iter 54700, avg. loss 0.00117 time elapsed 21.64sec\n",
      "epoch 34, iter 54800, avg. loss 0.00117 time elapsed 21.79sec\n",
      "epoch 34, iter 54900, avg. loss 0.00117 time elapsed 21.78sec\n",
      "epoch 34, iter 55000, avg. loss 0.00116 time elapsed 21.79sec\n",
      "epoch 34, iter 55100, avg. loss 0.00117 time elapsed 22.19sec\n",
      "epoch 34, iter 55200, avg. loss 0.00117 time elapsed 22.77sec\n",
      "epoch 34, iter 55300, avg. loss 0.00117 time elapsed 22.51sec\n",
      "epoch 34, iter 55400, avg. loss 0.00117 time elapsed 22.58sec\n",
      "epoch 34, iter 55500, avg. loss 0.00117 time elapsed 22.01sec\n",
      "==============================\n",
      "epoch 35, iter 55600, avg. loss 0.00116 time elapsed 4.59sec\n",
      "epoch 35, iter 55700, avg. loss 0.00117 time elapsed 22.68sec\n",
      "epoch 35, iter 55800, avg. loss 0.00117 time elapsed 22.22sec\n",
      "epoch 35, iter 55900, avg. loss 0.00117 time elapsed 22.30sec\n",
      "epoch 35, iter 56000, avg. loss 0.00117 time elapsed 21.98sec\n",
      "epoch 35, iter 56100, avg. loss 0.00117 time elapsed 22.96sec\n",
      "epoch 35, iter 56200, avg. loss 0.00117 time elapsed 22.44sec\n",
      "epoch 35, iter 56300, avg. loss 0.00116 time elapsed 22.00sec\n",
      "epoch 35, iter 56400, avg. loss 0.00116 time elapsed 21.68sec\n",
      "epoch 35, iter 56500, avg. loss 0.00116 time elapsed 21.71sec\n",
      "epoch 35, iter 56600, avg. loss 0.00116 time elapsed 21.98sec\n",
      "epoch 35, iter 56700, avg. loss 0.00116 time elapsed 22.07sec\n",
      "epoch 35, iter 56800, avg. loss 0.00116 time elapsed 21.93sec\n",
      "epoch 35, iter 56900, avg. loss 0.00116 time elapsed 21.78sec\n",
      "epoch 35, iter 57000, avg. loss 0.00116 time elapsed 21.93sec\n",
      "epoch 35, iter 57100, avg. loss 0.00116 time elapsed 21.84sec\n",
      "==============================\n",
      "epoch 36, iter 57200, avg. loss 0.00116 time elapsed 7.14sec\n",
      "epoch 36, iter 57300, avg. loss 0.00116 time elapsed 22.36sec\n",
      "epoch 36, iter 57400, avg. loss 0.00116 time elapsed 22.68sec\n",
      "epoch 36, iter 57500, avg. loss 0.00116 time elapsed 23.14sec\n",
      "epoch 36, iter 57600, avg. loss 0.00116 time elapsed 23.04sec\n",
      "epoch 36, iter 57700, avg. loss 0.00116 time elapsed 22.12sec\n",
      "epoch 36, iter 57800, avg. loss 0.00116 time elapsed 21.74sec\n",
      "epoch 36, iter 57900, avg. loss 0.00116 time elapsed 22.61sec\n",
      "epoch 36, iter 58000, avg. loss 0.00115 time elapsed 22.71sec\n",
      "epoch 36, iter 58100, avg. loss 0.00116 time elapsed 22.77sec\n",
      "epoch 36, iter 58200, avg. loss 0.00115 time elapsed 22.87sec\n",
      "epoch 36, iter 58300, avg. loss 0.00116 time elapsed 23.06sec\n",
      "epoch 36, iter 58400, avg. loss 0.00116 time elapsed 22.37sec\n",
      "epoch 36, iter 58500, avg. loss 0.00116 time elapsed 22.39sec\n",
      "epoch 36, iter 58600, avg. loss 0.00116 time elapsed 22.76sec\n",
      "epoch 36, iter 58700, avg. loss 0.00115 time elapsed 22.37sec\n",
      "==============================\n",
      "epoch 37, iter 58800, avg. loss 0.00116 time elapsed 10.00sec\n",
      "epoch 37, iter 58900, avg. loss 0.00116 time elapsed 22.50sec\n",
      "epoch 37, iter 59000, avg. loss 0.00116 time elapsed 22.39sec\n",
      "epoch 37, iter 59100, avg. loss 0.00115 time elapsed 22.87sec\n",
      "epoch 37, iter 59200, avg. loss 0.00116 time elapsed 22.67sec\n",
      "epoch 37, iter 59300, avg. loss 0.00116 time elapsed 22.35sec\n",
      "epoch 37, iter 59400, avg. loss 0.00116 time elapsed 22.50sec\n",
      "epoch 37, iter 59500, avg. loss 0.00115 time elapsed 22.30sec\n",
      "epoch 37, iter 59600, avg. loss 0.00115 time elapsed 21.65sec\n",
      "epoch 37, iter 59700, avg. loss 0.00115 time elapsed 21.86sec\n",
      "epoch 37, iter 59800, avg. loss 0.00115 time elapsed 21.92sec\n",
      "epoch 37, iter 59900, avg. loss 0.00115 time elapsed 22.10sec\n",
      "epoch 37, iter 60000, avg. loss 0.00115 time elapsed 21.66sec\n",
      "epoch 37, iter 60000, cum. loss 0.00117 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 60000, dev. acc 0.6709\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_37.checkpoint]\n",
      "epoch 37, iter 60100, avg. loss 0.00115 time elapsed 48.03sec\n",
      "epoch 37, iter 60200, avg. loss 0.00115 time elapsed 21.77sec\n",
      "epoch 37, iter 60300, avg. loss 0.00115 time elapsed 22.30sec\n",
      "==============================\n",
      "epoch 38, iter 60400, avg. loss 0.00115 time elapsed 12.51sec\n",
      "epoch 38, iter 60500, avg. loss 0.00115 time elapsed 21.94sec\n",
      "epoch 38, iter 60600, avg. loss 0.00115 time elapsed 21.92sec\n",
      "epoch 38, iter 60700, avg. loss 0.00115 time elapsed 22.21sec\n",
      "epoch 38, iter 60800, avg. loss 0.00115 time elapsed 22.26sec\n",
      "epoch 38, iter 60900, avg. loss 0.00115 time elapsed 21.69sec\n",
      "epoch 38, iter 61000, avg. loss 0.00115 time elapsed 21.77sec\n",
      "epoch 38, iter 61100, avg. loss 0.00115 time elapsed 22.40sec\n",
      "epoch 38, iter 61200, avg. loss 0.00115 time elapsed 22.11sec\n",
      "epoch 38, iter 61300, avg. loss 0.00115 time elapsed 21.96sec\n",
      "epoch 38, iter 61400, avg. loss 0.00115 time elapsed 22.59sec\n",
      "epoch 38, iter 61500, avg. loss 0.00115 time elapsed 22.94sec\n",
      "epoch 38, iter 61600, avg. loss 0.00114 time elapsed 22.51sec\n",
      "epoch 38, iter 61700, avg. loss 0.00114 time elapsed 22.79sec\n",
      "epoch 38, iter 61800, avg. loss 0.00115 time elapsed 22.67sec\n",
      "epoch 38, iter 61900, avg. loss 0.00114 time elapsed 22.40sec\n",
      "==============================\n",
      "epoch 39, iter 62000, avg. loss 0.00114 time elapsed 15.09sec\n",
      "epoch 39, iter 62100, avg. loss 0.00115 time elapsed 22.04sec\n",
      "epoch 39, iter 62200, avg. loss 0.00115 time elapsed 22.15sec\n",
      "epoch 39, iter 62300, avg. loss 0.00114 time elapsed 22.21sec\n",
      "epoch 39, iter 62400, avg. loss 0.00114 time elapsed 22.09sec\n",
      "epoch 39, iter 62500, avg. loss 0.00114 time elapsed 21.72sec\n",
      "epoch 39, iter 62600, avg. loss 0.00115 time elapsed 22.34sec\n",
      "epoch 39, iter 62700, avg. loss 0.00114 time elapsed 22.17sec\n",
      "epoch 39, iter 62800, avg. loss 0.00114 time elapsed 22.59sec\n",
      "epoch 39, iter 62900, avg. loss 0.00114 time elapsed 22.78sec\n",
      "epoch 39, iter 63000, avg. loss 0.00114 time elapsed 22.75sec\n",
      "epoch 39, iter 63100, avg. loss 0.00114 time elapsed 23.06sec\n",
      "epoch 39, iter 63200, avg. loss 0.00114 time elapsed 22.51sec\n",
      "epoch 39, iter 63300, avg. loss 0.00114 time elapsed 22.79sec\n",
      "epoch 39, iter 63400, avg. loss 0.00114 time elapsed 22.61sec\n",
      "epoch 39, iter 63500, avg. loss 0.00114 time elapsed 22.72sec\n",
      "==============================\n",
      "epoch 40, iter 63600, avg. loss 0.00114 time elapsed 18.20sec\n",
      "epoch 40, iter 63700, avg. loss 0.00114 time elapsed 22.37sec\n",
      "epoch 40, iter 63800, avg. loss 0.00114 time elapsed 22.22sec\n",
      "epoch 40, iter 63900, avg. loss 0.00114 time elapsed 22.17sec\n",
      "epoch 40, iter 64000, avg. loss 0.00114 time elapsed 22.93sec\n",
      "epoch 40, iter 64100, avg. loss 0.00114 time elapsed 22.38sec\n",
      "epoch 40, iter 64200, avg. loss 0.00114 time elapsed 22.33sec\n",
      "epoch 40, iter 64300, avg. loss 0.00114 time elapsed 21.69sec\n",
      "epoch 40, iter 64400, avg. loss 0.00114 time elapsed 21.72sec\n",
      "epoch 40, iter 64500, avg. loss 0.00114 time elapsed 21.86sec\n",
      "epoch 40, iter 64600, avg. loss 0.00114 time elapsed 21.87sec\n",
      "epoch 40, iter 64700, avg. loss 0.00114 time elapsed 22.09sec\n",
      "epoch 40, iter 64800, avg. loss 0.00114 time elapsed 22.39sec\n",
      "epoch 40, iter 64900, avg. loss 0.00114 time elapsed 22.52sec\n",
      "epoch 40, iter 65000, avg. loss 0.00114 time elapsed 22.19sec\n",
      "epoch 40, iter 65100, avg. loss 0.00114 time elapsed 22.85sec\n",
      "==============================\n",
      "epoch 41, iter 65200, avg. loss 0.00113 time elapsed 21.08sec\n",
      "epoch 41, iter 65300, avg. loss 0.00114 time elapsed 22.48sec\n",
      "epoch 41, iter 65400, avg. loss 0.00114 time elapsed 22.14sec\n",
      "epoch 41, iter 65500, avg. loss 0.00113 time elapsed 22.14sec\n",
      "epoch 41, iter 65600, avg. loss 0.00114 time elapsed 22.15sec\n",
      "epoch 41, iter 65700, avg. loss 0.00114 time elapsed 21.65sec\n",
      "epoch 41, iter 65800, avg. loss 0.00114 time elapsed 21.75sec\n",
      "epoch 41, iter 65900, avg. loss 0.00113 time elapsed 21.73sec\n",
      "epoch 41, iter 66000, avg. loss 0.00113 time elapsed 21.53sec\n",
      "epoch 41, iter 66100, avg. loss 0.00113 time elapsed 21.91sec\n",
      "epoch 41, iter 66200, avg. loss 0.00113 time elapsed 22.49sec\n",
      "epoch 41, iter 66300, avg. loss 0.00113 time elapsed 22.41sec\n",
      "epoch 41, iter 66400, avg. loss 0.00113 time elapsed 22.48sec\n",
      "epoch 41, iter 66500, avg. loss 0.00113 time elapsed 22.85sec\n",
      "epoch 41, iter 66600, avg. loss 0.00113 time elapsed 22.58sec\n",
      "==============================\n",
      "epoch 42, iter 66700, avg. loss 0.00113 time elapsed 0.93sec\n",
      "epoch 42, iter 66800, avg. loss 0.00113 time elapsed 22.96sec\n",
      "epoch 42, iter 66900, avg. loss 0.00113 time elapsed 22.72sec\n",
      "epoch 42, iter 67000, avg. loss 0.00113 time elapsed 22.92sec\n",
      "epoch 42, iter 67100, avg. loss 0.00113 time elapsed 22.77sec\n",
      "epoch 42, iter 67200, avg. loss 0.00113 time elapsed 22.42sec\n",
      "epoch 42, iter 67300, avg. loss 0.00113 time elapsed 22.32sec\n",
      "epoch 42, iter 67400, avg. loss 0.00113 time elapsed 22.29sec\n",
      "epoch 42, iter 67500, avg. loss 0.00113 time elapsed 22.20sec\n",
      "epoch 42, iter 67600, avg. loss 0.00113 time elapsed 22.17sec\n",
      "epoch 42, iter 67700, avg. loss 0.00113 time elapsed 21.86sec\n",
      "epoch 42, iter 67800, avg. loss 0.00113 time elapsed 22.15sec\n",
      "epoch 42, iter 67900, avg. loss 0.00113 time elapsed 21.70sec\n",
      "epoch 42, iter 68000, avg. loss 0.00113 time elapsed 21.73sec\n",
      "epoch 42, iter 68100, avg. loss 0.00113 time elapsed 22.02sec\n",
      "epoch 42, iter 68200, avg. loss 0.00113 time elapsed 21.68sec\n",
      "==============================\n",
      "epoch 43, iter 68300, avg. loss 0.00112 time elapsed 3.50sec\n",
      "epoch 43, iter 68400, avg. loss 0.00113 time elapsed 22.00sec\n",
      "epoch 43, iter 68500, avg. loss 0.00113 time elapsed 21.81sec\n",
      "epoch 43, iter 68600, avg. loss 0.00113 time elapsed 22.09sec\n",
      "epoch 43, iter 68700, avg. loss 0.00113 time elapsed 21.95sec\n",
      "epoch 43, iter 68800, avg. loss 0.00113 time elapsed 21.92sec\n",
      "epoch 43, iter 68900, avg. loss 0.00113 time elapsed 21.86sec\n",
      "epoch 43, iter 69000, avg. loss 0.00112 time elapsed 22.13sec\n",
      "epoch 43, iter 69100, avg. loss 0.00112 time elapsed 21.92sec\n",
      "epoch 43, iter 69200, avg. loss 0.00112 time elapsed 21.72sec\n",
      "epoch 43, iter 69300, avg. loss 0.00112 time elapsed 21.78sec\n",
      "epoch 43, iter 69400, avg. loss 0.00112 time elapsed 22.02sec\n",
      "epoch 43, iter 69500, avg. loss 0.00112 time elapsed 21.99sec\n",
      "epoch 43, iter 69600, avg. loss 0.00112 time elapsed 21.79sec\n",
      "epoch 43, iter 69700, avg. loss 0.00112 time elapsed 22.24sec\n",
      "epoch 43, iter 69800, avg. loss 0.00112 time elapsed 22.34sec\n",
      "==============================\n",
      "epoch 44, iter 69900, avg. loss 0.00112 time elapsed 6.38sec\n",
      "epoch 44, iter 70000, avg. loss 0.00112 time elapsed 22.62sec\n",
      "epoch 44, iter 70000, cum. loss 0.00114 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 70000, dev. acc 0.6793\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_44.checkpoint]\n",
      "epoch 44, iter 70100, avg. loss 0.00112 time elapsed 48.57sec\n",
      "epoch 44, iter 70200, avg. loss 0.00112 time elapsed 22.33sec\n",
      "epoch 44, iter 70300, avg. loss 0.00112 time elapsed 22.00sec\n",
      "epoch 44, iter 70400, avg. loss 0.00112 time elapsed 21.89sec\n",
      "epoch 44, iter 70500, avg. loss 0.00112 time elapsed 21.72sec\n",
      "epoch 44, iter 70600, avg. loss 0.00112 time elapsed 21.68sec\n",
      "epoch 44, iter 70700, avg. loss 0.00112 time elapsed 21.72sec\n",
      "epoch 44, iter 70800, avg. loss 0.00112 time elapsed 21.83sec\n",
      "epoch 44, iter 70900, avg. loss 0.00112 time elapsed 22.18sec\n",
      "epoch 44, iter 71000, avg. loss 0.00112 time elapsed 22.67sec\n",
      "epoch 44, iter 71100, avg. loss 0.00112 time elapsed 22.46sec\n",
      "epoch 44, iter 71200, avg. loss 0.00112 time elapsed 22.39sec\n",
      "epoch 44, iter 71300, avg. loss 0.00112 time elapsed 21.93sec\n",
      "epoch 44, iter 71400, avg. loss 0.00112 time elapsed 22.21sec\n",
      "==============================\n",
      "epoch 45, iter 71500, avg. loss 0.00112 time elapsed 9.22sec\n",
      "epoch 45, iter 71600, avg. loss 0.00112 time elapsed 22.93sec\n",
      "epoch 45, iter 71700, avg. loss 0.00112 time elapsed 22.48sec\n",
      "epoch 45, iter 71800, avg. loss 0.00112 time elapsed 22.81sec\n",
      "epoch 45, iter 71900, avg. loss 0.00112 time elapsed 22.59sec\n",
      "epoch 45, iter 72000, avg. loss 0.00112 time elapsed 22.33sec\n",
      "epoch 45, iter 72100, avg. loss 0.00112 time elapsed 22.57sec\n",
      "epoch 45, iter 72200, avg. loss 0.00112 time elapsed 22.46sec\n",
      "epoch 45, iter 72300, avg. loss 0.00112 time elapsed 22.58sec\n",
      "epoch 45, iter 72400, avg. loss 0.00112 time elapsed 22.51sec\n",
      "epoch 45, iter 72500, avg. loss 0.00111 time elapsed 22.26sec\n",
      "epoch 45, iter 72600, avg. loss 0.00112 time elapsed 22.67sec\n",
      "epoch 45, iter 72700, avg. loss 0.00111 time elapsed 22.08sec\n",
      "epoch 45, iter 72800, avg. loss 0.00111 time elapsed 22.58sec\n",
      "epoch 45, iter 72900, avg. loss 0.00112 time elapsed 22.23sec\n",
      "epoch 45, iter 73000, avg. loss 0.00111 time elapsed 22.40sec\n",
      "==============================\n",
      "epoch 46, iter 73100, avg. loss 0.00111 time elapsed 11.91sec\n",
      "epoch 46, iter 73200, avg. loss 0.00112 time elapsed 22.60sec\n",
      "epoch 46, iter 73300, avg. loss 0.00112 time elapsed 22.25sec\n",
      "epoch 46, iter 73400, avg. loss 0.00112 time elapsed 22.92sec\n",
      "epoch 46, iter 73500, avg. loss 0.00112 time elapsed 22.11sec\n",
      "epoch 46, iter 73600, avg. loss 0.00111 time elapsed 22.22sec\n",
      "epoch 46, iter 73700, avg. loss 0.00112 time elapsed 22.30sec\n",
      "epoch 46, iter 73800, avg. loss 0.00111 time elapsed 22.14sec\n",
      "epoch 46, iter 73900, avg. loss 0.00111 time elapsed 21.58sec\n",
      "epoch 46, iter 74000, avg. loss 0.00111 time elapsed 21.88sec\n",
      "epoch 46, iter 74100, avg. loss 0.00111 time elapsed 21.95sec\n",
      "epoch 46, iter 74200, avg. loss 0.00112 time elapsed 22.01sec\n",
      "epoch 46, iter 74300, avg. loss 0.00111 time elapsed 21.61sec\n",
      "epoch 46, iter 74400, avg. loss 0.00111 time elapsed 21.99sec\n",
      "epoch 46, iter 74500, avg. loss 0.00111 time elapsed 21.66sec\n",
      "epoch 46, iter 74600, avg. loss 0.00111 time elapsed 21.81sec\n",
      "==============================\n",
      "epoch 47, iter 74700, avg. loss 0.00111 time elapsed 14.12sec\n",
      "epoch 47, iter 74800, avg. loss 0.00111 time elapsed 21.98sec\n",
      "epoch 47, iter 74900, avg. loss 0.00112 time elapsed 21.64sec\n",
      "epoch 47, iter 75000, avg. loss 0.00111 time elapsed 22.29sec\n",
      "epoch 47, iter 75100, avg. loss 0.00112 time elapsed 22.08sec\n",
      "epoch 47, iter 75200, avg. loss 0.00111 time elapsed 21.63sec\n",
      "epoch 47, iter 75300, avg. loss 0.00112 time elapsed 21.72sec\n",
      "epoch 47, iter 75400, avg. loss 0.00111 time elapsed 21.65sec\n",
      "epoch 47, iter 75500, avg. loss 0.00111 time elapsed 21.65sec\n",
      "epoch 47, iter 75600, avg. loss 0.00111 time elapsed 21.96sec\n",
      "epoch 47, iter 75700, avg. loss 0.00111 time elapsed 21.84sec\n",
      "epoch 47, iter 75800, avg. loss 0.00111 time elapsed 22.13sec\n",
      "epoch 47, iter 75900, avg. loss 0.00110 time elapsed 21.73sec\n",
      "epoch 47, iter 76000, avg. loss 0.00110 time elapsed 21.98sec\n",
      "epoch 47, iter 76100, avg. loss 0.00111 time elapsed 22.12sec\n",
      "epoch 47, iter 76200, avg. loss 0.00111 time elapsed 22.20sec\n",
      "==============================\n",
      "epoch 48, iter 76300, avg. loss 0.00111 time elapsed 16.76sec\n",
      "epoch 48, iter 76400, avg. loss 0.00111 time elapsed 22.07sec\n",
      "epoch 48, iter 76500, avg. loss 0.00111 time elapsed 22.32sec\n",
      "epoch 48, iter 76600, avg. loss 0.00111 time elapsed 22.84sec\n",
      "epoch 48, iter 76700, avg. loss 0.00111 time elapsed 22.62sec\n",
      "epoch 48, iter 76800, avg. loss 0.00111 time elapsed 22.40sec\n",
      "epoch 48, iter 76900, avg. loss 0.00111 time elapsed 22.18sec\n",
      "epoch 48, iter 77000, avg. loss 0.00111 time elapsed 22.58sec\n",
      "epoch 48, iter 77100, avg. loss 0.00110 time elapsed 22.74sec\n",
      "epoch 48, iter 77200, avg. loss 0.00110 time elapsed 22.87sec\n",
      "epoch 48, iter 77300, avg. loss 0.00111 time elapsed 22.52sec\n",
      "epoch 48, iter 77400, avg. loss 0.00111 time elapsed 22.49sec\n",
      "epoch 48, iter 77500, avg. loss 0.00110 time elapsed 22.34sec\n",
      "epoch 48, iter 77600, avg. loss 0.00111 time elapsed 22.46sec\n",
      "epoch 48, iter 77700, avg. loss 0.00110 time elapsed 22.08sec\n",
      "epoch 48, iter 77800, avg. loss 0.00110 time elapsed 22.30sec\n",
      "==============================\n",
      "epoch 49, iter 77900, avg. loss 0.00110 time elapsed 19.39sec\n",
      "epoch 49, iter 78000, avg. loss 0.00111 time elapsed 21.79sec\n",
      "epoch 49, iter 78100, avg. loss 0.00111 time elapsed 21.89sec\n",
      "epoch 49, iter 78200, avg. loss 0.00111 time elapsed 22.16sec\n",
      "epoch 49, iter 78300, avg. loss 0.00111 time elapsed 22.02sec\n",
      "epoch 49, iter 78400, avg. loss 0.00110 time elapsed 21.65sec\n",
      "epoch 49, iter 78500, avg. loss 0.00110 time elapsed 21.68sec\n",
      "epoch 49, iter 78600, avg. loss 0.00110 time elapsed 21.73sec\n",
      "epoch 49, iter 78700, avg. loss 0.00110 time elapsed 22.09sec\n",
      "epoch 49, iter 78800, avg. loss 0.00110 time elapsed 22.53sec\n",
      "epoch 49, iter 78900, avg. loss 0.00110 time elapsed 22.63sec\n",
      "epoch 49, iter 79000, avg. loss 0.00110 time elapsed 22.31sec\n",
      "epoch 49, iter 79100, avg. loss 0.00110 time elapsed 22.12sec\n",
      "epoch 49, iter 79200, avg. loss 0.00110 time elapsed 21.96sec\n",
      "epoch 49, iter 79300, avg. loss 0.00110 time elapsed 22.06sec\n",
      "epoch 49, iter 79400, avg. loss 0.00110 time elapsed 22.36sec\n",
      "==============================\n",
      "epoch 50, iter 79500, avg. loss 0.00110 time elapsed 22.76sec\n",
      "epoch 50, iter 79600, avg. loss 0.00110 time elapsed 22.58sec\n",
      "epoch 50, iter 79700, avg. loss 0.00110 time elapsed 22.16sec\n",
      "epoch 50, iter 79800, avg. loss 0.00110 time elapsed 22.78sec\n",
      "epoch 50, iter 79900, avg. loss 0.00110 time elapsed 22.59sec\n",
      "epoch 50, iter 80000, avg. loss 0.00110 time elapsed 22.62sec\n",
      "epoch 50, iter 80000, cum. loss 0.00111 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 80000, dev. acc 0.6842\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_50.checkpoint]\n",
      "epoch 50, iter 80100, avg. loss 0.00110 time elapsed 49.24sec\n",
      "epoch 50, iter 80200, avg. loss 0.00110 time elapsed 22.59sec\n",
      "epoch 50, iter 80300, avg. loss 0.00110 time elapsed 22.58sec\n",
      "epoch 50, iter 80400, avg. loss 0.00110 time elapsed 23.06sec\n",
      "epoch 50, iter 80500, avg. loss 0.00110 time elapsed 23.12sec\n",
      "epoch 50, iter 80600, avg. loss 0.00110 time elapsed 22.73sec\n",
      "epoch 50, iter 80700, avg. loss 0.00110 time elapsed 22.70sec\n",
      "epoch 50, iter 80800, avg. loss 0.00110 time elapsed 23.04sec\n",
      "epoch 50, iter 80900, avg. loss 0.00110 time elapsed 22.72sec\n",
      "==============================\n",
      "epoch 51, iter 81000, avg. loss 0.00110 time elapsed 2.74sec\n",
      "epoch 51, iter 81100, avg. loss 0.00110 time elapsed 23.11sec\n",
      "epoch 51, iter 81200, avg. loss 0.00110 time elapsed 22.79sec\n",
      "epoch 51, iter 81300, avg. loss 0.00110 time elapsed 23.15sec\n",
      "epoch 51, iter 81400, avg. loss 0.00110 time elapsed 23.08sec\n",
      "epoch 51, iter 81500, avg. loss 0.00110 time elapsed 22.84sec\n",
      "epoch 51, iter 81600, avg. loss 0.00110 time elapsed 22.33sec\n",
      "epoch 51, iter 81700, avg. loss 0.00110 time elapsed 22.49sec\n",
      "epoch 51, iter 81800, avg. loss 0.00110 time elapsed 22.76sec\n",
      "epoch 51, iter 81900, avg. loss 0.00110 time elapsed 22.73sec\n",
      "epoch 51, iter 82000, avg. loss 0.00109 time elapsed 22.84sec\n",
      "epoch 51, iter 82100, avg. loss 0.00110 time elapsed 23.11sec\n",
      "epoch 51, iter 82200, avg. loss 0.00110 time elapsed 22.71sec\n",
      "epoch 51, iter 82300, avg. loss 0.00109 time elapsed 22.16sec\n",
      "epoch 51, iter 82400, avg. loss 0.00110 time elapsed 22.05sec\n",
      "epoch 51, iter 82500, avg. loss 0.00109 time elapsed 22.01sec\n",
      "==============================\n",
      "epoch 52, iter 82600, avg. loss 0.00109 time elapsed 5.60sec\n",
      "epoch 52, iter 82700, avg. loss 0.00110 time elapsed 23.00sec\n",
      "epoch 52, iter 82800, avg. loss 0.00110 time elapsed 22.62sec\n",
      "epoch 52, iter 82900, avg. loss 0.00110 time elapsed 22.80sec\n",
      "epoch 52, iter 83000, avg. loss 0.00109 time elapsed 22.56sec\n",
      "epoch 52, iter 83100, avg. loss 0.00109 time elapsed 22.47sec\n",
      "epoch 52, iter 83200, avg. loss 0.00110 time elapsed 22.27sec\n",
      "epoch 52, iter 83300, avg. loss 0.00109 time elapsed 22.23sec\n",
      "epoch 52, iter 83400, avg. loss 0.00109 time elapsed 22.38sec\n",
      "epoch 52, iter 83500, avg. loss 0.00109 time elapsed 22.41sec\n",
      "epoch 52, iter 83600, avg. loss 0.00109 time elapsed 22.37sec\n",
      "epoch 52, iter 83700, avg. loss 0.00109 time elapsed 22.69sec\n",
      "epoch 52, iter 83800, avg. loss 0.00109 time elapsed 22.32sec\n",
      "epoch 52, iter 83900, avg. loss 0.00109 time elapsed 21.90sec\n",
      "epoch 52, iter 84000, avg. loss 0.00110 time elapsed 22.40sec\n",
      "epoch 52, iter 84100, avg. loss 0.00109 time elapsed 22.39sec\n",
      "==============================\n",
      "epoch 53, iter 84200, avg. loss 0.00109 time elapsed 8.25sec\n",
      "epoch 53, iter 84300, avg. loss 0.00109 time elapsed 22.56sec\n",
      "epoch 53, iter 84400, avg. loss 0.00110 time elapsed 22.22sec\n",
      "epoch 53, iter 84500, avg. loss 0.00109 time elapsed 22.79sec\n",
      "epoch 53, iter 84600, avg. loss 0.00110 time elapsed 22.55sec\n",
      "epoch 53, iter 84700, avg. loss 0.00109 time elapsed 22.41sec\n",
      "epoch 53, iter 84800, avg. loss 0.00110 time elapsed 22.22sec\n",
      "epoch 53, iter 84900, avg. loss 0.00109 time elapsed 22.23sec\n",
      "epoch 53, iter 85000, avg. loss 0.00109 time elapsed 21.77sec\n",
      "epoch 53, iter 85100, avg. loss 0.00109 time elapsed 21.88sec\n",
      "epoch 53, iter 85200, avg. loss 0.00109 time elapsed 21.78sec\n",
      "epoch 53, iter 85300, avg. loss 0.00109 time elapsed 22.12sec\n",
      "epoch 53, iter 85400, avg. loss 0.00109 time elapsed 21.62sec\n",
      "epoch 53, iter 85500, avg. loss 0.00109 time elapsed 21.95sec\n",
      "epoch 53, iter 85600, avg. loss 0.00109 time elapsed 21.82sec\n",
      "epoch 53, iter 85700, avg. loss 0.00109 time elapsed 21.68sec\n",
      "==============================\n",
      "epoch 54, iter 85800, avg. loss 0.00109 time elapsed 11.17sec\n",
      "epoch 54, iter 85900, avg. loss 0.00109 time elapsed 22.55sec\n",
      "epoch 54, iter 86000, avg. loss 0.00109 time elapsed 22.10sec\n",
      "epoch 54, iter 86100, avg. loss 0.00109 time elapsed 22.24sec\n",
      "epoch 54, iter 86200, avg. loss 0.00109 time elapsed 22.02sec\n",
      "epoch 54, iter 86300, avg. loss 0.00109 time elapsed 21.73sec\n",
      "epoch 54, iter 86400, avg. loss 0.00109 time elapsed 21.66sec\n",
      "epoch 54, iter 86500, avg. loss 0.00109 time elapsed 22.14sec\n",
      "epoch 54, iter 86600, avg. loss 0.00108 time elapsed 22.58sec\n",
      "epoch 54, iter 86700, avg. loss 0.00109 time elapsed 22.86sec\n",
      "epoch 54, iter 86800, avg. loss 0.00109 time elapsed 22.76sec\n",
      "epoch 54, iter 86900, avg. loss 0.00109 time elapsed 22.14sec\n",
      "epoch 54, iter 87000, avg. loss 0.00108 time elapsed 22.59sec\n",
      "epoch 54, iter 87100, avg. loss 0.00108 time elapsed 22.48sec\n",
      "epoch 54, iter 87200, avg. loss 0.00109 time elapsed 22.15sec\n",
      "epoch 54, iter 87300, avg. loss 0.00109 time elapsed 21.73sec\n",
      "==============================\n",
      "epoch 55, iter 87400, avg. loss 0.00108 time elapsed 13.20sec\n",
      "epoch 55, iter 87500, avg. loss 0.00109 time elapsed 21.91sec\n",
      "epoch 55, iter 87600, avg. loss 0.00109 time elapsed 21.70sec\n",
      "epoch 55, iter 87700, avg. loss 0.00109 time elapsed 22.24sec\n",
      "epoch 55, iter 87800, avg. loss 0.00109 time elapsed 22.25sec\n",
      "epoch 55, iter 87900, avg. loss 0.00109 time elapsed 21.93sec\n",
      "epoch 55, iter 88000, avg. loss 0.00109 time elapsed 21.98sec\n",
      "epoch 55, iter 88100, avg. loss 0.00108 time elapsed 21.88sec\n",
      "epoch 55, iter 88200, avg. loss 0.00108 time elapsed 21.63sec\n",
      "epoch 55, iter 88300, avg. loss 0.00108 time elapsed 21.95sec\n",
      "epoch 55, iter 88400, avg. loss 0.00108 time elapsed 21.84sec\n",
      "epoch 55, iter 88500, avg. loss 0.00109 time elapsed 22.12sec\n",
      "epoch 55, iter 88600, avg. loss 0.00108 time elapsed 21.66sec\n",
      "epoch 55, iter 88700, avg. loss 0.00108 time elapsed 21.89sec\n",
      "epoch 55, iter 88800, avg. loss 0.00108 time elapsed 21.79sec\n",
      "epoch 55, iter 88900, avg. loss 0.00108 time elapsed 22.25sec\n",
      "==============================\n",
      "epoch 56, iter 89000, avg. loss 0.00108 time elapsed 16.22sec\n",
      "epoch 56, iter 89100, avg. loss 0.00109 time elapsed 21.90sec\n",
      "epoch 56, iter 89200, avg. loss 0.00108 time elapsed 21.71sec\n",
      "epoch 56, iter 89300, avg. loss 0.00108 time elapsed 22.29sec\n",
      "epoch 56, iter 89400, avg. loss 0.00108 time elapsed 22.03sec\n",
      "epoch 56, iter 89500, avg. loss 0.00108 time elapsed 21.99sec\n",
      "epoch 56, iter 89600, avg. loss 0.00109 time elapsed 21.84sec\n",
      "epoch 56, iter 89700, avg. loss 0.00108 time elapsed 21.79sec\n",
      "epoch 56, iter 89800, avg. loss 0.00108 time elapsed 21.66sec\n",
      "epoch 56, iter 89900, avg. loss 0.00108 time elapsed 21.93sec\n",
      "epoch 56, iter 90000, avg. loss 0.00108 time elapsed 21.92sec\n",
      "epoch 56, iter 90000, cum. loss 0.00109 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 90000, dev. acc 0.6897\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_56.checkpoint]\n",
      "epoch 56, iter 90100, avg. loss 0.00108 time elapsed 48.16sec\n",
      "epoch 56, iter 90200, avg. loss 0.00108 time elapsed 21.72sec\n",
      "epoch 56, iter 90300, avg. loss 0.00108 time elapsed 21.95sec\n",
      "epoch 56, iter 90400, avg. loss 0.00108 time elapsed 21.87sec\n",
      "epoch 56, iter 90500, avg. loss 0.00108 time elapsed 21.96sec\n",
      "==============================\n",
      "epoch 57, iter 90600, avg. loss 0.00108 time elapsed 18.52sec\n",
      "epoch 57, iter 90700, avg. loss 0.00108 time elapsed 21.89sec\n",
      "epoch 57, iter 90800, avg. loss 0.00108 time elapsed 21.96sec\n",
      "epoch 57, iter 90900, avg. loss 0.00108 time elapsed 22.80sec\n",
      "epoch 57, iter 91000, avg. loss 0.00108 time elapsed 22.52sec\n",
      "epoch 57, iter 91100, avg. loss 0.00108 time elapsed 22.34sec\n",
      "epoch 57, iter 91200, avg. loss 0.00108 time elapsed 22.11sec\n",
      "epoch 57, iter 91300, avg. loss 0.00108 time elapsed 22.00sec\n",
      "epoch 57, iter 91400, avg. loss 0.00108 time elapsed 21.74sec\n",
      "epoch 57, iter 91500, avg. loss 0.00107 time elapsed 22.13sec\n",
      "epoch 57, iter 91600, avg. loss 0.00108 time elapsed 22.51sec\n",
      "epoch 57, iter 91700, avg. loss 0.00108 time elapsed 22.14sec\n",
      "epoch 57, iter 91800, avg. loss 0.00107 time elapsed 21.87sec\n",
      "epoch 57, iter 91900, avg. loss 0.00108 time elapsed 22.40sec\n",
      "epoch 57, iter 92000, avg. loss 0.00108 time elapsed 22.16sec\n",
      "epoch 57, iter 92100, avg. loss 0.00108 time elapsed 22.01sec\n",
      "==============================\n",
      "epoch 58, iter 92200, avg. loss 0.00108 time elapsed 21.09sec\n",
      "epoch 58, iter 92300, avg. loss 0.00108 time elapsed 21.79sec\n",
      "epoch 58, iter 92400, avg. loss 0.00108 time elapsed 21.89sec\n",
      "epoch 58, iter 92500, avg. loss 0.00108 time elapsed 22.25sec\n",
      "epoch 58, iter 92600, avg. loss 0.00108 time elapsed 21.91sec\n",
      "epoch 58, iter 92700, avg. loss 0.00108 time elapsed 21.84sec\n",
      "epoch 58, iter 92800, avg. loss 0.00108 time elapsed 21.89sec\n",
      "epoch 58, iter 92900, avg. loss 0.00108 time elapsed 22.06sec\n",
      "epoch 58, iter 93000, avg. loss 0.00107 time elapsed 22.16sec\n",
      "epoch 58, iter 93100, avg. loss 0.00107 time elapsed 22.08sec\n",
      "epoch 58, iter 93200, avg. loss 0.00107 time elapsed 22.37sec\n",
      "epoch 58, iter 93300, avg. loss 0.00108 time elapsed 21.91sec\n",
      "epoch 58, iter 93400, avg. loss 0.00107 time elapsed 21.73sec\n",
      "epoch 58, iter 93500, avg. loss 0.00107 time elapsed 21.96sec\n",
      "epoch 58, iter 93600, avg. loss 0.00107 time elapsed 21.82sec\n",
      "==============================\n",
      "epoch 59, iter 93700, avg. loss 0.00107 time elapsed 1.77sec\n",
      "epoch 59, iter 93800, avg. loss 0.00107 time elapsed 22.01sec\n",
      "epoch 59, iter 93900, avg. loss 0.00108 time elapsed 21.76sec\n",
      "epoch 59, iter 94000, avg. loss 0.00107 time elapsed 22.21sec\n",
      "epoch 59, iter 94100, avg. loss 0.00107 time elapsed 22.06sec\n",
      "epoch 59, iter 94200, avg. loss 0.00108 time elapsed 22.10sec\n",
      "epoch 59, iter 94300, avg. loss 0.00108 time elapsed 21.69sec\n",
      "epoch 59, iter 94400, avg. loss 0.00107 time elapsed 21.54sec\n",
      "epoch 59, iter 94500, avg. loss 0.00107 time elapsed 21.74sec\n",
      "epoch 59, iter 94600, avg. loss 0.00107 time elapsed 21.70sec\n",
      "epoch 59, iter 94700, avg. loss 0.00107 time elapsed 21.80sec\n",
      "epoch 59, iter 94800, avg. loss 0.00107 time elapsed 22.60sec\n",
      "epoch 59, iter 94900, avg. loss 0.00107 time elapsed 22.09sec\n",
      "epoch 59, iter 95000, avg. loss 0.00107 time elapsed 21.77sec\n",
      "epoch 59, iter 95100, avg. loss 0.00107 time elapsed 22.02sec\n",
      "epoch 59, iter 95200, avg. loss 0.00107 time elapsed 21.91sec\n",
      "==============================\n",
      "epoch 60, iter 95300, avg. loss 0.00107 time elapsed 4.43sec\n",
      "epoch 60, iter 95400, avg. loss 0.00107 time elapsed 22.27sec\n",
      "epoch 60, iter 95500, avg. loss 0.00107 time elapsed 22.31sec\n",
      "epoch 60, iter 95600, avg. loss 0.00107 time elapsed 22.73sec\n",
      "epoch 60, iter 95700, avg. loss 0.00107 time elapsed 22.30sec\n",
      "epoch 60, iter 95800, avg. loss 0.00107 time elapsed 21.96sec\n",
      "epoch 60, iter 95900, avg. loss 0.00107 time elapsed 21.58sec\n",
      "epoch 60, iter 96000, avg. loss 0.00107 time elapsed 21.98sec\n",
      "epoch 60, iter 96100, avg. loss 0.00107 time elapsed 21.79sec\n",
      "epoch 60, iter 96200, avg. loss 0.00107 time elapsed 21.71sec\n",
      "epoch 60, iter 96300, avg. loss 0.00107 time elapsed 21.83sec\n",
      "epoch 60, iter 96400, avg. loss 0.00107 time elapsed 22.09sec\n",
      "epoch 60, iter 96500, avg. loss 0.00107 time elapsed 21.91sec\n",
      "epoch 60, iter 96600, avg. loss 0.00107 time elapsed 21.74sec\n",
      "epoch 60, iter 96700, avg. loss 0.00107 time elapsed 21.90sec\n",
      "epoch 60, iter 96800, avg. loss 0.00107 time elapsed 21.93sec\n",
      "==============================\n",
      "epoch 61, iter 96900, avg. loss 0.00107 time elapsed 7.16sec\n",
      "epoch 61, iter 97000, avg. loss 0.00107 time elapsed 22.10sec\n",
      "epoch 61, iter 97100, avg. loss 0.00107 time elapsed 21.75sec\n",
      "epoch 61, iter 97200, avg. loss 0.00107 time elapsed 22.21sec\n",
      "epoch 61, iter 97300, avg. loss 0.00107 time elapsed 22.23sec\n",
      "epoch 61, iter 97400, avg. loss 0.00107 time elapsed 21.87sec\n",
      "epoch 61, iter 97500, avg. loss 0.00107 time elapsed 21.68sec\n",
      "epoch 61, iter 97600, avg. loss 0.00107 time elapsed 21.79sec\n",
      "epoch 61, iter 97700, avg. loss 0.00107 time elapsed 21.90sec\n",
      "epoch 61, iter 97800, avg. loss 0.00107 time elapsed 21.90sec\n",
      "epoch 61, iter 97900, avg. loss 0.00107 time elapsed 22.17sec\n",
      "epoch 61, iter 98000, avg. loss 0.00107 time elapsed 22.65sec\n",
      "epoch 61, iter 98100, avg. loss 0.00107 time elapsed 22.19sec\n",
      "epoch 61, iter 98200, avg. loss 0.00106 time elapsed 22.74sec\n",
      "epoch 61, iter 98300, avg. loss 0.00107 time elapsed 22.27sec\n",
      "epoch 61, iter 98400, avg. loss 0.00106 time elapsed 22.46sec\n",
      "==============================\n",
      "epoch 62, iter 98500, avg. loss 0.00106 time elapsed 10.14sec\n",
      "epoch 62, iter 98600, avg. loss 0.00107 time elapsed 23.04sec\n",
      "epoch 62, iter 98700, avg. loss 0.00107 time elapsed 22.62sec\n",
      "epoch 62, iter 98800, avg. loss 0.00106 time elapsed 22.85sec\n",
      "epoch 62, iter 98900, avg. loss 0.00107 time elapsed 22.46sec\n",
      "epoch 62, iter 99000, avg. loss 0.00106 time elapsed 22.23sec\n",
      "epoch 62, iter 99100, avg. loss 0.00107 time elapsed 22.32sec\n",
      "epoch 62, iter 99200, avg. loss 0.00107 time elapsed 22.02sec\n",
      "epoch 62, iter 99300, avg. loss 0.00106 time elapsed 21.56sec\n",
      "epoch 62, iter 99400, avg. loss 0.00107 time elapsed 22.58sec\n",
      "epoch 62, iter 99500, avg. loss 0.00106 time elapsed 22.95sec\n",
      "epoch 62, iter 99600, avg. loss 0.00106 time elapsed 22.22sec\n",
      "epoch 62, iter 99700, avg. loss 0.00106 time elapsed 21.71sec\n",
      "epoch 62, iter 99800, avg. loss 0.00106 time elapsed 22.09sec\n",
      "epoch 62, iter 99900, avg. loss 0.00106 time elapsed 21.99sec\n",
      "epoch 62, iter 100000, avg. loss 0.00106 time elapsed 22.23sec\n",
      "epoch 62, iter 100000, cum. loss 0.00107 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 100000, dev. acc 0.6940\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_62.checkpoint]\n",
      "==============================\n",
      "epoch 63, iter 100100, avg. loss 0.00106 time elapsed 12.72sec\n",
      "epoch 63, iter 100200, avg. loss 0.00107 time elapsed 22.35sec\n",
      "epoch 63, iter 100300, avg. loss 0.00106 time elapsed 22.34sec\n",
      "epoch 63, iter 100400, avg. loss 0.00106 time elapsed 22.72sec\n",
      "epoch 63, iter 100500, avg. loss 0.00106 time elapsed 22.76sec\n",
      "epoch 63, iter 100600, avg. loss 0.00106 time elapsed 22.17sec\n",
      "epoch 63, iter 100700, avg. loss 0.00107 time elapsed 22.25sec\n",
      "epoch 63, iter 100800, avg. loss 0.00106 time elapsed 22.30sec\n",
      "epoch 63, iter 100900, avg. loss 0.00106 time elapsed 22.18sec\n",
      "epoch 63, iter 101000, avg. loss 0.00106 time elapsed 22.45sec\n",
      "epoch 63, iter 101100, avg. loss 0.00106 time elapsed 22.44sec\n",
      "epoch 63, iter 101200, avg. loss 0.00106 time elapsed 22.68sec\n",
      "epoch 63, iter 101300, avg. loss 0.00106 time elapsed 22.18sec\n",
      "epoch 63, iter 101400, avg. loss 0.00106 time elapsed 22.46sec\n",
      "epoch 63, iter 101500, avg. loss 0.00106 time elapsed 22.35sec\n",
      "epoch 63, iter 101600, avg. loss 0.00106 time elapsed 21.86sec\n",
      "==============================\n",
      "epoch 64, iter 101700, avg. loss 0.00106 time elapsed 15.04sec\n",
      "epoch 64, iter 101800, avg. loss 0.00106 time elapsed 21.88sec\n",
      "epoch 64, iter 101900, avg. loss 0.00106 time elapsed 21.74sec\n",
      "epoch 64, iter 102000, avg. loss 0.00106 time elapsed 22.24sec\n",
      "epoch 64, iter 102100, avg. loss 0.00106 time elapsed 22.08sec\n",
      "epoch 64, iter 102200, avg. loss 0.00106 time elapsed 21.76sec\n",
      "epoch 64, iter 102300, avg. loss 0.00107 time elapsed 21.66sec\n",
      "epoch 64, iter 102400, avg. loss 0.00106 time elapsed 21.77sec\n",
      "epoch 64, iter 102500, avg. loss 0.00106 time elapsed 22.29sec\n",
      "epoch 64, iter 102600, avg. loss 0.00106 time elapsed 22.46sec\n",
      "epoch 64, iter 102700, avg. loss 0.00106 time elapsed 22.12sec\n",
      "epoch 64, iter 102800, avg. loss 0.00106 time elapsed 22.13sec\n",
      "epoch 64, iter 102900, avg. loss 0.00106 time elapsed 21.65sec\n",
      "epoch 64, iter 103000, avg. loss 0.00106 time elapsed 22.00sec\n",
      "epoch 64, iter 103100, avg. loss 0.00106 time elapsed 22.12sec\n",
      "epoch 64, iter 103200, avg. loss 0.00106 time elapsed 22.39sec\n",
      "==============================\n",
      "epoch 65, iter 103300, avg. loss 0.00106 time elapsed 18.10sec\n",
      "epoch 65, iter 103400, avg. loss 0.00106 time elapsed 22.10sec\n",
      "epoch 65, iter 103500, avg. loss 0.00106 time elapsed 22.41sec\n",
      "epoch 65, iter 103600, avg. loss 0.00106 time elapsed 22.70sec\n",
      "epoch 65, iter 103700, avg. loss 0.00106 time elapsed 22.63sec\n",
      "epoch 65, iter 103800, avg. loss 0.00106 time elapsed 22.36sec\n",
      "epoch 65, iter 103900, avg. loss 0.00106 time elapsed 22.21sec\n",
      "epoch 65, iter 104000, avg. loss 0.00106 time elapsed 22.12sec\n",
      "epoch 65, iter 104100, avg. loss 0.00106 time elapsed 21.91sec\n",
      "epoch 65, iter 104200, avg. loss 0.00106 time elapsed 21.90sec\n",
      "epoch 65, iter 104300, avg. loss 0.00106 time elapsed 21.89sec\n",
      "epoch 65, iter 104400, avg. loss 0.00106 time elapsed 21.86sec\n",
      "epoch 65, iter 104500, avg. loss 0.00105 time elapsed 21.78sec\n",
      "epoch 65, iter 104600, avg. loss 0.00106 time elapsed 21.95sec\n",
      "epoch 65, iter 104700, avg. loss 0.00106 time elapsed 21.99sec\n",
      "epoch 65, iter 104800, avg. loss 0.00106 time elapsed 22.19sec\n",
      "==============================\n",
      "epoch 66, iter 104900, avg. loss 0.00106 time elapsed 20.73sec\n",
      "epoch 66, iter 105000, avg. loss 0.00106 time elapsed 22.29sec\n",
      "epoch 66, iter 105100, avg. loss 0.00106 time elapsed 22.63sec\n",
      "epoch 66, iter 105200, avg. loss 0.00106 time elapsed 22.72sec\n",
      "epoch 66, iter 105300, avg. loss 0.00106 time elapsed 22.61sec\n",
      "epoch 66, iter 105400, avg. loss 0.00106 time elapsed 22.21sec\n",
      "epoch 66, iter 105500, avg. loss 0.00106 time elapsed 22.35sec\n",
      "epoch 66, iter 105600, avg. loss 0.00106 time elapsed 22.25sec\n",
      "epoch 66, iter 105700, avg. loss 0.00105 time elapsed 22.09sec\n",
      "epoch 66, iter 105800, avg. loss 0.00105 time elapsed 22.55sec\n",
      "epoch 66, iter 105900, avg. loss 0.00106 time elapsed 22.62sec\n",
      "epoch 66, iter 106000, avg. loss 0.00106 time elapsed 22.30sec\n",
      "epoch 66, iter 106100, avg. loss 0.00105 time elapsed 22.13sec\n",
      "epoch 66, iter 106200, avg. loss 0.00106 time elapsed 21.86sec\n",
      "epoch 66, iter 106300, avg. loss 0.00105 time elapsed 21.70sec\n",
      "==============================\n",
      "epoch 67, iter 106400, avg. loss 0.00105 time elapsed 0.90sec\n",
      "epoch 67, iter 106500, avg. loss 0.00106 time elapsed 21.99sec\n",
      "epoch 67, iter 106600, avg. loss 0.00106 time elapsed 22.10sec\n",
      "epoch 67, iter 106700, avg. loss 0.00105 time elapsed 22.56sec\n",
      "epoch 67, iter 106800, avg. loss 0.00106 time elapsed 22.54sec\n",
      "epoch 67, iter 106900, avg. loss 0.00106 time elapsed 21.86sec\n",
      "epoch 67, iter 107000, avg. loss 0.00106 time elapsed 21.65sec\n",
      "epoch 67, iter 107100, avg. loss 0.00106 time elapsed 21.93sec\n",
      "epoch 67, iter 107200, avg. loss 0.00106 time elapsed 21.79sec\n",
      "epoch 67, iter 107300, avg. loss 0.00105 time elapsed 21.66sec\n",
      "epoch 67, iter 107400, avg. loss 0.00105 time elapsed 21.87sec\n",
      "epoch 67, iter 107500, avg. loss 0.00105 time elapsed 22.24sec\n",
      "epoch 67, iter 107600, avg. loss 0.00105 time elapsed 21.74sec\n",
      "epoch 67, iter 107700, avg. loss 0.00105 time elapsed 22.06sec\n",
      "epoch 67, iter 107800, avg. loss 0.00105 time elapsed 22.03sec\n",
      "epoch 67, iter 107900, avg. loss 0.00105 time elapsed 21.71sec\n",
      "==============================\n",
      "epoch 68, iter 108000, avg. loss 0.00105 time elapsed 3.59sec\n",
      "epoch 68, iter 108100, avg. loss 0.00105 time elapsed 22.27sec\n",
      "epoch 68, iter 108200, avg. loss 0.00106 time elapsed 21.95sec\n",
      "epoch 68, iter 108300, avg. loss 0.00105 time elapsed 22.13sec\n",
      "epoch 68, iter 108400, avg. loss 0.00105 time elapsed 22.01sec\n",
      "epoch 68, iter 108500, avg. loss 0.00105 time elapsed 21.99sec\n",
      "epoch 68, iter 108600, avg. loss 0.00105 time elapsed 21.61sec\n",
      "epoch 68, iter 108700, avg. loss 0.00105 time elapsed 21.62sec\n",
      "epoch 68, iter 108800, avg. loss 0.00105 time elapsed 22.06sec\n",
      "epoch 68, iter 108900, avg. loss 0.00105 time elapsed 22.07sec\n",
      "epoch 68, iter 109000, avg. loss 0.00105 time elapsed 21.93sec\n",
      "epoch 68, iter 109100, avg. loss 0.00105 time elapsed 22.31sec\n",
      "epoch 68, iter 109200, avg. loss 0.00105 time elapsed 22.59sec\n",
      "epoch 68, iter 109300, avg. loss 0.00105 time elapsed 22.55sec\n",
      "epoch 68, iter 109400, avg. loss 0.00105 time elapsed 22.24sec\n",
      "epoch 68, iter 109500, avg. loss 0.00105 time elapsed 22.40sec\n",
      "==============================\n",
      "epoch 69, iter 109600, avg. loss 0.00105 time elapsed 12.03sec\n",
      "epoch 69, iter 109700, avg. loss 0.00105 time elapsed 43.37sec\n",
      "epoch 69, iter 109800, avg. loss 0.00105 time elapsed 42.93sec\n",
      "epoch 69, iter 109900, avg. loss 0.00105 time elapsed 43.71sec\n",
      "epoch 69, iter 110000, avg. loss 0.00105 time elapsed 44.17sec\n",
      "epoch 69, iter 110000, cum. loss 0.00106 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 110000, dev. acc 0.6955\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_69.checkpoint]\n",
      "epoch 69, iter 110100, avg. loss 0.00105 time elapsed 101.72sec\n",
      "epoch 69, iter 110200, avg. loss 0.00106 time elapsed 43.86sec\n",
      "epoch 69, iter 110300, avg. loss 0.00105 time elapsed 43.92sec\n",
      "epoch 69, iter 110400, avg. loss 0.00105 time elapsed 44.55sec\n",
      "epoch 69, iter 110500, avg. loss 0.00105 time elapsed 43.74sec\n",
      "epoch 69, iter 110600, avg. loss 0.00105 time elapsed 44.55sec\n",
      "epoch 69, iter 110700, avg. loss 0.00105 time elapsed 44.21sec\n",
      "epoch 69, iter 110800, avg. loss 0.00105 time elapsed 44.57sec\n",
      "epoch 69, iter 110900, avg. loss 0.00105 time elapsed 43.22sec\n",
      "epoch 69, iter 111000, avg. loss 0.00105 time elapsed 44.92sec\n",
      "epoch 69, iter 111100, avg. loss 0.00105 time elapsed 44.54sec\n",
      "==============================\n",
      "epoch 70, iter 111200, avg. loss 0.00105 time elapsed 18.15sec\n",
      "epoch 70, iter 111300, avg. loss 0.00105 time elapsed 44.76sec\n",
      "epoch 70, iter 111400, avg. loss 0.00105 time elapsed 44.55sec\n",
      "epoch 70, iter 111500, avg. loss 0.00105 time elapsed 44.49sec\n",
      "epoch 70, iter 111600, avg. loss 0.00105 time elapsed 44.08sec\n",
      "epoch 70, iter 111700, avg. loss 0.00105 time elapsed 44.13sec\n",
      "epoch 70, iter 111800, avg. loss 0.00105 time elapsed 43.29sec\n",
      "epoch 70, iter 111900, avg. loss 0.00105 time elapsed 44.39sec\n",
      "epoch 70, iter 112000, avg. loss 0.00104 time elapsed 43.85sec\n",
      "epoch 70, iter 112100, avg. loss 0.00105 time elapsed 44.23sec\n",
      "epoch 70, iter 112200, avg. loss 0.00105 time elapsed 44.26sec\n",
      "epoch 70, iter 112300, avg. loss 0.00105 time elapsed 44.45sec\n",
      "epoch 70, iter 112400, avg. loss 0.00104 time elapsed 43.53sec\n",
      "epoch 70, iter 112500, avg. loss 0.00104 time elapsed 44.93sec\n",
      "epoch 70, iter 112600, avg. loss 0.00105 time elapsed 43.96sec\n",
      "epoch 70, iter 112700, avg. loss 0.00105 time elapsed 42.89sec\n",
      "==============================\n",
      "epoch 71, iter 112800, avg. loss 0.00105 time elapsed 22.97sec\n",
      "epoch 71, iter 112900, avg. loss 0.00105 time elapsed 44.29sec\n",
      "epoch 71, iter 113000, avg. loss 0.00105 time elapsed 44.79sec\n",
      "epoch 71, iter 113100, avg. loss 0.00104 time elapsed 44.73sec\n",
      "epoch 71, iter 113200, avg. loss 0.00105 time elapsed 45.38sec\n",
      "epoch 71, iter 113300, avg. loss 0.00104 time elapsed 44.14sec\n",
      "epoch 71, iter 113400, avg. loss 0.00105 time elapsed 44.48sec\n",
      "epoch 71, iter 113500, avg. loss 0.00104 time elapsed 43.51sec\n",
      "epoch 71, iter 113600, avg. loss 0.00104 time elapsed 42.91sec\n",
      "epoch 71, iter 113700, avg. loss 0.00104 time elapsed 44.58sec\n",
      "epoch 71, iter 113800, avg. loss 0.00104 time elapsed 44.88sec\n",
      "epoch 71, iter 113900, avg. loss 0.00105 time elapsed 45.06sec\n",
      "epoch 71, iter 114000, avg. loss 0.00104 time elapsed 43.25sec\n",
      "epoch 71, iter 114100, avg. loss 0.00104 time elapsed 43.48sec\n",
      "epoch 71, iter 114200, avg. loss 0.00104 time elapsed 43.43sec\n",
      "epoch 71, iter 114300, avg. loss 0.00104 time elapsed 44.33sec\n",
      "==============================\n",
      "epoch 72, iter 114400, avg. loss 0.00104 time elapsed 28.54sec\n",
      "epoch 72, iter 114500, avg. loss 0.00104 time elapsed 42.91sec\n",
      "epoch 72, iter 114600, avg. loss 0.00104 time elapsed 43.99sec\n",
      "epoch 72, iter 114700, avg. loss 0.00104 time elapsed 41.82sec\n",
      "epoch 72, iter 114800, avg. loss 0.00105 time elapsed 45.88sec\n",
      "epoch 72, iter 114900, avg. loss 0.00104 time elapsed 44.31sec\n",
      "epoch 72, iter 115000, avg. loss 0.00105 time elapsed 43.83sec\n",
      "epoch 72, iter 115100, avg. loss 0.00104 time elapsed 43.27sec\n",
      "epoch 72, iter 115200, avg. loss 0.00104 time elapsed 33.34sec\n",
      "epoch 72, iter 115300, avg. loss 0.00104 time elapsed 41.00sec\n",
      "epoch 72, iter 115400, avg. loss 0.00104 time elapsed 44.39sec\n",
      "epoch 72, iter 115500, avg. loss 0.00104 time elapsed 44.68sec\n",
      "epoch 72, iter 115600, avg. loss 0.00104 time elapsed 44.37sec\n",
      "epoch 72, iter 115700, avg. loss 0.00104 time elapsed 43.91sec\n",
      "epoch 72, iter 115800, avg. loss 0.00104 time elapsed 44.14sec\n",
      "epoch 72, iter 115900, avg. loss 0.00104 time elapsed 44.52sec\n",
      "==============================\n",
      "epoch 73, iter 116000, avg. loss 0.00104 time elapsed 33.75sec\n",
      "epoch 73, iter 116100, avg. loss 0.00104 time elapsed 44.58sec\n",
      "epoch 73, iter 116200, avg. loss 0.00104 time elapsed 43.60sec\n",
      "epoch 73, iter 116300, avg. loss 0.00104 time elapsed 44.88sec\n",
      "epoch 73, iter 116400, avg. loss 0.00104 time elapsed 44.33sec\n",
      "epoch 73, iter 116500, avg. loss 0.00104 time elapsed 43.55sec\n",
      "epoch 73, iter 116600, avg. loss 0.00104 time elapsed 43.18sec\n",
      "epoch 73, iter 116700, avg. loss 0.00104 time elapsed 43.83sec\n",
      "epoch 73, iter 116800, avg. loss 0.00104 time elapsed 42.69sec\n",
      "epoch 73, iter 116900, avg. loss 0.00104 time elapsed 44.49sec\n",
      "epoch 73, iter 117000, avg. loss 0.00104 time elapsed 44.65sec\n",
      "epoch 73, iter 117100, avg. loss 0.00104 time elapsed 43.70sec\n",
      "epoch 73, iter 117200, avg. loss 0.00104 time elapsed 42.87sec\n",
      "epoch 73, iter 117300, avg. loss 0.00104 time elapsed 43.82sec\n",
      "epoch 73, iter 117400, avg. loss 0.00104 time elapsed 44.17sec\n",
      "epoch 73, iter 117500, avg. loss 0.00104 time elapsed 45.16sec\n",
      "==============================\n",
      "epoch 74, iter 117600, avg. loss 0.00104 time elapsed 39.72sec\n",
      "epoch 74, iter 117700, avg. loss 0.00104 time elapsed 43.80sec\n",
      "epoch 74, iter 117800, avg. loss 0.00104 time elapsed 44.39sec\n",
      "epoch 74, iter 117900, avg. loss 0.00104 time elapsed 44.52sec\n",
      "epoch 74, iter 118000, avg. loss 0.00104 time elapsed 44.34sec\n",
      "epoch 74, iter 118100, avg. loss 0.00104 time elapsed 43.57sec\n",
      "epoch 74, iter 118200, avg. loss 0.00104 time elapsed 43.69sec\n",
      "epoch 74, iter 118300, avg. loss 0.00104 time elapsed 44.54sec\n",
      "epoch 74, iter 118400, avg. loss 0.00104 time elapsed 43.62sec\n",
      "epoch 74, iter 118500, avg. loss 0.00103 time elapsed 43.37sec\n",
      "epoch 74, iter 118600, avg. loss 0.00104 time elapsed 44.49sec\n",
      "epoch 74, iter 118700, avg. loss 0.00104 time elapsed 44.46sec\n",
      "epoch 74, iter 118800, avg. loss 0.00103 time elapsed 43.77sec\n",
      "epoch 74, iter 118900, avg. loss 0.00104 time elapsed 44.44sec\n",
      "epoch 74, iter 119000, avg. loss 0.00104 time elapsed 43.63sec\n",
      "epoch 74, iter 119100, avg. loss 0.00104 time elapsed 44.30sec\n",
      "==============================\n",
      "epoch 75, iter 119200, avg. loss 0.00104 time elapsed 43.92sec\n",
      "epoch 75, iter 119300, avg. loss 0.00104 time elapsed 44.25sec\n",
      "epoch 75, iter 119400, avg. loss 0.00104 time elapsed 44.98sec\n",
      "epoch 75, iter 119500, avg. loss 0.00104 time elapsed 44.85sec\n",
      "epoch 75, iter 119600, avg. loss 0.00104 time elapsed 44.45sec\n",
      "epoch 75, iter 119700, avg. loss 0.00104 time elapsed 44.51sec\n",
      "epoch 75, iter 119800, avg. loss 0.00104 time elapsed 44.14sec\n",
      "epoch 75, iter 119900, avg. loss 0.00104 time elapsed 43.95sec\n",
      "epoch 75, iter 120000, avg. loss 0.00103 time elapsed 43.85sec\n",
      "epoch 75, iter 120000, cum. loss 0.00104 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 120000, dev. acc 0.7002\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_75.checkpoint]\n",
      "epoch 75, iter 120100, avg. loss 0.00104 time elapsed 104.68sec\n",
      "epoch 75, iter 120200, avg. loss 0.00104 time elapsed 44.49sec\n",
      "epoch 75, iter 120300, avg. loss 0.00104 time elapsed 44.03sec\n",
      "epoch 75, iter 120400, avg. loss 0.00103 time elapsed 43.85sec\n",
      "epoch 75, iter 120500, avg. loss 0.00104 time elapsed 45.22sec\n",
      "epoch 75, iter 120600, avg. loss 0.00103 time elapsed 44.33sec\n",
      "==============================\n",
      "epoch 76, iter 120700, avg. loss 0.00103 time elapsed 5.33sec\n",
      "epoch 76, iter 120800, avg. loss 0.00104 time elapsed 38.63sec\n",
      "epoch 76, iter 120900, avg. loss 0.00104 time elapsed 38.67sec\n",
      "epoch 76, iter 121000, avg. loss 0.00104 time elapsed 44.30sec\n",
      "epoch 76, iter 121100, avg. loss 0.00103 time elapsed 44.10sec\n",
      "epoch 76, iter 121200, avg. loss 0.00104 time elapsed 44.35sec\n",
      "epoch 76, iter 121300, avg. loss 0.00104 time elapsed 44.15sec\n",
      "epoch 76, iter 121400, avg. loss 0.00103 time elapsed 43.08sec\n",
      "epoch 76, iter 121500, avg. loss 0.00103 time elapsed 43.38sec\n",
      "epoch 76, iter 121600, avg. loss 0.00103 time elapsed 43.22sec\n",
      "epoch 76, iter 121700, avg. loss 0.00103 time elapsed 44.73sec\n",
      "epoch 76, iter 121800, avg. loss 0.00103 time elapsed 44.30sec\n",
      "epoch 76, iter 121900, avg. loss 0.00103 time elapsed 44.22sec\n",
      "epoch 76, iter 122000, avg. loss 0.00103 time elapsed 42.90sec\n",
      "epoch 76, iter 122100, avg. loss 0.00103 time elapsed 43.80sec\n",
      "epoch 76, iter 122200, avg. loss 0.00103 time elapsed 43.62sec\n",
      "==============================\n",
      "epoch 77, iter 122300, avg. loss 0.00103 time elapsed 11.21sec\n",
      "epoch 77, iter 122400, avg. loss 0.00103 time elapsed 44.44sec\n",
      "epoch 77, iter 122500, avg. loss 0.00103 time elapsed 43.80sec\n",
      "epoch 77, iter 122600, avg. loss 0.00103 time elapsed 45.33sec\n",
      "epoch 77, iter 122700, avg. loss 0.00103 time elapsed 44.19sec\n",
      "epoch 77, iter 122800, avg. loss 0.00104 time elapsed 44.38sec\n",
      "epoch 77, iter 122900, avg. loss 0.00104 time elapsed 43.42sec\n",
      "epoch 77, iter 123000, avg. loss 0.00103 time elapsed 43.74sec\n",
      "epoch 77, iter 123100, avg. loss 0.00103 time elapsed 43.31sec\n",
      "epoch 77, iter 123200, avg. loss 0.00103 time elapsed 43.73sec\n",
      "epoch 77, iter 123300, avg. loss 0.00103 time elapsed 44.44sec\n",
      "epoch 77, iter 123400, avg. loss 0.00103 time elapsed 45.44sec\n",
      "epoch 77, iter 123500, avg. loss 0.00103 time elapsed 43.03sec\n",
      "epoch 77, iter 123600, avg. loss 0.00103 time elapsed 43.66sec\n",
      "epoch 77, iter 123700, avg. loss 0.00103 time elapsed 43.86sec\n",
      "epoch 77, iter 123800, avg. loss 0.00103 time elapsed 43.72sec\n",
      "==============================\n",
      "epoch 78, iter 123900, avg. loss 0.00103 time elapsed 16.06sec\n",
      "epoch 78, iter 124000, avg. loss 0.00104 time elapsed 44.28sec\n",
      "epoch 78, iter 124100, avg. loss 0.00103 time elapsed 44.09sec\n",
      "epoch 78, iter 124200, avg. loss 0.00103 time elapsed 45.09sec\n",
      "epoch 78, iter 124300, avg. loss 0.00103 time elapsed 44.39sec\n",
      "epoch 78, iter 124400, avg. loss 0.00104 time elapsed 42.87sec\n",
      "epoch 78, iter 124500, avg. loss 0.00103 time elapsed 43.82sec\n",
      "epoch 78, iter 124600, avg. loss 0.00103 time elapsed 43.06sec\n",
      "epoch 78, iter 124700, avg. loss 0.00103 time elapsed 43.04sec\n",
      "epoch 78, iter 124800, avg. loss 0.00103 time elapsed 44.78sec\n",
      "epoch 78, iter 124900, avg. loss 0.00103 time elapsed 43.11sec\n",
      "epoch 78, iter 125000, avg. loss 0.00103 time elapsed 44.97sec\n",
      "epoch 78, iter 125100, avg. loss 0.00103 time elapsed 44.01sec\n",
      "epoch 78, iter 125200, avg. loss 0.00103 time elapsed 44.85sec\n",
      "epoch 78, iter 125300, avg. loss 0.00103 time elapsed 44.75sec\n",
      "epoch 78, iter 125400, avg. loss 0.00103 time elapsed 43.19sec\n",
      "==============================\n",
      "epoch 79, iter 125500, avg. loss 0.00103 time elapsed 21.38sec\n",
      "epoch 79, iter 125600, avg. loss 0.00103 time elapsed 44.89sec\n",
      "epoch 79, iter 125700, avg. loss 0.00103 time elapsed 44.29sec\n",
      "epoch 79, iter 125800, avg. loss 0.00103 time elapsed 44.49sec\n",
      "epoch 79, iter 125900, avg. loss 0.00103 time elapsed 44.77sec\n",
      "epoch 79, iter 126000, avg. loss 0.00103 time elapsed 43.91sec\n",
      "epoch 79, iter 126100, avg. loss 0.00103 time elapsed 43.15sec\n",
      "epoch 79, iter 126200, avg. loss 0.00103 time elapsed 43.90sec\n",
      "epoch 79, iter 126300, avg. loss 0.00103 time elapsed 43.06sec\n",
      "epoch 79, iter 126400, avg. loss 0.00103 time elapsed 44.97sec\n",
      "epoch 79, iter 126500, avg. loss 0.00103 time elapsed 43.45sec\n",
      "epoch 79, iter 126600, avg. loss 0.00103 time elapsed 37.39sec\n",
      "epoch 79, iter 126700, avg. loss 0.00103 time elapsed 36.30sec\n",
      "epoch 79, iter 126800, avg. loss 0.00103 time elapsed 44.42sec\n",
      "epoch 79, iter 126900, avg. loss 0.00103 time elapsed 43.37sec\n",
      "epoch 79, iter 127000, avg. loss 0.00103 time elapsed 43.44sec\n",
      "==============================\n",
      "epoch 80, iter 127100, avg. loss 0.00103 time elapsed 26.42sec\n",
      "epoch 80, iter 127200, avg. loss 0.00103 time elapsed 43.11sec\n",
      "epoch 80, iter 127300, avg. loss 0.00103 time elapsed 44.23sec\n",
      "epoch 80, iter 127400, avg. loss 0.00103 time elapsed 44.75sec\n",
      "epoch 80, iter 127500, avg. loss 0.00103 time elapsed 44.49sec\n",
      "epoch 80, iter 127600, avg. loss 0.00103 time elapsed 43.29sec\n",
      "epoch 80, iter 127700, avg. loss 0.00103 time elapsed 42.96sec\n",
      "epoch 80, iter 127800, avg. loss 0.00103 time elapsed 44.25sec\n",
      "epoch 80, iter 127900, avg. loss 0.00102 time elapsed 42.91sec\n",
      "epoch 80, iter 128000, avg. loss 0.00103 time elapsed 43.60sec\n",
      "epoch 80, iter 128100, avg. loss 0.00103 time elapsed 43.57sec\n",
      "epoch 80, iter 128200, avg. loss 0.00103 time elapsed 44.11sec\n",
      "epoch 80, iter 128300, avg. loss 0.00103 time elapsed 43.25sec\n",
      "epoch 80, iter 128400, avg. loss 0.00102 time elapsed 43.49sec\n",
      "epoch 80, iter 128500, avg. loss 0.00103 time elapsed 43.30sec\n",
      "epoch 80, iter 128600, avg. loss 0.00102 time elapsed 43.52sec\n",
      "==============================\n",
      "epoch 81, iter 128700, avg. loss 0.00102 time elapsed 31.57sec\n",
      "epoch 81, iter 128800, avg. loss 0.00103 time elapsed 44.13sec\n",
      "epoch 81, iter 128900, avg. loss 0.00103 time elapsed 43.46sec\n",
      "epoch 81, iter 129000, avg. loss 0.00103 time elapsed 45.60sec\n",
      "epoch 81, iter 129100, avg. loss 0.00103 time elapsed 43.91sec\n",
      "epoch 81, iter 129200, avg. loss 0.00103 time elapsed 44.59sec\n",
      "epoch 81, iter 129300, avg. loss 0.00103 time elapsed 43.40sec\n",
      "epoch 81, iter 129400, avg. loss 0.00103 time elapsed 42.08sec\n",
      "epoch 81, iter 129500, avg. loss 0.00102 time elapsed 43.53sec\n",
      "epoch 81, iter 129600, avg. loss 0.00102 time elapsed 44.43sec\n",
      "epoch 81, iter 129700, avg. loss 0.00102 time elapsed 44.25sec\n",
      "epoch 81, iter 129800, avg. loss 0.00103 time elapsed 44.24sec\n",
      "epoch 81, iter 129900, avg. loss 0.00102 time elapsed 43.76sec\n",
      "epoch 81, iter 130000, avg. loss 0.00102 time elapsed 43.83sec\n",
      "epoch 81, iter 130000, cum. loss 0.00103 examples 10240000.0\n",
      "begin evaluation...\n",
      "validation: iter 130000, dev. acc 0.7011\n",
      "save currently the best model to [model/bert.checkpointBest_epoch_81.checkpoint]\n",
      "epoch 81, iter 130100, avg. loss 0.00102 time elapsed 101.60sec\n",
      "epoch 81, iter 130200, avg. loss 0.00102 time elapsed 44.10sec\n",
      "==============================\n",
      "epoch 82, iter 130300, avg. loss 0.00102 time elapsed 37.64sec\n",
      "epoch 82, iter 130400, avg. loss 0.00102 time elapsed 44.52sec\n",
      "epoch 82, iter 130500, avg. loss 0.00102 time elapsed 44.44sec\n",
      "epoch 82, iter 130600, avg. loss 0.00103 time elapsed 44.66sec\n",
      "epoch 82, iter 130700, avg. loss 0.00102 time elapsed 44.12sec\n",
      "epoch 82, iter 130800, avg. loss 0.00103 time elapsed 44.42sec\n",
      "epoch 82, iter 130900, avg. loss 0.00103 time elapsed 44.89sec\n",
      "epoch 82, iter 131000, avg. loss 0.00103 time elapsed 43.37sec\n",
      "epoch 82, iter 131100, avg. loss 0.00102 time elapsed 43.48sec\n",
      "epoch 82, iter 131200, avg. loss 0.00102 time elapsed 44.01sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(x, y)\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model_opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m model_opt\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_predict(model, input_str, vocab, mask_token=mask_token):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    word = list(input_str.replace(\" \", \"\"))\n",
    "    \n",
    "    vocab_size = len(vocab.char2id)\n",
    "    word_token = torch.from_numpy(np.array([vocab.char2id[c] for c in word])).unsqueeze(0).to(model.device)\n",
    "    word_token_mask = word_token.unsqueeze(1).to(model.device)\n",
    "    print(word_token, word_token_mask)\n",
    "    out = model.forward(word_token, word_token_mask)\n",
    "    generator_mask = torch.zeros(word_token.shape[0], vocab_size, device=model.device)\n",
    "    generator_mask = generator_mask.scatter_(1, word_token, mask_token)\n",
    "\n",
    "    #batch_loss = loss_compute(out, batch.tgt, generator_mask)\n",
    "    x = model.generator(out, generator_mask)\n",
    "    x = x.masked_fill(generator_mask == mask_token, -1e9)\n",
    "    x = (torch.argmax((nn.Softmax(dim=1)(x))).cpu().numpy()).astype(int)\n",
    "    x = vocab.id2char[int(x)]\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_str = \"a p p l _ \"\n",
    "model_predict(model, input_str, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
