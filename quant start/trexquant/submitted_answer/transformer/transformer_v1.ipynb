{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from vocabulary import Vocab\n",
    "from utils import *\n",
    "from embedding import PositionalEncoding, Embeddings\n",
    "from layers import *\n",
    "from criterion import KLLossMasked\n",
    "from optimizer import NoamOpt\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every_iter = 100\n",
    "validate_every_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        result, _ = torch.max(x, dim=0)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'model_v1/'\n",
    "if not os.path.isdir(directory):\n",
    "    os.mkdir(directory)\n",
    "model_save_path = 'bert.checkpoint'\n",
    "model_save_path = os.path.join(directory, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_size = False\n",
    "use_checkpoint = False\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "d_model = 512\n",
    "d_hid = 2048\n",
    "n_heads = 8\n",
    "n_encoders = 6\n",
    "dropout = 0.1\n",
    "\n",
    "batch_size = 512\n",
    "train_iter = report_loss = cum_loss = valid_num = 0\n",
    "report_samples = cum_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "ntoken = len(vocab.char2id)\n",
    "print(ntoken)\n",
    "model = TransformerModel(ntoken, d_model, n_heads, d_hid, n_encoders, dropout)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(device)\n",
    "\n",
    "generator = Generator()\n",
    "for p in generator.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "generator = generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-9, betas=(0.9, 0.99), eps=1e-9)\n",
    "#criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "criterion = nn.KLDivLoss(reduction='sum')\n",
    "\n",
    "train_data = read_train_data(filepath=\"./pairs_train.txt\", small = small_size)\n",
    "dev_data = read_dev_data(filepath=\"./pairs_valid.txt\", small = small_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_valid_scores = []\n",
    "current_epoch = 0\n",
    "current_train_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "epoch 0 step 0 batchloss: 1173.3553466796875\n",
      "epoch 0 step 100 batchloss: 1165.57470703125\n",
      "epoch 0 step 200 batchloss: 1151.71728515625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(current_epoch, 1000):\n",
    "    print(\"=\" * 30)\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    train_data_iter = create_words_batch(train_data, vocab, mini_batch=batch_size, shuffle=False, device = device)\n",
    "    for i, batch in enumerate(train_data_iter):\n",
    "        output = model.forward(batch.src.transpose(1, 0), None)\n",
    "        generator_mask = torch.zeros(batch.src.shape[0], ntoken, device = device)\n",
    "        \n",
    "        generator_mask = generator_mask.scatter_(1, batch.src, mask_token)\n",
    "\n",
    "\n",
    "        x = generator(output)\n",
    "        x = x.masked_fill(generator_mask == mask_token, -1e9)\n",
    "        x = nn.LogSoftmax(dim=1)(x)\n",
    "        y = batch.tgt.masked_fill(batch.tgt == generator_mask, 0)\n",
    "        y = y/(torch.sum(y, dim=1, keepdim=True) + 1e-12)\n",
    "        #y = nn.LogSoftmax(dim=1)(y)\n",
    "        loss = criterion(x, y)\n",
    "        batch_loss = loss.data\n",
    "        #print(x)\n",
    "        if(i % 100 == 0):\n",
    "            print(\"epoch %d step %d batchloss:\"%(epoch, i), loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    directory = 'model/'\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory)\n",
    "    model_save_path = 'bert.checkpoint'\n",
    "    model_save_path = os.path.join(directory, model_save_path)\n",
    "\n",
    "    small_size = False\n",
    "    use_checkpoint = False\n",
    "    use_cuda = True\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "    vocab = Vocab()\n",
    "    V = len(vocab.char2id)\n",
    "    d_model = 256\n",
    "    d_ff = 1024\n",
    "    h = 4\n",
    "    n_encoders = 4\n",
    "\n",
    "    batch_size = 32\n",
    "    train_iter = report_loss = cum_loss = valid_num = 0\n",
    "    report_samples = cum_samples = 0\n",
    "\n",
    "    self_attn = MultiHeadedAttention(h=h, d_model=d_model, d_k=d_model // h, d_v=d_model // h, dropout=0.1)\n",
    "    feed_forward = FullyConnectedFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "    position = PositionalEncoding(d_model, dropout=0.1)\n",
    "    embedding = nn.Sequential(Embeddings(d_model=d_model, vocab=V), position)\n",
    "\n",
    "    encoder = Encoder(self_attn=self_attn, feed_forward=feed_forward, size=d_model, dropout=0.1)\n",
    "    generator = Generator(d_model=d_model, vocab_size=V)\n",
    "    model = Bert(encoder=encoder, embedding=embedding, generator=generator, n_layers=n_encoders)\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-9, betas=(0.9, 0.98), eps=1e-9)\n",
    "    model_opt = NoamOpt(d_model, 0.001, 4000, opt)\n",
    "    #criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if use_cuda:\n",
    "        criterion.cuda(device=device)\n",
    "    vocab = Vocab()\n",
    "\n",
    "    train_data = read_train_data(filepath=\"./pairs_train.txt\", small = small_size)\n",
    "    dev_data = read_dev_data(filepath=\"./pairs_valid.txt\", small = small_size)\n",
    "\n",
    "    \n",
    "    hist_valid_scores = []\n",
    "\n",
    "\n",
    "    current_epoch = 0\n",
    "    current_train_iter = 0\n",
    "\n",
    "    for epoch in range(current_epoch, 100):\n",
    "        print(\"=\" * 30)\n",
    "        model.train()\n",
    "\n",
    "        start = time.time()\n",
    "        train_data_iter = create_words_batch(train_data, vocab, mini_batch=batch_size, shuffle=False,\n",
    "                                                device=model.device)\n",
    "        for i, batch in enumerate(train_data_iter):\n",
    "            out = model.forward(batch.src, batch.src_mask)\n",
    "            generator_mask = torch.zeros(batch.src.shape[0], V, device=model.device)\n",
    "            generator_mask = generator_mask.scatter_(1, batch.src, 1)\n",
    "\n",
    "            #batch_loss = loss_compute(out, batch.tgt, generator_mask)\n",
    "\n",
    "            x = model.generator(out, generator_mask)\n",
    "            x = x.masked_fill(generator_mask == 1, -1e9)\n",
    "            x = nn.Softmax(dim=1)(x)\n",
    "            y = batch.tgt\n",
    "            #y = batch.tgt.masked_fill(batch.tgt == generator_mask, -1e9)\n",
    "            #y = nn.LogSoftmax(dim=1)(y)\n",
    "            loss = criterion(x, y)\n",
    "            batch_loss = loss.data\n",
    "            print(\"step %d batchloss:\"%(train_iter), loss)\n",
    "            loss.backward()\n",
    "            model_opt.step()\n",
    "            model_opt.optimizer.zero_grad()\n",
    "\n",
    "            batch_loss_val = batch_loss.item()\n",
    "            report_loss += batch_loss_val\n",
    "            cum_loss += batch_loss_val\n",
    "            report_samples += batch_size\n",
    "            cum_samples += batch_size\n",
    "\n",
    "            train_iter += 1\n",
    "\n",
    "            if train_iter % log_every_iter == 0:\n",
    "                elapsed = time.time() - start\n",
    "                print(f'epoch {epoch}, iter {train_iter}, avg. loss {report_loss / report_samples:.2f} time elapsed {elapsed:.2f}sec')\n",
    "                start = time.time()\n",
    "                report_loss = report_samples = 0\n",
    "\n",
    "            if train_iter % validate_every_iter == 0:\n",
    "                print(f'epoch {epoch}, iter {train_iter}, cum. loss {cum_loss / cum_samples:.2f} examples {cum_samples}')\n",
    "                cum_samples = cum_loss = 0.\n",
    "\n",
    "                print('begin evaluation...')\n",
    "                valid_num += 1\n",
    "                acc = evaluate_acc(model, vocab, dev_data, device=model.device)\n",
    "                print(f'validation: iter {train_iter}, dev. acc {acc:.4f}')\n",
    "\n",
    "                valid_metric = acc\n",
    "\n",
    "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
    "                hist_valid_scores.append(valid_metric)\n",
    "\n",
    "                if is_better:\n",
    "                    print('save currently the best model to [%s]' % model_save_path)\n",
    "                    torch.save({'epoch': epoch,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': model_opt.optimizer.state_dict(),\n",
    "                                'loss': cum_loss,\n",
    "                                '_rate': model_opt._rate,\n",
    "                                '_step': model_opt._step,\n",
    "                                'train_iter': train_iter,\n",
    "                                'hist_valid_scores': hist_valid_scores,\n",
    "                                }, model_save_path)\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': model_opt.optimizer.state_dict(),\n",
    "                    'loss': cum_loss,\n",
    "                    '_rate': model_opt._rate,\n",
    "                    '_step': model_opt._step,\n",
    "                    'train_iter': train_iter,\n",
    "                    'hist_valid_scores': hist_valid_scores,\n",
    "                    }, os.path.join(directory, f'real_model_{epoch}.checkpoint'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
