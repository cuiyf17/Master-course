{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from vocabulary import Vocab\n",
    "from utils import *\n",
    "from embedding import PositionalEncoding, Embeddings\n",
    "from layers import *\n",
    "from criterion import KLLossMasked\n",
    "from optimizer import NoamOpt\n",
    "from bert import Bert, Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every_iter = 100\n",
    "validate_every_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'model/'\n",
    "if not os.path.isdir(directory):\n",
    "    os.mkdir(directory)\n",
    "model_save_path = 'bert.checkpoint'\n",
    "model_save_path = os.path.join(directory, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_size = False\n",
    "use_checkpoint = False\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "vocab = Vocab()\n",
    "V = len(vocab.char2id)\n",
    "d_model = 256\n",
    "d_ff = 1024\n",
    "h = 4\n",
    "n_encoders = 4\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = MultiHeadedAttention(h=h, d_model=d_model, d_k=d_model // h, d_v=d_model // h, dropout=0.1)\n",
    "feed_forward = FullyConnectedFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "position = PositionalEncoding(d_model, dropout=0.1)\n",
    "embedding = nn.Sequential(Embeddings(d_model=d_model, vocab=V), position)\n",
    "\n",
    "encoder = Encoder(self_attn=self_attn, feed_forward=feed_forward, size=d_model, dropout=0.1)\n",
    "generator = Generator(d_model=d_model, vocab_size=V)\n",
    "model = Bert(encoder=encoder, embedding=embedding, generator=generator, n_layers=n_encoders)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-9, betas=(0.9, 0.98), eps=1e-9)\n",
    "model_opt = NoamOpt(d_model, 2, 4000, opt)\n",
    "criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "if use_cuda:\n",
    "    criterion.cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_train_data(filepath=\"./pairs_train.txt\", small = small_size)\n",
    "dev_data = read_dev_data(filepath=\"./pairs_valid.txt\", small = small_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_valid_scores = []\n",
    "\n",
    "if use_checkpoint:\n",
    "    checkpoint = torch.load(model_save_path)\n",
    "    current_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    step = checkpoint['_step']\n",
    "    rate = checkpoint['_rate']\n",
    "    current_train_iter = checkpoint['train_iter']\n",
    "    model_opt._step = step\n",
    "    model_opt._rate = rate\n",
    "    print(f'reading checkpoint from epoch {current_epoch}, iter {current_train_iter}')\n",
    "else:\n",
    "    current_epoch = 0\n",
    "    current_train_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    train_iter = report_loss = cum_loss = valid_num = 0\n",
    "    report_samples = cum_samples = 0\n",
    "    for epoch in range(current_epoch, 1000):\n",
    "        print(\"=\" * 30)\n",
    "        model.train()\n",
    "        #loss_compute = KLLossMasked(model.generator, criterion, opt=model_opt)\n",
    "\n",
    "        start = time.time()\n",
    "        train_data_iter = create_words_batch(train_data, vocab, mini_batch=batch_size, shuffle=False, device=model.device)\n",
    "        for i, batch in enumerate(train_data_iter):\n",
    "            if use_checkpoint and train_iter <= current_train_iter:\n",
    "                train_iter = current_train_iter\n",
    "                continue\n",
    "            out = model.forward(batch.src, batch.src_mask)\n",
    "            generator_mask = torch.zeros(batch.src.shape[0], V, device=model.device)\n",
    "            generator_mask = generator_mask.scatter_(1, batch.src, mask_token)\n",
    "\n",
    "            #batch_loss = loss_compute(out, batch.tgt, generator_mask)\n",
    "            x = model.generator(out, generator_mask)\n",
    "            x = x.masked_fill(generator_mask == mask_token, -1e9)\n",
    "            x = nn.LogSoftmax(dim=1)(x)\n",
    "            y = batch.tgt.masked_fill(generator_mask == mask_token, 0)\n",
    "            y = y/torch.sum(y, dim=1, keepdim=True)\n",
    "            #y = nn.Softmax(dim=1)(y)\n",
    "            batch_loss = criterion(x, y)\n",
    "            loss = batch_loss\n",
    "            loss.backward()\n",
    "\n",
    "            model_opt.step()\n",
    "            model_opt.optimizer.zero_grad()\n",
    "\n",
    "            batch_loss_val = batch_loss.item()\n",
    "            report_loss += batch_loss_val\n",
    "            cum_loss += batch_loss_val\n",
    "            report_samples += batch_size\n",
    "            cum_samples += batch_size\n",
    "\n",
    "            train_iter += 1\n",
    "\n",
    "            if train_iter % log_every_iter == 0:\n",
    "                elapsed = time.time() - start\n",
    "                print(f'epoch {epoch}, iter {train_iter}, avg. loss {report_loss / report_samples:.2f} time elapsed {elapsed:.2f}sec')\n",
    "                start = time.time()\n",
    "                report_loss = report_samples = 0\n",
    "\n",
    "            if train_iter % validate_every_iter == 0:\n",
    "                print(f'epoch {epoch}, iter {train_iter}, cum. loss {cum_loss / cum_samples:.2f} examples {cum_samples}')\n",
    "                cum_samples = cum_loss = 0.\n",
    "\n",
    "                print('begin evaluation...')\n",
    "                valid_num += 1\n",
    "                acc = evaluate_acc(model, vocab, dev_data, device=model.device)\n",
    "                print(f'validation: iter {train_iter}, dev. acc {acc:.4f}')\n",
    "\n",
    "                valid_metric = acc\n",
    "\n",
    "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
    "                hist_valid_scores.append(valid_metric)\n",
    "\n",
    "                if is_better:\n",
    "                    print('save currently the best model to [%s]' % model_save_path)\n",
    "                    torch.save({'epoch': epoch,\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': model_opt.optimizer.state_dict(),\n",
    "                                'loss': cum_loss,\n",
    "                                '_rate': model_opt._rate,\n",
    "                                '_step': model_opt._step,\n",
    "                                'train_iter': train_iter,\n",
    "                                'hist_valid_scores': hist_valid_scores,\n",
    "                                }, model_save_path)\n",
    "\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': model_opt.optimizer.state_dict(),\n",
    "                    'loss': cum_loss,\n",
    "                    '_rate': model_opt._rate,\n",
    "                    '_step': model_opt._step,\n",
    "                    'train_iter': train_iter,\n",
    "                    'hist_valid_scores': hist_valid_scores,\n",
    "                    }, os.path.join(directory, f'real_model_{epoch}.checkpoint'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "epoch 0, iter 100, avg. loss 1.72 time elapsed 1.76sec\n",
      "epoch 0, iter 200, avg. loss 1.45 time elapsed 1.74sec\n",
      "epoch 0, iter 300, avg. loss 1.43 time elapsed 1.72sec\n",
      "epoch 0, iter 400, avg. loss 1.41 time elapsed 1.72sec\n",
      "epoch 0, iter 500, avg. loss 1.41 time elapsed 1.71sec\n",
      "epoch 0, iter 600, avg. loss 1.40 time elapsed 1.73sec\n",
      "epoch 0, iter 700, avg. loss 1.41 time elapsed 1.77sec\n",
      "epoch 0, iter 800, avg. loss 1.40 time elapsed 1.70sec\n",
      "epoch 0, iter 900, avg. loss 1.39 time elapsed 1.70sec\n",
      "epoch 0, iter 1000, avg. loss 1.39 time elapsed 1.74sec\n",
      "epoch 0, iter 1100, avg. loss 1.39 time elapsed 1.68sec\n",
      "epoch 0, iter 1200, avg. loss 1.38 time elapsed 1.62sec\n",
      "epoch 0, iter 1300, avg. loss 1.37 time elapsed 1.59sec\n",
      "epoch 0, iter 1400, avg. loss 1.38 time elapsed 1.58sec\n",
      "epoch 0, iter 1500, avg. loss 1.37 time elapsed 1.59sec\n",
      "epoch 0, iter 1600, avg. loss 1.38 time elapsed 1.60sec\n",
      "epoch 0, iter 1700, avg. loss 1.37 time elapsed 1.61sec\n",
      "epoch 0, iter 1800, avg. loss 1.37 time elapsed 1.83sec\n",
      "epoch 0, iter 1900, avg. loss 1.37 time elapsed 1.78sec\n",
      "epoch 0, iter 2000, avg. loss 1.36 time elapsed 1.71sec\n",
      "epoch 0, iter 2100, avg. loss 1.37 time elapsed 1.63sec\n",
      "epoch 0, iter 2200, avg. loss 1.36 time elapsed 1.62sec\n",
      "epoch 0, iter 2300, avg. loss 1.36 time elapsed 1.77sec\n",
      "epoch 0, iter 2398, cum. loss 1.40 examples 613888\n",
      "begin evaluation...\n",
      "validation: iter 2398, dev. acc 0.4450\n",
      "save currently the best model to [model/bert.checkpointbest_epoch0.checkpoint]\n",
      "==============================\n",
      "epoch 1, iter 2400, avg. loss 1.35 time elapsed 0.03sec\n",
      "epoch 1, iter 2500, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 1, iter 2600, avg. loss 1.36 time elapsed 1.63sec\n",
      "epoch 1, iter 2700, avg. loss 1.37 time elapsed 1.67sec\n",
      "epoch 1, iter 2800, avg. loss 1.36 time elapsed 1.67sec\n",
      "epoch 1, iter 2900, avg. loss 1.36 time elapsed 1.68sec\n",
      "epoch 1, iter 3000, avg. loss 1.36 time elapsed 1.61sec\n",
      "epoch 1, iter 3100, avg. loss 1.37 time elapsed 1.58sec\n",
      "epoch 1, iter 3200, avg. loss 1.37 time elapsed 1.60sec\n",
      "epoch 1, iter 3300, avg. loss 1.36 time elapsed 1.58sec\n",
      "epoch 1, iter 3400, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 1, iter 3500, avg. loss 1.37 time elapsed 1.67sec\n",
      "epoch 1, iter 3600, avg. loss 1.37 time elapsed 1.60sec\n",
      "epoch 1, iter 3700, avg. loss 1.36 time elapsed 1.69sec\n",
      "epoch 1, iter 3800, avg. loss 1.37 time elapsed 1.70sec\n",
      "epoch 1, iter 3900, avg. loss 1.37 time elapsed 1.62sec\n",
      "epoch 1, iter 4000, avg. loss 1.38 time elapsed 1.58sec\n",
      "epoch 1, iter 4100, avg. loss 1.37 time elapsed 1.61sec\n",
      "epoch 1, iter 4200, avg. loss 1.37 time elapsed 1.60sec\n",
      "epoch 1, iter 4300, avg. loss 1.37 time elapsed 1.60sec\n",
      "epoch 1, iter 4400, avg. loss 1.37 time elapsed 1.60sec\n",
      "epoch 1, iter 4500, avg. loss 1.37 time elapsed 1.72sec\n",
      "epoch 1, iter 4600, avg. loss 1.37 time elapsed 1.80sec\n",
      "epoch 1, iter 4700, avg. loss 1.36 time elapsed 1.75sec\n",
      "epoch 1, iter 4796, cum. loss 1.37 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 4796, dev. acc 0.4307\n",
      "==============================\n",
      "epoch 2, iter 4800, avg. loss 1.36 time elapsed 0.07sec\n",
      "epoch 2, iter 4900, avg. loss 1.37 time elapsed 1.78sec\n",
      "epoch 2, iter 5000, avg. loss 1.36 time elapsed 1.77sec\n",
      "epoch 2, iter 5100, avg. loss 1.37 time elapsed 1.67sec\n",
      "epoch 2, iter 5200, avg. loss 1.36 time elapsed 1.70sec\n",
      "epoch 2, iter 5300, avg. loss 1.37 time elapsed 1.69sec\n",
      "epoch 2, iter 5400, avg. loss 1.37 time elapsed 1.71sec\n",
      "epoch 2, iter 5500, avg. loss 1.37 time elapsed 1.82sec\n",
      "epoch 2, iter 5600, avg. loss 1.37 time elapsed 1.63sec\n",
      "epoch 2, iter 5700, avg. loss 1.36 time elapsed 1.61sec\n",
      "epoch 2, iter 5800, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 2, iter 5900, avg. loss 1.37 time elapsed 1.65sec\n",
      "epoch 2, iter 6000, avg. loss 1.37 time elapsed 1.63sec\n",
      "epoch 2, iter 6100, avg. loss 1.36 time elapsed 1.61sec\n",
      "epoch 2, iter 6200, avg. loss 1.36 time elapsed 1.67sec\n",
      "epoch 2, iter 6300, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 2, iter 6400, avg. loss 1.37 time elapsed 1.58sec\n",
      "epoch 2, iter 6500, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 2, iter 6600, avg. loss 1.36 time elapsed 1.58sec\n",
      "epoch 2, iter 6700, avg. loss 1.37 time elapsed 1.61sec\n",
      "epoch 2, iter 6800, avg. loss 1.36 time elapsed 1.59sec\n",
      "epoch 2, iter 6900, avg. loss 1.36 time elapsed 1.62sec\n",
      "epoch 2, iter 7000, avg. loss 1.36 time elapsed 1.72sec\n",
      "epoch 2, iter 7100, avg. loss 1.36 time elapsed 1.63sec\n",
      "epoch 2, iter 7194, cum. loss 1.36 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 7194, dev. acc 0.4470\n",
      "save currently the best model to [model/bert.checkpointbest_epoch2.checkpoint]\n",
      "==============================\n",
      "epoch 3, iter 7200, avg. loss 1.35 time elapsed 0.11sec\n",
      "epoch 3, iter 7300, avg. loss 1.36 time elapsed 1.71sec\n",
      "epoch 3, iter 7400, avg. loss 1.35 time elapsed 1.67sec\n",
      "epoch 3, iter 7500, avg. loss 1.36 time elapsed 1.66sec\n",
      "epoch 3, iter 7600, avg. loss 1.35 time elapsed 1.62sec\n",
      "epoch 3, iter 7700, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 3, iter 7800, avg. loss 1.35 time elapsed 1.60sec\n",
      "epoch 3, iter 7900, avg. loss 1.36 time elapsed 1.65sec\n",
      "epoch 3, iter 8000, avg. loss 1.36 time elapsed 1.71sec\n",
      "epoch 3, iter 8100, avg. loss 1.35 time elapsed 1.60sec\n",
      "epoch 3, iter 8200, avg. loss 1.36 time elapsed 1.58sec\n",
      "epoch 3, iter 8300, avg. loss 1.36 time elapsed 1.58sec\n",
      "epoch 3, iter 8400, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 3, iter 8500, avg. loss 1.35 time elapsed 1.62sec\n",
      "epoch 3, iter 8600, avg. loss 1.35 time elapsed 1.67sec\n",
      "epoch 3, iter 8700, avg. loss 1.35 time elapsed 1.67sec\n",
      "epoch 3, iter 8800, avg. loss 1.36 time elapsed 1.60sec\n",
      "epoch 3, iter 8900, avg. loss 1.35 time elapsed 1.62sec\n",
      "epoch 3, iter 9000, avg. loss 1.35 time elapsed 1.60sec\n",
      "epoch 3, iter 9100, avg. loss 1.36 time elapsed 1.61sec\n",
      "epoch 3, iter 9200, avg. loss 1.35 time elapsed 1.68sec\n",
      "epoch 3, iter 9300, avg. loss 1.35 time elapsed 1.59sec\n",
      "epoch 3, iter 9400, avg. loss 1.35 time elapsed 1.60sec\n",
      "epoch 3, iter 9500, avg. loss 1.35 time elapsed 1.60sec\n",
      "epoch 3, iter 9592, cum. loss 1.35 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 9592, dev. acc 0.4434\n",
      "==============================\n",
      "epoch 4, iter 9600, avg. loss 1.34 time elapsed 0.13sec\n",
      "epoch 4, iter 9700, avg. loss 1.35 time elapsed 1.68sec\n",
      "epoch 4, iter 9800, avg. loss 1.35 time elapsed 1.76sec\n",
      "epoch 4, iter 9900, avg. loss 1.36 time elapsed 1.72sec\n",
      "epoch 4, iter 10000, avg. loss 1.35 time elapsed 1.75sec\n",
      "epoch 4, iter 10100, avg. loss 1.35 time elapsed 1.81sec\n",
      "epoch 4, iter 10200, avg. loss 1.35 time elapsed 1.79sec\n",
      "epoch 4, iter 10300, avg. loss 1.36 time elapsed 1.82sec\n",
      "epoch 4, iter 10400, avg. loss 1.36 time elapsed 1.80sec\n",
      "epoch 4, iter 10500, avg. loss 1.35 time elapsed 1.78sec\n",
      "epoch 4, iter 10600, avg. loss 1.35 time elapsed 1.71sec\n",
      "epoch 4, iter 10700, avg. loss 1.35 time elapsed 1.67sec\n",
      "epoch 4, iter 10800, avg. loss 1.35 time elapsed 1.59sec\n",
      "epoch 4, iter 10900, avg. loss 1.35 time elapsed 1.59sec\n",
      "epoch 4, iter 11000, avg. loss 1.35 time elapsed 1.61sec\n",
      "epoch 4, iter 11100, avg. loss 1.34 time elapsed 1.64sec\n",
      "epoch 4, iter 11200, avg. loss 1.35 time elapsed 1.79sec\n",
      "epoch 4, iter 11300, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 4, iter 11400, avg. loss 1.35 time elapsed 1.63sec\n",
      "epoch 4, iter 11500, avg. loss 1.35 time elapsed 1.71sec\n",
      "epoch 4, iter 11600, avg. loss 1.34 time elapsed 1.72sec\n",
      "epoch 4, iter 11700, avg. loss 1.34 time elapsed 1.76sec\n",
      "epoch 4, iter 11800, avg. loss 1.34 time elapsed 1.81sec\n",
      "epoch 4, iter 11900, avg. loss 1.34 time elapsed 1.76sec\n",
      "epoch 4, iter 11990, cum. loss 1.35 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 11990, dev. acc 0.4546\n",
      "save currently the best model to [model/bert.checkpointbest_epoch4.checkpoint]\n",
      "==============================\n",
      "epoch 5, iter 12000, avg. loss 1.33 time elapsed 0.18sec\n",
      "epoch 5, iter 12100, avg. loss 1.34 time elapsed 1.70sec\n",
      "epoch 5, iter 12200, avg. loss 1.34 time elapsed 1.63sec\n",
      "epoch 5, iter 12300, avg. loss 1.35 time elapsed 1.69sec\n",
      "epoch 5, iter 12400, avg. loss 1.34 time elapsed 1.71sec\n",
      "epoch 5, iter 12500, avg. loss 1.34 time elapsed 1.65sec\n",
      "epoch 5, iter 12600, avg. loss 1.34 time elapsed 1.62sec\n",
      "epoch 5, iter 12700, avg. loss 1.35 time elapsed 1.65sec\n",
      "epoch 5, iter 12800, avg. loss 1.35 time elapsed 1.61sec\n",
      "epoch 5, iter 12900, avg. loss 1.34 time elapsed 1.69sec\n",
      "epoch 5, iter 13000, avg. loss 1.35 time elapsed 1.70sec\n",
      "epoch 5, iter 13100, avg. loss 1.34 time elapsed 1.69sec\n",
      "epoch 5, iter 13200, avg. loss 1.34 time elapsed 1.70sec\n",
      "epoch 5, iter 13300, avg. loss 1.34 time elapsed 1.70sec\n",
      "epoch 5, iter 13400, avg. loss 1.34 time elapsed 1.75sec\n",
      "epoch 5, iter 13500, avg. loss 1.34 time elapsed 1.72sec\n",
      "epoch 5, iter 13600, avg. loss 1.35 time elapsed 1.76sec\n",
      "epoch 5, iter 13700, avg. loss 1.34 time elapsed 1.73sec\n",
      "epoch 5, iter 13800, avg. loss 1.34 time elapsed 1.66sec\n",
      "epoch 5, iter 13900, avg. loss 1.34 time elapsed 1.67sec\n",
      "epoch 5, iter 14000, avg. loss 1.34 time elapsed 1.64sec\n",
      "epoch 5, iter 14100, avg. loss 1.34 time elapsed 1.66sec\n",
      "epoch 5, iter 14200, avg. loss 1.34 time elapsed 1.66sec\n",
      "epoch 5, iter 14300, avg. loss 1.34 time elapsed 1.67sec\n",
      "epoch 5, iter 14388, cum. loss 1.34 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 14388, dev. acc 0.4528\n",
      "==============================\n",
      "epoch 6, iter 14400, avg. loss 1.33 time elapsed 0.20sec\n",
      "epoch 6, iter 14500, avg. loss 1.34 time elapsed 1.79sec\n",
      "epoch 6, iter 14600, avg. loss 1.34 time elapsed 1.73sec\n",
      "epoch 6, iter 14700, avg. loss 1.34 time elapsed 1.72sec\n",
      "epoch 6, iter 14800, avg. loss 1.34 time elapsed 1.70sec\n",
      "epoch 6, iter 14900, avg. loss 1.34 time elapsed 1.75sec\n",
      "epoch 6, iter 15000, avg. loss 1.34 time elapsed 1.63sec\n",
      "epoch 6, iter 15100, avg. loss 1.35 time elapsed 1.69sec\n",
      "epoch 6, iter 15200, avg. loss 1.34 time elapsed 1.66sec\n",
      "epoch 6, iter 15300, avg. loss 1.33 time elapsed 1.60sec\n",
      "epoch 6, iter 15400, avg. loss 1.34 time elapsed 1.59sec\n",
      "epoch 6, iter 15500, avg. loss 1.34 time elapsed 1.58sec\n",
      "epoch 6, iter 15600, avg. loss 1.34 time elapsed 1.59sec\n",
      "epoch 6, iter 15700, avg. loss 1.33 time elapsed 1.58sec\n",
      "epoch 6, iter 15800, avg. loss 1.34 time elapsed 1.60sec\n",
      "epoch 6, iter 15900, avg. loss 1.34 time elapsed 1.59sec\n",
      "epoch 6, iter 16000, avg. loss 1.34 time elapsed 1.58sec\n",
      "epoch 6, iter 16100, avg. loss 1.34 time elapsed 1.62sec\n",
      "epoch 6, iter 16200, avg. loss 1.34 time elapsed 1.59sec\n",
      "epoch 6, iter 16300, avg. loss 1.34 time elapsed 1.59sec\n",
      "epoch 6, iter 16400, avg. loss 1.34 time elapsed 1.57sec\n",
      "epoch 6, iter 16500, avg. loss 1.34 time elapsed 1.66sec\n",
      "epoch 6, iter 16600, avg. loss 1.33 time elapsed 1.77sec\n",
      "epoch 6, iter 16700, avg. loss 1.33 time elapsed 1.79sec\n",
      "epoch 6, iter 16786, cum. loss 1.34 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 16786, dev. acc 0.4628\n",
      "save currently the best model to [model/bert.checkpointbest_epoch6.checkpoint]\n",
      "==============================\n",
      "epoch 7, iter 16800, avg. loss 1.32 time elapsed 0.24sec\n",
      "epoch 7, iter 16900, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 7, iter 17000, avg. loss 1.33 time elapsed 1.70sec\n",
      "epoch 7, iter 17100, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 7, iter 17200, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 7, iter 17300, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 7, iter 17400, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 7, iter 17500, avg. loss 1.34 time elapsed 1.69sec\n",
      "epoch 7, iter 17600, avg. loss 1.34 time elapsed 1.65sec\n",
      "epoch 7, iter 17700, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 7, iter 17800, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 7, iter 17900, avg. loss 1.34 time elapsed 1.66sec\n",
      "epoch 7, iter 18000, avg. loss 1.34 time elapsed 1.69sec\n",
      "epoch 7, iter 18100, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 7, iter 18200, avg. loss 1.34 time elapsed 1.69sec\n",
      "epoch 7, iter 18300, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 7, iter 18400, avg. loss 1.34 time elapsed 1.66sec\n",
      "epoch 7, iter 18500, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 7, iter 18600, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 7, iter 18700, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 7, iter 18800, avg. loss 1.33 time elapsed 1.65sec\n",
      "epoch 7, iter 18900, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 7, iter 19000, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 7, iter 19100, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 7, iter 19184, cum. loss 1.33 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 19184, dev. acc 0.4585\n",
      "==============================\n",
      "epoch 8, iter 19200, avg. loss 1.32 time elapsed 0.27sec\n",
      "epoch 8, iter 19300, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 8, iter 19400, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 8, iter 19500, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 8, iter 19600, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 8, iter 19700, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 8, iter 19800, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 8, iter 19900, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 8, iter 20000, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 8, iter 20100, avg. loss 1.32 time elapsed 1.69sec\n",
      "epoch 8, iter 20200, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 8, iter 20300, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 8, iter 20400, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 8, iter 20500, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 8, iter 20600, avg. loss 1.33 time elapsed 1.70sec\n",
      "epoch 8, iter 20700, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 8, iter 20800, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 8, iter 20900, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 8, iter 21000, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 8, iter 21100, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 8, iter 21200, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 8, iter 21300, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 8, iter 21400, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 8, iter 21500, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 8, iter 21582, cum. loss 1.33 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 21582, dev. acc 0.4599\n",
      "==============================\n",
      "epoch 9, iter 21600, avg. loss 1.32 time elapsed 0.30sec\n",
      "epoch 9, iter 21700, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 9, iter 21800, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 9, iter 21900, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 9, iter 22000, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 9, iter 22100, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 9, iter 22200, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 9, iter 22300, avg. loss 1.34 time elapsed 1.68sec\n",
      "epoch 9, iter 22400, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 9, iter 22500, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 9, iter 22600, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 9, iter 22700, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 9, iter 22800, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 9, iter 22900, avg. loss 1.32 time elapsed 1.66sec\n",
      "epoch 9, iter 23000, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 9, iter 23100, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 9, iter 23200, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 9, iter 23300, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 9, iter 23400, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 9, iter 23500, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 9, iter 23600, avg. loss 1.32 time elapsed 1.66sec\n",
      "epoch 9, iter 23700, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 9, iter 23800, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 9, iter 23900, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 9, iter 23980, cum. loss 1.33 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 23980, dev. acc 0.4626\n",
      "==============================\n",
      "epoch 10, iter 24000, avg. loss 1.31 time elapsed 0.34sec\n",
      "epoch 10, iter 24100, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 10, iter 24200, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 10, iter 24300, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 10, iter 24400, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 10, iter 24500, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 10, iter 24600, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 10, iter 24700, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 10, iter 24800, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 10, iter 24900, avg. loss 1.32 time elapsed 1.69sec\n",
      "epoch 10, iter 25000, avg. loss 1.33 time elapsed 1.67sec\n",
      "epoch 10, iter 25100, avg. loss 1.32 time elapsed 1.66sec\n",
      "epoch 10, iter 25200, avg. loss 1.33 time elapsed 1.69sec\n",
      "epoch 10, iter 25300, avg. loss 1.32 time elapsed 1.66sec\n",
      "epoch 10, iter 25400, avg. loss 1.32 time elapsed 1.69sec\n",
      "epoch 10, iter 25500, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 10, iter 25600, avg. loss 1.33 time elapsed 1.66sec\n",
      "epoch 10, iter 25700, avg. loss 1.32 time elapsed 1.69sec\n",
      "epoch 10, iter 25800, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 10, iter 25900, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 10, iter 26000, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 10, iter 26100, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 10, iter 26200, avg. loss 1.31 time elapsed 1.68sec\n",
      "epoch 10, iter 26300, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 10, iter 26378, cum. loss 1.32 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 26378, dev. acc 0.4687\n",
      "save currently the best model to [model/bert.checkpointbest_epoch10.checkpoint]\n",
      "==============================\n",
      "epoch 11, iter 26400, avg. loss 1.31 time elapsed 0.37sec\n",
      "epoch 11, iter 26500, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 11, iter 26600, avg. loss 1.32 time elapsed 1.69sec\n",
      "epoch 11, iter 26700, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 11, iter 26800, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 11, iter 26900, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 11, iter 27000, avg. loss 1.32 time elapsed 1.66sec\n",
      "epoch 11, iter 27100, avg. loss 1.33 time elapsed 1.68sec\n",
      "epoch 11, iter 27200, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 11, iter 27300, avg. loss 1.32 time elapsed 1.69sec\n",
      "epoch 11, iter 27400, avg. loss 1.32 time elapsed 1.67sec\n",
      "epoch 11, iter 27500, avg. loss 1.32 time elapsed 1.66sec\n",
      "epoch 11, iter 27600, avg. loss 1.32 time elapsed 1.69sec\n",
      "epoch 11, iter 27700, avg. loss 1.31 time elapsed 1.68sec\n",
      "epoch 11, iter 27800, avg. loss 1.32 time elapsed 1.73sec\n",
      "epoch 11, iter 27900, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 11, iter 28000, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 11, iter 28100, avg. loss 1.32 time elapsed 1.70sec\n",
      "epoch 11, iter 28200, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 11, iter 28300, avg. loss 1.32 time elapsed 1.70sec\n",
      "epoch 11, iter 28400, avg. loss 1.32 time elapsed 1.76sec\n",
      "epoch 11, iter 28500, avg. loss 1.32 time elapsed 1.73sec\n",
      "epoch 11, iter 28600, avg. loss 1.31 time elapsed 1.77sec\n",
      "epoch 11, iter 28700, avg. loss 1.32 time elapsed 1.79sec\n",
      "epoch 11, iter 28776, cum. loss 1.32 examples 613888.0\n",
      "begin evaluation...\n",
      "validation: iter 28776, dev. acc 0.4701\n",
      "save currently the best model to [model/bert.checkpointbest_epoch11.checkpoint]\n",
      "==============================\n",
      "epoch 12, iter 28800, avg. loss 1.30 time elapsed 0.42sec\n",
      "epoch 12, iter 28900, avg. loss 1.32 time elapsed 1.73sec\n",
      "epoch 12, iter 29000, avg. loss 1.32 time elapsed 1.76sec\n",
      "epoch 12, iter 29100, avg. loss 1.32 time elapsed 1.72sec\n",
      "epoch 12, iter 29200, avg. loss 1.32 time elapsed 1.71sec\n",
      "epoch 12, iter 29300, avg. loss 1.31 time elapsed 1.73sec\n",
      "epoch 12, iter 29400, avg. loss 1.32 time elapsed 1.71sec\n",
      "epoch 12, iter 29500, avg. loss 1.32 time elapsed 1.68sec\n",
      "epoch 12, iter 29600, avg. loss 1.32 time elapsed 1.65sec\n",
      "epoch 12, iter 29700, avg. loss 1.31 time elapsed 1.64sec\n",
      "epoch 12, iter 29800, avg. loss 1.32 time elapsed 1.64sec\n",
      "epoch 12, iter 29900, avg. loss 1.32 time elapsed 1.73sec\n",
      "epoch 12, iter 30000, avg. loss 1.32 time elapsed 1.75sec\n",
      "epoch 12, iter 30100, avg. loss 1.31 time elapsed 1.72sec\n",
      "epoch 12, iter 30200, avg. loss 1.32 time elapsed 1.76sec\n",
      "epoch 12, iter 30300, avg. loss 1.31 time elapsed 1.73sec\n",
      "epoch 12, iter 30400, avg. loss 1.32 time elapsed 1.71sec\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
