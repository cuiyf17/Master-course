{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2865992-3b68-4b91-85d9-32de47a73b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fa0bb-835c-4321-b757-f5a64db86395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#内置的一维精确搜索算法为牛顿法\n",
    "class Func():\n",
    "    def __init__(self):\n",
    "        self.delta = 1e-16\n",
    "        self.epsilon = 1e-16\n",
    "        \n",
    "    def value(self, X: np.array((2,1))):\n",
    "        return (1-X[0,0])**2+3*(X[1,0]-X[0,0]**2)**2\n",
    "    \n",
    "    def grad(self, X: np.array((2,1))):\n",
    "        x1=-2*(1-X[0,0])-12*X[0,0]*(X[1,0]-X[0,0]**2)\n",
    "        x2=6*(X[1,0]-X[0,0]**2)\n",
    "        return np.array([[x1],[x2]])\n",
    "        \n",
    "    def hessian(self, X: np.array((2,1))):\n",
    "        x11=2-12*(X[1,0]-3*X[0,0]**2)\n",
    "        x12=-12*X[0,0]\n",
    "        x21=-12*X[0,0]\n",
    "        x22=6\n",
    "        return np.array([[x11,x12],[x21,x22]])\n",
    "    \n",
    "    def diff_t(self, X: np.array((2,1)), D: np.array((2,1)), t):\n",
    "        return -2*(1-X[0,0]-D[0,0]*t)*D[0,0]+6*(X[1,0]+D[1,0]*t-(X[0,0]+D[0,0]*t)**2)*(D[1,0]-2*(X[0,0]+D[0,0]*t)*D[0,0])\n",
    "    \n",
    "    def diff_2_t(self, X: np.array((2,1)), D: np.array((2,1)), t):\n",
    "        return 2*D[0,0]**2+6*(-2*(X[1,0]+D[1,0]*t-(X[0,0]+D[0,0]*t)**2)*D[0,0]**2+(D[1,0]-2*D[0,0]*(X[0,0]+D[0,0]*t))**2)\n",
    "    \n",
    "    def Newton_1D_Method(self, X0: np.array((2,1)), D: np.array((2,1)), delta):#一维精确搜索——牛顿法\n",
    "        t_tmp=0\n",
    "        while(abs(self.diff_t(X0,D,t_tmp))>delta):\n",
    "            t_tmp=t_tmp-self.diff_t(X0,D,t_tmp)/self.diff_2_t(X0,D,t_tmp)\n",
    "        return X0+t_tmp*D\n",
    "    \n",
    "    def Method_0618(self, X0: np.array((2,1)), D: np.array((2,1)), delta):#一维精确搜索——0.618法\n",
    "        gamma = 1\n",
    "        t_tmp=0\n",
    "        ini_diff = self.diff_t(X0, D, t_tmp)\n",
    "        ini_value = self.value(X0+t_tmp*D)\n",
    "        if(abs(ini_diff)<=delta):\n",
    "            return X0+t_tmp*D\n",
    "        elif(ini_diff<0):\n",
    "            gamma = 1\n",
    "        else:\n",
    "            gamma = -1\n",
    "            \n",
    "        while(self.value(X0+(t_tmp + gamma)*D)<ini_value):\n",
    "            gamma = 2*gamma\n",
    "        \n",
    "        a_tmp = t_tmp\n",
    "        b_tmp = t_tmp + gamma\n",
    "        eps = 1e-16\n",
    "        while(abs(a_tmp-b_tmp)>eps):\n",
    "            a_0618 = (0.618*a_tmp+(1-0.618)*b_tmp)\n",
    "            b_0618 = (0.618*b_tmp+(1-0.618)*a_tmp)\n",
    "            if(self.value(X0+a_0618*D)>self.value(X0+b_0618*D)):\n",
    "                a_tmp = a_0618\n",
    "            else:\n",
    "                b_tmp = b_0618\n",
    "        t_tmp = (a_tmp+b_tmp)/2\n",
    "        \n",
    "        return X0+t_tmp*D\n",
    "    \n",
    "    def L1NormMethod(self, X0: np.array((2,1)), epsilon):#L1范数下降\n",
    "        X_tmp=X0\n",
    "        X_list=[X_tmp]\n",
    "        X_value_list=[self.value(X_tmp)]\n",
    "        while(np.dot(self.grad(X_tmp).T, self.grad(X_tmp))>epsilon):\n",
    "            graditude = np.abs(self.grad(X_tmp))\n",
    "            D_tmp=-(graditude//np.max(graditude))*self.grad(X_tmp)\n",
    "            X_tmp=self.Method_0618(X_tmp, D_tmp, self.delta)\n",
    "            X_list.append(X_tmp)\n",
    "            X_value_list.append(self.value(X_tmp))\n",
    "        return X_list, X_value_list, X_tmp, self.value(X_tmp)\n",
    "    \n",
    "    def L2NormMethod(self, X0: np.array((2,1)), epsilon):#L2范数下降\n",
    "        X_tmp=X0\n",
    "        X_list=[X_tmp]\n",
    "        X_value_list=[self.value(X_tmp)]\n",
    "        while(np.dot(self.grad(X_tmp).T, self.grad(X_tmp))>epsilon):\n",
    "            D_tmp = -self.grad(X_tmp)\n",
    "            X_tmp=self.Method_0618(X_tmp, D_tmp, self.delta)\n",
    "            X_list.append(X_tmp)\n",
    "            X_value_list.append(self.value(X_tmp))\n",
    "        return X_list, X_value_list, X_tmp, self.value(X_tmp)\n",
    "    \n",
    "    def LInfinityNormMethod(self, X0: np.array((2,1)), epsilon):#L∞范数下降\n",
    "        X_tmp=X0\n",
    "        X_list=[X_tmp]\n",
    "        X_value_list=[self.value(X_tmp)]\n",
    "        while(np.dot(self.grad(X_tmp).T, self.grad(X_tmp))>epsilon):\n",
    "            graditude = self.grad(X_tmp)\n",
    "            D_tmp=-(graditude+1e-64)/np.abs(graditude+1e-64)  #为了防止出现 0/0\n",
    "            X_tmp=self.Method_0618(X_tmp, D_tmp, self.delta)\n",
    "            X_list.append(X_tmp)\n",
    "            X_value_list.append(self.value(X_tmp))\n",
    "        return X_list, X_value_list, X_tmp, self.value(X_tmp)\n",
    "    \n",
    "    def FRMethod(self, X0: np.array((2,1)), epsilon):#FR共轭梯度下降\n",
    "        X_tmp=X0\n",
    "        X_list=[X_tmp]\n",
    "        X_value_list=[self.value(X_tmp)]\n",
    "        X_pre=None\n",
    "        k=0\n",
    "        n=2\n",
    "        while(np.dot(self.grad(X_tmp).T, self.grad(X_tmp))>epsilon):\n",
    "            graditude = self.grad(X_tmp)\n",
    "            if(k%n==0):\n",
    "                D_tmp=-graditude\n",
    "            else:\n",
    "                D_tmp=-graditude+np.dot(self.grad(X_tmp).T,self.grad(X_tmp))/np.dot(self.grad(X_pre).T,self.grad(X_pre))*D_tmp\n",
    "            X_pre=X_tmp\n",
    "            X_tmp=self.Method_0618(X_tmp, D_tmp, self.delta)\n",
    "            X_list.append(X_tmp)\n",
    "            X_value_list.append(self.value(X_tmp))\n",
    "            k+=1\n",
    "        return X_list, X_value_list, X_tmp, self.value(X_tmp)\n",
    "    \n",
    "    def PRMethod(self, X0: np.array((2,1)), epsilon):#PR共轭梯度下降\n",
    "        X_tmp=X0\n",
    "        X_list=[X_tmp]\n",
    "        X_value_list=[self.value(X_tmp)]\n",
    "        X_pre=None\n",
    "        k=0\n",
    "        n=2\n",
    "        while(np.dot(self.grad(X_tmp).T, self.grad(X_tmp))>epsilon):\n",
    "            graditude = self.grad(X_tmp)\n",
    "            if(k%n==0):\n",
    "                D_tmp=-graditude\n",
    "            else:\n",
    "                D_tmp=-graditude+np.dot(self.grad(X_tmp).T,(self.grad(X_tmp)-self.grad(X_pre)))/np.dot(self.grad(X_pre).T,self.grad(X_pre))*D_tmp\n",
    "            X_pre=X_tmp\n",
    "            X_tmp=self.Method_0618(X_tmp, D_tmp, self.delta)\n",
    "            X_list.append(X_tmp)\n",
    "            X_value_list.append(self.value(X_tmp))\n",
    "            k+=1\n",
    "        return X_list, X_value_list, X_tmp, self.value(X_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3d2cf-12e8-46c3-a592-56400258dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.array([[0],[0]])\n",
    "epsilon = 1e-8\n",
    "func=Func()\n",
    "X_list1, X_value_list1, X1, value1 = func.L1NormMethod(X0, epsilon)\n",
    "X_list2, X_value_list2, X2, value2 = func.L2NormMethod(X0, epsilon)\n",
    "X_list3, X_value_list3, X3, value3 = func.LInfinityNormMethod(X0, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12178cab-2d38-4ec1-9683-283f937cb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"L1范数下降法迭代的次数为%d, 最优解为x1 = %.16f, x2 = %.16f, 最优值为%.16f\"%(len(X_list1), X1[0,0], X1[1,0], value1))\n",
    "print(\"L2范数下降法迭代的次数为%d, 最优解为x1 = %.16f, x2 = %.16f, 最优值为%.16f\"%(len(X_list2), X2[0,0], X2[1,0], value2))\n",
    "print(\"L∞范数下降法迭代的次数为%d, 最优解为x1 = %.16f, x2 = %.16f, 最优值为%.16f\"%(len(X_list3), X3[0,0], X3[1,0], value3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce396e-951e-455b-9f37-f7ac65b12026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figures(X_list, X_value_list, title):\n",
    "    def value(x, y):\n",
    "        return (1-x)**2+3*(y-x**2)**2\n",
    "    \n",
    "    X_list = np.concatenate(X_list,axis=1)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    eps = 1e-2\n",
    "    n = 2000\n",
    "    x = np.linspace(np.min(X_list[0,:])-eps, np.max(X_list[0,:])+eps, n)\n",
    "    y = np.linspace(np.min(X_list[1,:])-eps, np.max(X_list[1,:])+eps, n)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    plt.contourf(X, Y, value(X, Y), 100, cmap=\"jet\")\n",
    "    plt.plot(X_list[0,:], X_list[1,:], color = \"red\", label = \"trajectory\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Trajectory of \"+title)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.plot(X_value_list, color = \"red\", label = \"value\")\n",
    "    plt.xlabel(\"num_iterations\")\n",
    "    plt.ylabel(\"value\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Objective Function Value of \"+title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251ccaf-f189-4d45-8e15-4f793451ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(X_list1, X_value_list1, title = \"L1-norm Method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3b912-5caf-485d-b7a5-6b7743e34a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(X_list2, X_value_list2, title = \"L2-norm Method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8856b36-d763-43a5-8e8b-d54687d96481",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(X_list3, X_value_list3, title = \"LInfinity-norm Method\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
